{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c10e07e",
   "metadata": {
    "id": "3c10e07e"
   },
   "source": [
    "### 資料載入\n",
    "使用下面的程式碼載入資料:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a79e564",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a79e564",
    "outputId": "6a4b7650-c6ee-477b-a4d7-fac75a28d596"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape =  (463715, 90)\n",
      "X_subtrain shape =  (417344, 90)\n",
      "X_valid shape =  (46371, 90)\n",
      "Y_subtrain shape =  (417344,)\n",
      "Y_valid shape =  (46371,)\n",
      "X_test shape =  (51630, 90)\n"
     ]
    }
   ],
   "source": [
    "# load packages\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Load data\n",
    "with open('data/msd_full.pickle', 'rb') as fh1:\n",
    "    msd_data = pickle.load(fh1)\n",
    "\n",
    "doscaling = 1\n",
    "if (doscaling == 1):\n",
    "    xscaler = preprocessing.StandardScaler().fit(msd_data['X_train'])\n",
    "    # standardize feature values\n",
    "    X_train = xscaler.transform(msd_data['X_train'])\n",
    "    X_test = xscaler.transform(msd_data['X_test'])\n",
    "else:\n",
    "    X_train = msd_data['X_train']\n",
    "    X_test = msd_data['X_test']\n",
    "\n",
    "Y_train = msd_data['Y_train']\n",
    "Y_test = msd_data['Y_test'].astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "y_mean = Y_train.mean()\n",
    "Y_train_keep = Y_train.copy()\n",
    "Y_test_keep = Y_test.copy()\n",
    "Y_train = Y_train - y_mean\n",
    "Y_test = Y_test - y_mean\n",
    "\n",
    "\n",
    "# validation is the last 10% of training, subtraining is the first 90% of training\n",
    "nvalid = int(X_train.shape[0] * 0.1)\n",
    "nsubtrain = X_train.shape[0] - nvalid\n",
    "\n",
    "X_subtrain = X_train[0:nsubtrain, :].astype('float32')\n",
    "X_valid = X_train[nsubtrain:, :].astype('float32')\n",
    "Y_subtrain = Y_train[0:nsubtrain].astype('float32')\n",
    "Y_valid = Y_train[nsubtrain:].astype('float32')\n",
    "\n",
    "Y_subtrain_keep = Y_train_keep[0:nsubtrain].astype('float32')\n",
    "Y_valid_keep = Y_train_keep[nsubtrain:].astype('float32')\n",
    "\n",
    "print(\"X_train shape = \", X_train.shape)\n",
    "print(\"X_subtrain shape = \", X_subtrain.shape)\n",
    "print(\"X_valid shape = \", X_valid.shape)\n",
    "print(\"Y_subtrain shape = \", Y_subtrain.shape)\n",
    "print(\"Y_valid shape = \", Y_valid.shape)\n",
    "print(\"X_test shape = \", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ce52f",
   "metadata": {
    "id": "b90ce52f"
   },
   "source": [
    "### Q1 (5%)\n",
    "使用Training資料訓練一個Ordinary Least Square模型，並進行預測。列出此模型的RMSE與前五個特徵的參數。OLS模型應包含常數項，且不應有任何Regularization。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a906247",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5a906247",
    "outputId": "86db77da-ee20-4fe4-d94e-070d5dc8ea85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE = 9.510160684544399\n",
      "前五個特徵的參數:[ 5.30975265 -2.88088114 -1.53234348  0.05737583 -0.33952889]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np \n",
    "\n",
    "linreg = LinearRegression().fit(X_train,Y_train)\n",
    "y_pred = linreg.predict(X_test)\n",
    "\n",
    "RMSE = (np.mean((Y_test - y_pred)**2))**0.5\n",
    "print(f'RMSE = {RMSE}')\n",
    "print(f'前五個特徵的參數:{linreg.coef_[:5]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0c5d6",
   "metadata": {
    "id": "74f0c5d6"
   },
   "source": [
    "### Q2 MLP with Four Hidden Layers (15%)\n",
    "建構一個有四層Hidden Layer的MLP。此模型由輸入層開始，90個Input Features通過線性層轉換為H個Hidden Nodes，並通過ReLu Activation Function，此為第一層Hidden Layer。\n",
    "接著通過下一個線性層與ReLu Activation Function，此為第二層。接著下一個線性層與ReLu Activation Function，此為第三層。\n",
    "然後下一個線性層與ReLu Activation Function，此為第四層。最後通過一個線性層輸出。\n",
    "所有Hidden Layer的寬度都為H。\n",
    "\n",
    "令H= 45, 使用Stochastic Gradient Descent更新參數，設Learning Rate = 0.00001，無Weight Decay與Momentum。畫出模型訓練過程中的Training與Validation RMSE，列出Test RMSE。 並討論訓練過程中Training與Validation RMSE的圖形意義。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c03954",
   "metadata": {
    "id": "30c03954"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, Xnp, Ynp):\n",
    "        'Initialization, passing Xnp and Ynp'\n",
    "        self.labels = Ynp\n",
    "        self.nobs = Xnp.shape[0]        \n",
    "        self.Xnp = Xnp\n",
    "        self.Ynp = Ynp\n",
    "  def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.nobs\n",
    "  def __getitem__(self, index):\n",
    "        'Generates one sample of data'        \n",
    "        X = self.Xnp[index]\n",
    "        y = self.Ynp[index]\n",
    "        return X, y\n",
    "    \n",
    "#y_train is a pandas.core.series.Series    \n",
    "trainset = Dataset(X_train, Y_train)   \n",
    "subtrainset = Dataset(X_subtrain,Y_subtrain)\n",
    "validset = Dataset(X_valid,Y_valid)\n",
    "testset = Dataset(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f134cc3d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f134cc3d",
    "outputId": "b842d26b-6cf4-49e5-d98e-5d42279e6195"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "trainloader = data.DataLoader(trainset, batch_size=1000, shuffle=True, num_workers=4)\n",
    "subtrainloader = data.DataLoader(subtrainset, batch_size=1000, shuffle=True, num_workers=4)\n",
    "validloader = data.DataLoader(validset, batch_size=1000, shuffle=True, num_workers=4)\n",
    "testloader = data.DataLoader(testset, batch_size=1000, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced6736a",
   "metadata": {
    "id": "ced6736a"
   },
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyMLP,self).__init__()\n",
    "        self.patience = 5000\n",
    "        self.epoch = 100\n",
    "        self.calc_rmse_intvls = 100\n",
    "        self.best_step_count = None\n",
    "        self.best_loss = np.inf\n",
    "        self.best_model = None\n",
    "        self.sse = nn.MSELoss(reduction='sum')\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "        self.train_rmse_lst = []\n",
    "        self.valid_rmse_lst = []\n",
    "        \n",
    "    def net(self,D_in,D_out,H,device,dropout = False):\n",
    "        if dropout:\n",
    "            model = nn.Sequential(\n",
    "                nn.Linear(D_in,H),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(H,H),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(H,H),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(H,H),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(p=0.5),\n",
    "                nn.Linear(H,D_out)\n",
    "                    )\n",
    "        else :\n",
    "            model = nn.Sequential(\n",
    "                            nn.Linear(D_in,H),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(H,H),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(H,H),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(H,H),\n",
    "                            nn.ReLU(),\n",
    "                            nn.Linear(H,D_out)\n",
    "                                )\n",
    "        model = model.float()\n",
    "        self.model = model.to(device)\n",
    "        return self.model \n",
    "        \n",
    "    def train(self,device,optimizer,verbose=True):\n",
    "        step_count = 0\n",
    "        total_loss = 0\n",
    "        for epoch_idx in range(self.epoch):\n",
    "            for batch_idx, (inputs, targets) in enumerate(subtrainloader):\n",
    "                targets = targets.reshape((-1, 1))\n",
    "                self.model.train()\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                step_count += 1\n",
    "                outputs = self.model(inputs)\n",
    "                loss_SSE = self.sse(outputs, targets) \n",
    "                loss_MSE = self.mse(outputs,targets)\n",
    "                loss_SSE.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss_MSE\n",
    "                if step_count % self.calc_rmse_intvls == 0:\n",
    "                    rmse_train = ((total_loss/step_count)**0.5).cpu().detach().numpy()\n",
    "                    rmse_valid =self.calc_rmse(device,validloader,self.model).cpu().detach().numpy()\n",
    "                    self.train_rmse_lst.append(rmse_train)\n",
    "                    self.valid_rmse_lst.append(rmse_valid)\n",
    "                    if verbose:\n",
    "                      print(f'Epoch:{epoch_idx} Batch:{step_count}')\n",
    "                      print(f'Training RMSE:{rmse_train}')\n",
    "                      print(f'Validation RMSE:{rmse_valid}')\n",
    "                      print('=====================================')\n",
    "                    #total_loss = 0\n",
    "                    if rmse_valid < self.best_loss:\n",
    "                        self.best_loss = rmse_valid\n",
    "                        self.best_step_count = step_count\n",
    "                        self.best_model = self.model \n",
    "                    elif step_count - self.best_step_count > self.patience:\n",
    "                        self.end_step = step_count\n",
    "                        print('============Early Stop==================')\n",
    "                        print(f'best step:{self.best_step_count} best loss:{self.best_loss}')\n",
    "                        return self.best_model  \n",
    "        self.end_step = step_count\n",
    "        return self.best_model\n",
    "                    \n",
    "                    \n",
    "    def calc_rmse(self,device,loader,model):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (inputs, targets) in enumerate(loader):\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                targets = targets.reshape((-1, 1))\n",
    "                outputs = model(inputs)        \n",
    "                loss= self.mse(outputs, targets)      \n",
    "                total_loss += loss\n",
    "            rmse = (total_loss/len(loader))**0.5\n",
    "        self.model.train()\n",
    "        return rmse\n",
    "    \n",
    "    def test(self,device,loader):\n",
    "        test_rmse = self.calc_rmse(device, loader, self.best_model)\n",
    "        return test_rmse\n",
    "    \n",
    "    def plot(self,H,dropout=False):\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(self.train_rmse_lst, label='subtrain')\n",
    "        plt.plot(self.valid_rmse_lst, label='valid')\n",
    "        plt.title(f'MLP with Four Hidden Layers (H = {H},dropout = {dropout})')\n",
    "        plt.xlabel('batchs (100 per)')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7c3f11",
   "metadata": {
    "id": "ba7c3f11"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.00001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4-FgChJMIJqr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "4-FgChJMIJqr",
    "outputId": "bc273bb7-d114-4880-90ba-0dd45a82c595"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.964855194091797\n",
      "Validation RMSE:10.919556617736816\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.719779968261719\n",
      "Validation RMSE:9.47946548461914\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:10.244072914123535\n",
      "Validation RMSE:9.087101936340332\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:9.951957702636719\n",
      "Validation RMSE:8.867131233215332\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.746618270874023\n",
      "Validation RMSE:8.792344093322754\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.611337661743164\n",
      "Validation RMSE:8.796989440917969\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.502579689025879\n",
      "Validation RMSE:8.743254661560059\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.426331520080566\n",
      "Validation RMSE:8.754172325134277\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.35663890838623\n",
      "Validation RMSE:8.695259094238281\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.300616264343262\n",
      "Validation RMSE:8.708649635314941\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.254692077636719\n",
      "Validation RMSE:8.755413055419922\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.215755462646484\n",
      "Validation RMSE:8.686851501464844\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.17808723449707\n",
      "Validation RMSE:8.638467788696289\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.146020889282227\n",
      "Validation RMSE:8.656335830688477\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.116716384887695\n",
      "Validation RMSE:8.646146774291992\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.09007453918457\n",
      "Validation RMSE:8.71174430847168\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.0633544921875\n",
      "Validation RMSE:8.63299560546875\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.042275428771973\n",
      "Validation RMSE:8.631092071533203\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.024490356445312\n",
      "Validation RMSE:8.621445655822754\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.004691123962402\n",
      "Validation RMSE:8.573974609375\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:8.989128112792969\n",
      "Validation RMSE:8.614930152893066\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:8.970710754394531\n",
      "Validation RMSE:8.633260726928711\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:8.956449508666992\n",
      "Validation RMSE:8.607399940490723\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:8.942362785339355\n",
      "Validation RMSE:8.57361125946045\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:8.930514335632324\n",
      "Validation RMSE:8.595283508300781\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:8.918354034423828\n",
      "Validation RMSE:8.643710136413574\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:8.906340599060059\n",
      "Validation RMSE:8.591374397277832\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:8.895090103149414\n",
      "Validation RMSE:8.638806343078613\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:8.883820533752441\n",
      "Validation RMSE:8.557632446289062\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:8.873950004577637\n",
      "Validation RMSE:8.682788848876953\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:8.862753868103027\n",
      "Validation RMSE:8.73054313659668\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:8.853982925415039\n",
      "Validation RMSE:8.576467514038086\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:8.846168518066406\n",
      "Validation RMSE:8.604541778564453\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:8.83839225769043\n",
      "Validation RMSE:8.601502418518066\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:8.828866004943848\n",
      "Validation RMSE:8.582632064819336\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:8.821664810180664\n",
      "Validation RMSE:8.5768404006958\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:8.814932823181152\n",
      "Validation RMSE:8.595086097717285\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:8.806975364685059\n",
      "Validation RMSE:8.582927703857422\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:8.799238204956055\n",
      "Validation RMSE:8.637237548828125\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:8.792848587036133\n",
      "Validation RMSE:8.658886909484863\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:8.787060737609863\n",
      "Validation RMSE:8.615548133850098\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:8.781510353088379\n",
      "Validation RMSE:8.58290958404541\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:8.775466918945312\n",
      "Validation RMSE:8.585742950439453\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:8.769293785095215\n",
      "Validation RMSE:8.75920295715332\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:8.763631820678711\n",
      "Validation RMSE:8.58654499053955\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:8.758496284484863\n",
      "Validation RMSE:8.578608512878418\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:8.752588272094727\n",
      "Validation RMSE:8.623747825622559\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:8.747733116149902\n",
      "Validation RMSE:8.57146167755127\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:8.742530822753906\n",
      "Validation RMSE:8.567607879638672\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:8.737265586853027\n",
      "Validation RMSE:8.572721481323242\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:8.732112884521484\n",
      "Validation RMSE:8.588411331176758\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:8.726863861083984\n",
      "Validation RMSE:8.565470695495605\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:8.723892211914062\n",
      "Validation RMSE:8.560606002807617\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:8.718594551086426\n",
      "Validation RMSE:8.561065673828125\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:8.714911460876465\n",
      "Validation RMSE:8.591599464416504\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:8.710590362548828\n",
      "Validation RMSE:8.59099292755127\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:8.706082344055176\n",
      "Validation RMSE:8.601423263549805\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:8.702001571655273\n",
      "Validation RMSE:8.569405555725098\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:8.698227882385254\n",
      "Validation RMSE:8.599602699279785\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:8.694313049316406\n",
      "Validation RMSE:8.587651252746582\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:8.690217971801758\n",
      "Validation RMSE:8.61799144744873\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:8.687056541442871\n",
      "Validation RMSE:8.545676231384277\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:8.682229995727539\n",
      "Validation RMSE:8.59316635131836\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:8.677824974060059\n",
      "Validation RMSE:8.56020736694336\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:8.6741304397583\n",
      "Validation RMSE:8.61161994934082\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:8.670969009399414\n",
      "Validation RMSE:8.58384895324707\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:8.668303489685059\n",
      "Validation RMSE:8.578327178955078\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:8.665184020996094\n",
      "Validation RMSE:8.576555252075195\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:8.661458015441895\n",
      "Validation RMSE:8.60750675201416\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:8.65810489654541\n",
      "Validation RMSE:8.633801460266113\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:8.655411720275879\n",
      "Validation RMSE:8.589071273803711\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:8.651801109313965\n",
      "Validation RMSE:8.59090518951416\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:8.648444175720215\n",
      "Validation RMSE:8.579394340515137\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:8.646025657653809\n",
      "Validation RMSE:8.710065841674805\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:8.643362998962402\n",
      "Validation RMSE:8.608182907104492\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:8.640117645263672\n",
      "Validation RMSE:8.574417114257812\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:8.637349128723145\n",
      "Validation RMSE:8.60248851776123\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:8.633949279785156\n",
      "Validation RMSE:8.59604549407959\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:8.631829261779785\n",
      "Validation RMSE:8.567825317382812\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:8.628440856933594\n",
      "Validation RMSE:8.556283950805664\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:8.62533950805664\n",
      "Validation RMSE:8.582144737243652\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:8.6234130859375\n",
      "Validation RMSE:8.604501724243164\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:8.620564460754395\n",
      "Validation RMSE:8.635603904724121\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:8.618131637573242\n",
      "Validation RMSE:8.598398208618164\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:8.614683151245117\n",
      "Validation RMSE:8.65304946899414\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:8.612309455871582\n",
      "Validation RMSE:8.571296691894531\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:8.60988712310791\n",
      "Validation RMSE:8.585769653320312\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:8.60766315460205\n",
      "Validation RMSE:8.59485912322998\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:8.604698181152344\n",
      "Validation RMSE:8.599764823913574\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:8.60231876373291\n",
      "Validation RMSE:8.590723991394043\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:8.600457191467285\n",
      "Validation RMSE:8.583352088928223\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:8.598394393920898\n",
      "Validation RMSE:8.618985176086426\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:8.595314025878906\n",
      "Validation RMSE:8.599554061889648\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:8.593066215515137\n",
      "Validation RMSE:8.616728782653809\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:8.591618537902832\n",
      "Validation RMSE:8.588482856750488\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:8.589183807373047\n",
      "Validation RMSE:8.595233917236328\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:8.587249755859375\n",
      "Validation RMSE:8.608793258666992\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:8.584850311279297\n",
      "Validation RMSE:8.623250007629395\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:8.582588195800781\n",
      "Validation RMSE:8.578542709350586\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:8.580327987670898\n",
      "Validation RMSE:8.612262725830078\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:8.57838249206543\n",
      "Validation RMSE:8.613991737365723\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:8.576136589050293\n",
      "Validation RMSE:8.58641242980957\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:8.57381534576416\n",
      "Validation RMSE:8.624463081359863\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:8.572216033935547\n",
      "Validation RMSE:8.650572776794434\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:8.56997013092041\n",
      "Validation RMSE:8.632597923278809\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:8.567509651184082\n",
      "Validation RMSE:8.628604888916016\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:8.565794944763184\n",
      "Validation RMSE:8.644224166870117\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:8.563749313354492\n",
      "Validation RMSE:8.61106014251709\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:8.561455726623535\n",
      "Validation RMSE:8.614727973937988\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:8.55929946899414\n",
      "Validation RMSE:8.592812538146973\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:8.557332038879395\n",
      "Validation RMSE:8.638703346252441\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:8.555588722229004\n",
      "Validation RMSE:8.574507713317871\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:8.554342269897461\n",
      "Validation RMSE:8.630475997924805\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:6200 best loss:8.545676231384277\n",
      "H = 45 Test_RMSE = 8.921512603759766\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Q2_model = MyMLP()\n",
    "model = Q2_model.net(input_shape,output_shape,H,device,dropout=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "model = Q2_model.train(device,optimizer)\n",
    "test_RMSE = Q2_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')\n",
    "Q2_model.plot(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mq49OXQygh1p",
   "metadata": {
    "id": "Mq49OXQygh1p"
   },
   "source": [
    "#### 討論訓練過程中Training與Validation RMSE的圖形意義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81lTgItLg4vh",
   "metadata": {
    "id": "81lTgItLg4vh"
   },
   "source": [
    "從以上圖表可知，Training RMSE不斷下降，Validation RMSE則大概到1500batch之後就不再下降而呈現上下不斷波動的情形，在9000batch之後甚至出現overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sYGvqG7ojla8",
   "metadata": {
    "id": "sYGvqG7ojla8"
   },
   "source": [
    "#### 刪除資料集，釋放記憶體"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c01cdc4",
   "metadata": {
    "id": "0c01cdc4"
   },
   "outputs": [],
   "source": [
    "del X_subtrain, Y_subtrain\n",
    "del X_valid, Y_valid\n",
    "del X_train, Y_train\n",
    "del X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a376e9",
   "metadata": {
    "id": "19a376e9"
   },
   "source": [
    "### Q3 (10%)\n",
    "重複上題步驟，使用H = 90與180。無須畫訓練過程的RMSE。列出這兩個Test RMSE。討論H = 45, 90, 180的Test RMSE。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MzdKQMRyIssv",
   "metadata": {
    "id": "MzdKQMRyIssv"
   },
   "source": [
    "H = 90時，Test RMSE數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132911fc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "132911fc",
    "outputId": "28530a72-fd1d-43a9-b492-eb733bdcad71"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.909226417541504\n",
      "Validation RMSE:10.817122459411621\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.583501815795898\n",
      "Validation RMSE:9.232998847961426\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:10.252165794372559\n",
      "Validation RMSE:9.03223991394043\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:10.030466079711914\n",
      "Validation RMSE:8.970934867858887\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.899024963378906\n",
      "Validation RMSE:8.928123474121094\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.795539855957031\n",
      "Validation RMSE:8.973295211791992\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.707758903503418\n",
      "Validation RMSE:8.910743713378906\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.63921070098877\n",
      "Validation RMSE:8.879053115844727\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.585904121398926\n",
      "Validation RMSE:8.878338813781738\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.54293441772461\n",
      "Validation RMSE:8.868816375732422\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.50635814666748\n",
      "Validation RMSE:8.865644454956055\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.474724769592285\n",
      "Validation RMSE:8.832792282104492\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.442312240600586\n",
      "Validation RMSE:8.891087532043457\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.414395332336426\n",
      "Validation RMSE:8.792920112609863\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.394633293151855\n",
      "Validation RMSE:8.737990379333496\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.37645149230957\n",
      "Validation RMSE:8.872310638427734\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.359614372253418\n",
      "Validation RMSE:8.757208824157715\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.340615272521973\n",
      "Validation RMSE:8.81044864654541\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.323677062988281\n",
      "Validation RMSE:8.76019287109375\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.311031341552734\n",
      "Validation RMSE:8.762516021728516\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.297410011291504\n",
      "Validation RMSE:8.724562644958496\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.286473274230957\n",
      "Validation RMSE:8.733782768249512\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.27448558807373\n",
      "Validation RMSE:8.754826545715332\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.262686729431152\n",
      "Validation RMSE:8.775113105773926\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.253701210021973\n",
      "Validation RMSE:8.747883796691895\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.245376586914062\n",
      "Validation RMSE:8.763276100158691\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.23554801940918\n",
      "Validation RMSE:8.741515159606934\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:9.226341247558594\n",
      "Validation RMSE:8.725532531738281\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:9.218844413757324\n",
      "Validation RMSE:8.732745170593262\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:9.212284088134766\n",
      "Validation RMSE:8.783781051635742\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:9.204158782958984\n",
      "Validation RMSE:8.712809562683105\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:9.19800853729248\n",
      "Validation RMSE:8.714920043945312\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:9.19235897064209\n",
      "Validation RMSE:8.711836814880371\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:9.185712814331055\n",
      "Validation RMSE:8.673601150512695\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:9.178593635559082\n",
      "Validation RMSE:8.719392776489258\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:9.172526359558105\n",
      "Validation RMSE:8.710490226745605\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:9.169243812561035\n",
      "Validation RMSE:8.667436599731445\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:9.162858009338379\n",
      "Validation RMSE:8.727631568908691\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:9.156898498535156\n",
      "Validation RMSE:8.688389778137207\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:9.152238845825195\n",
      "Validation RMSE:8.72017765045166\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:9.147200584411621\n",
      "Validation RMSE:8.682619094848633\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:9.1434965133667\n",
      "Validation RMSE:8.716875076293945\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:9.140198707580566\n",
      "Validation RMSE:8.71173095703125\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:9.136560440063477\n",
      "Validation RMSE:8.685758590698242\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:9.131463050842285\n",
      "Validation RMSE:8.694201469421387\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:9.12730884552002\n",
      "Validation RMSE:8.705605506896973\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:9.121848106384277\n",
      "Validation RMSE:8.7322359085083\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:9.118416786193848\n",
      "Validation RMSE:8.705260276794434\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:9.115023612976074\n",
      "Validation RMSE:8.679464340209961\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:9.112517356872559\n",
      "Validation RMSE:8.694755554199219\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:9.109380722045898\n",
      "Validation RMSE:8.678529739379883\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:9.105791091918945\n",
      "Validation RMSE:8.674378395080566\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:9.103442192077637\n",
      "Validation RMSE:8.67941951751709\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:9.10030746459961\n",
      "Validation RMSE:8.672582626342773\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:9.09750747680664\n",
      "Validation RMSE:8.670032501220703\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:9.094608306884766\n",
      "Validation RMSE:8.680970191955566\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:9.091485023498535\n",
      "Validation RMSE:8.680428504943848\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:9.089064598083496\n",
      "Validation RMSE:8.64419937133789\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:9.086280822753906\n",
      "Validation RMSE:8.666895866394043\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:9.08313274383545\n",
      "Validation RMSE:8.675429344177246\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:9.080941200256348\n",
      "Validation RMSE:8.717033386230469\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:9.077800750732422\n",
      "Validation RMSE:8.682225227355957\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:9.075969696044922\n",
      "Validation RMSE:8.681894302368164\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:9.073555946350098\n",
      "Validation RMSE:8.711795806884766\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:9.070854187011719\n",
      "Validation RMSE:8.684809684753418\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:9.069038391113281\n",
      "Validation RMSE:8.65300464630127\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:9.066520690917969\n",
      "Validation RMSE:8.628461837768555\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:9.063485145568848\n",
      "Validation RMSE:8.679064750671387\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:9.06159496307373\n",
      "Validation RMSE:8.646997451782227\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:9.05909538269043\n",
      "Validation RMSE:8.646173477172852\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:9.057527542114258\n",
      "Validation RMSE:8.698233604431152\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:9.055368423461914\n",
      "Validation RMSE:8.650317192077637\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:9.053702354431152\n",
      "Validation RMSE:8.650869369506836\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:9.051702499389648\n",
      "Validation RMSE:8.65079116821289\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:9.049384117126465\n",
      "Validation RMSE:8.707289695739746\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:9.047298431396484\n",
      "Validation RMSE:8.661574363708496\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:9.04550552368164\n",
      "Validation RMSE:8.647733688354492\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:9.043257713317871\n",
      "Validation RMSE:8.648940086364746\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:9.041375160217285\n",
      "Validation RMSE:8.64880084991455\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:9.039186477661133\n",
      "Validation RMSE:8.684223175048828\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:9.036961555480957\n",
      "Validation RMSE:8.627843856811523\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:9.035457611083984\n",
      "Validation RMSE:8.620073318481445\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:9.03423023223877\n",
      "Validation RMSE:8.618147850036621\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:9.032099723815918\n",
      "Validation RMSE:8.664384841918945\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:9.030745506286621\n",
      "Validation RMSE:8.653197288513184\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:9.029211044311523\n",
      "Validation RMSE:8.636009216308594\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:9.027525901794434\n",
      "Validation RMSE:8.644022941589355\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:9.026176452636719\n",
      "Validation RMSE:8.676396369934082\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:9.024151802062988\n",
      "Validation RMSE:8.614328384399414\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:9.022747039794922\n",
      "Validation RMSE:8.635733604431152\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:9.021214485168457\n",
      "Validation RMSE:8.611230850219727\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:9.020180702209473\n",
      "Validation RMSE:8.686400413513184\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:9.019431114196777\n",
      "Validation RMSE:8.666065216064453\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:9.017400741577148\n",
      "Validation RMSE:8.64248275756836\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:9.015871047973633\n",
      "Validation RMSE:8.651483535766602\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:9.014202117919922\n",
      "Validation RMSE:8.635249137878418\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:9.01240062713623\n",
      "Validation RMSE:8.633387565612793\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:9.011427879333496\n",
      "Validation RMSE:8.631333351135254\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:9.010254859924316\n",
      "Validation RMSE:8.612724304199219\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:9.008965492248535\n",
      "Validation RMSE:8.642311096191406\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:9.007979393005371\n",
      "Validation RMSE:8.742609024047852\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:9.006906509399414\n",
      "Validation RMSE:8.631377220153809\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:9.005722999572754\n",
      "Validation RMSE:8.645281791687012\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:9.004415512084961\n",
      "Validation RMSE:8.68307113647461\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:9.00291919708252\n",
      "Validation RMSE:8.6574125289917\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:9.00171184539795\n",
      "Validation RMSE:8.648465156555176\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:9.000539779663086\n",
      "Validation RMSE:8.6088285446167\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:8.999798774719238\n",
      "Validation RMSE:8.66038703918457\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:8.998095512390137\n",
      "Validation RMSE:8.614974021911621\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:8.99699592590332\n",
      "Validation RMSE:8.616939544677734\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:8.99553108215332\n",
      "Validation RMSE:8.633392333984375\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:8.994464874267578\n",
      "Validation RMSE:8.617448806762695\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:8.993804931640625\n",
      "Validation RMSE:8.626839637756348\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:8.992342948913574\n",
      "Validation RMSE:8.60501480102539\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:8.991003036499023\n",
      "Validation RMSE:8.64859390258789\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:8.990398406982422\n",
      "Validation RMSE:8.641813278198242\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:8.989286422729492\n",
      "Validation RMSE:8.62179946899414\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:8.988444328308105\n",
      "Validation RMSE:8.595564842224121\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:8.98725414276123\n",
      "Validation RMSE:8.679220199584961\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:8.98660659790039\n",
      "Validation RMSE:8.647491455078125\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:8.985288619995117\n",
      "Validation RMSE:8.654619216918945\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:8.984572410583496\n",
      "Validation RMSE:8.618247032165527\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:8.983691215515137\n",
      "Validation RMSE:8.646977424621582\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:8.982678413391113\n",
      "Validation RMSE:8.594572067260742\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:8.981263160705566\n",
      "Validation RMSE:8.638282775878906\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:8.979608535766602\n",
      "Validation RMSE:8.61587905883789\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:8.979144096374512\n",
      "Validation RMSE:8.601009368896484\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:8.97817325592041\n",
      "Validation RMSE:8.683091163635254\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:8.97728157043457\n",
      "Validation RMSE:8.608476638793945\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:8.976411819458008\n",
      "Validation RMSE:8.613896369934082\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:8.97562313079834\n",
      "Validation RMSE:8.647858619689941\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:8.97447681427002\n",
      "Validation RMSE:8.671022415161133\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:8.973541259765625\n",
      "Validation RMSE:8.619401931762695\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:8.972757339477539\n",
      "Validation RMSE:8.618032455444336\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:8.972004890441895\n",
      "Validation RMSE:8.704866409301758\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:8.971331596374512\n",
      "Validation RMSE:8.63648509979248\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:8.970224380493164\n",
      "Validation RMSE:8.669028282165527\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:8.969429969787598\n",
      "Validation RMSE:8.604941368103027\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:8.968276977539062\n",
      "Validation RMSE:8.645706176757812\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:8.966997146606445\n",
      "Validation RMSE:8.616372108459473\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:8.96632194519043\n",
      "Validation RMSE:8.633722305297852\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:8.965814590454102\n",
      "Validation RMSE:8.604950904846191\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:8.965049743652344\n",
      "Validation RMSE:8.650958061218262\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:8.964029312133789\n",
      "Validation RMSE:8.623464584350586\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:8.963083267211914\n",
      "Validation RMSE:8.632427215576172\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:8.96220874786377\n",
      "Validation RMSE:8.62409496307373\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:8.961437225341797\n",
      "Validation RMSE:8.598862648010254\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:8.960580825805664\n",
      "Validation RMSE:8.640871047973633\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:8.959985733032227\n",
      "Validation RMSE:8.617599487304688\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:8.959360122680664\n",
      "Validation RMSE:8.628178596496582\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:8.958269119262695\n",
      "Validation RMSE:8.611367225646973\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:8.957648277282715\n",
      "Validation RMSE:8.60274600982666\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:8.956596374511719\n",
      "Validation RMSE:8.63265609741211\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:8.955854415893555\n",
      "Validation RMSE:8.615446090698242\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:8.955212593078613\n",
      "Validation RMSE:8.642885208129883\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:8.954673767089844\n",
      "Validation RMSE:8.622793197631836\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:8.954411506652832\n",
      "Validation RMSE:8.608585357666016\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:8.952905654907227\n",
      "Validation RMSE:8.625260353088379\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:8.952306747436523\n",
      "Validation RMSE:8.641643524169922\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:8.951871871948242\n",
      "Validation RMSE:8.6145658493042\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:8.951135635375977\n",
      "Validation RMSE:8.604811668395996\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:8.950593948364258\n",
      "Validation RMSE:8.644532203674316\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:8.949482917785645\n",
      "Validation RMSE:8.624402046203613\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:8.948814392089844\n",
      "Validation RMSE:8.67532730102539\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:8.947964668273926\n",
      "Validation RMSE:8.625889778137207\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:8.947123527526855\n",
      "Validation RMSE:8.617005348205566\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:8.946568489074707\n",
      "Validation RMSE:8.604745864868164\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:8.946076393127441\n",
      "Validation RMSE:8.617876052856445\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:8.945390701293945\n",
      "Validation RMSE:8.607939720153809\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:8.944605827331543\n",
      "Validation RMSE:8.699381828308105\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:8.9442138671875\n",
      "Validation RMSE:8.605420112609863\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:8.943625450134277\n",
      "Validation RMSE:8.632735252380371\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:8.942747116088867\n",
      "Validation RMSE:8.600605010986328\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:8.9423828125\n",
      "Validation RMSE:8.606762886047363\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:8.941715240478516\n",
      "Validation RMSE:8.617118835449219\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:12400 best loss:8.594572067260742\n",
      "H = 90 Test_RMSE = 8.78969669342041\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H=90\n",
    "\n",
    "Q3_model = MyMLP()\n",
    "model = Q3_model.net(input_shape,output_shape,H,device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "model = Q3_model.train(device,optimizer)\n",
    "test_RMSE = Q3_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cXr4d3ORJRI8",
   "metadata": {
    "id": "cXr4d3ORJRI8"
   },
   "source": [
    "H = 180時，Test RMSE數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QOkzKWaRJN2h",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QOkzKWaRJN2h",
    "outputId": "d10369e7-57e2-46ce-e0a9-574895b4f494"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.841863632202148\n",
      "Validation RMSE:10.037274360656738\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.246490478515625\n",
      "Validation RMSE:9.134331703186035\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:9.941608428955078\n",
      "Validation RMSE:8.949620246887207\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:9.754101753234863\n",
      "Validation RMSE:8.896788597106934\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.628193855285645\n",
      "Validation RMSE:8.834697723388672\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.541803359985352\n",
      "Validation RMSE:8.869587898254395\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.483091354370117\n",
      "Validation RMSE:8.841927528381348\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.429625511169434\n",
      "Validation RMSE:8.806578636169434\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.3860502243042\n",
      "Validation RMSE:8.79455280303955\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.345904350280762\n",
      "Validation RMSE:8.749462127685547\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.315070152282715\n",
      "Validation RMSE:8.738259315490723\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.28564167022705\n",
      "Validation RMSE:8.731098175048828\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.258356094360352\n",
      "Validation RMSE:8.726624488830566\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.239638328552246\n",
      "Validation RMSE:8.737968444824219\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.223648071289062\n",
      "Validation RMSE:8.702582359313965\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.203723907470703\n",
      "Validation RMSE:8.6990966796875\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.18637466430664\n",
      "Validation RMSE:8.660726547241211\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.168644905090332\n",
      "Validation RMSE:8.74819564819336\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.156065940856934\n",
      "Validation RMSE:8.65714168548584\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.1432466506958\n",
      "Validation RMSE:8.695512771606445\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.131551742553711\n",
      "Validation RMSE:8.70322322845459\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.119848251342773\n",
      "Validation RMSE:8.69044303894043\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.109182357788086\n",
      "Validation RMSE:8.662358283996582\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.097746849060059\n",
      "Validation RMSE:8.75726318359375\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.089483261108398\n",
      "Validation RMSE:8.665144920349121\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.083619117736816\n",
      "Validation RMSE:8.675834655761719\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.073868751525879\n",
      "Validation RMSE:8.651208877563477\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:9.065184593200684\n",
      "Validation RMSE:8.652421951293945\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:9.056446075439453\n",
      "Validation RMSE:8.654632568359375\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:9.05005168914795\n",
      "Validation RMSE:8.648164749145508\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:9.043622016906738\n",
      "Validation RMSE:8.638933181762695\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:9.035911560058594\n",
      "Validation RMSE:8.655411720275879\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:9.03015422821045\n",
      "Validation RMSE:8.6514253616333\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:9.024579048156738\n",
      "Validation RMSE:8.633270263671875\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:9.019827842712402\n",
      "Validation RMSE:8.667521476745605\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:9.015080451965332\n",
      "Validation RMSE:8.649579048156738\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:9.00887680053711\n",
      "Validation RMSE:8.640786170959473\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:9.003859519958496\n",
      "Validation RMSE:8.657176971435547\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:8.999981880187988\n",
      "Validation RMSE:8.642451286315918\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:8.994828224182129\n",
      "Validation RMSE:8.625003814697266\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:8.989442825317383\n",
      "Validation RMSE:8.60446548461914\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:8.984894752502441\n",
      "Validation RMSE:8.608891487121582\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:8.979838371276855\n",
      "Validation RMSE:8.632816314697266\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:8.974026679992676\n",
      "Validation RMSE:8.595438003540039\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:8.970830917358398\n",
      "Validation RMSE:8.603560447692871\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:8.967583656311035\n",
      "Validation RMSE:8.621026992797852\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:8.964136123657227\n",
      "Validation RMSE:8.637845039367676\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:8.959176063537598\n",
      "Validation RMSE:8.607646942138672\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:8.95639705657959\n",
      "Validation RMSE:8.663261413574219\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:8.952140808105469\n",
      "Validation RMSE:8.600751876831055\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:8.948763847351074\n",
      "Validation RMSE:8.602161407470703\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:8.94473934173584\n",
      "Validation RMSE:8.59453296661377\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:8.941661834716797\n",
      "Validation RMSE:8.620081901550293\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:8.938408851623535\n",
      "Validation RMSE:8.597943305969238\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:8.936080932617188\n",
      "Validation RMSE:8.578778266906738\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:8.932875633239746\n",
      "Validation RMSE:8.589197158813477\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:8.92929744720459\n",
      "Validation RMSE:8.568825721740723\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:8.92645263671875\n",
      "Validation RMSE:8.59410572052002\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:8.923426628112793\n",
      "Validation RMSE:8.570609092712402\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:8.920413970947266\n",
      "Validation RMSE:8.564176559448242\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:8.916814804077148\n",
      "Validation RMSE:8.584258079528809\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:8.915251731872559\n",
      "Validation RMSE:8.56541919708252\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:8.912498474121094\n",
      "Validation RMSE:8.614734649658203\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:8.910161972045898\n",
      "Validation RMSE:8.600455284118652\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:8.907172203063965\n",
      "Validation RMSE:8.596256256103516\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:8.904725074768066\n",
      "Validation RMSE:8.571782112121582\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:8.902021408081055\n",
      "Validation RMSE:8.583040237426758\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:8.899057388305664\n",
      "Validation RMSE:8.575315475463867\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:8.896418571472168\n",
      "Validation RMSE:8.56838607788086\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:8.89443302154541\n",
      "Validation RMSE:8.581548690795898\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:8.892284393310547\n",
      "Validation RMSE:8.559386253356934\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:8.889564514160156\n",
      "Validation RMSE:8.57104778289795\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:8.88717269897461\n",
      "Validation RMSE:8.599358558654785\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:8.884435653686523\n",
      "Validation RMSE:8.59529972076416\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:8.88293743133545\n",
      "Validation RMSE:8.581141471862793\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:8.881163597106934\n",
      "Validation RMSE:8.581526756286621\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:8.878657341003418\n",
      "Validation RMSE:8.59233283996582\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:8.876609802246094\n",
      "Validation RMSE:8.535967826843262\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:8.874774932861328\n",
      "Validation RMSE:8.611762046813965\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:8.87258529663086\n",
      "Validation RMSE:8.57685661315918\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:8.87060546875\n",
      "Validation RMSE:8.573424339294434\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:8.868786811828613\n",
      "Validation RMSE:8.577534675598145\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:8.866893768310547\n",
      "Validation RMSE:8.588224411010742\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:8.865580558776855\n",
      "Validation RMSE:8.554030418395996\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:8.86319637298584\n",
      "Validation RMSE:8.587106704711914\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:8.861311912536621\n",
      "Validation RMSE:8.547215461730957\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:8.860087394714355\n",
      "Validation RMSE:8.61551284790039\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:8.85808277130127\n",
      "Validation RMSE:8.589937210083008\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:8.856517791748047\n",
      "Validation RMSE:8.585755348205566\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:8.85527515411377\n",
      "Validation RMSE:8.587909698486328\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:8.853260040283203\n",
      "Validation RMSE:8.593432426452637\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:8.851109504699707\n",
      "Validation RMSE:8.538273811340332\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:8.849400520324707\n",
      "Validation RMSE:8.556976318359375\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:8.847797393798828\n",
      "Validation RMSE:8.53538703918457\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:8.84595775604248\n",
      "Validation RMSE:8.549552917480469\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:8.844637870788574\n",
      "Validation RMSE:8.537592887878418\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:8.8429536819458\n",
      "Validation RMSE:8.561193466186523\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:8.84224796295166\n",
      "Validation RMSE:8.550328254699707\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:8.84019660949707\n",
      "Validation RMSE:8.564725875854492\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:8.839025497436523\n",
      "Validation RMSE:8.556093215942383\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:8.836953163146973\n",
      "Validation RMSE:8.577648162841797\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:8.835362434387207\n",
      "Validation RMSE:8.55274486541748\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:8.833924293518066\n",
      "Validation RMSE:8.55903434753418\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:8.832646369934082\n",
      "Validation RMSE:8.540228843688965\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:8.830909729003906\n",
      "Validation RMSE:8.56402587890625\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:8.829148292541504\n",
      "Validation RMSE:8.549457550048828\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:8.82808780670166\n",
      "Validation RMSE:8.54839038848877\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:8.827223777770996\n",
      "Validation RMSE:8.572275161743164\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:8.826077461242676\n",
      "Validation RMSE:8.548738479614258\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:8.824490547180176\n",
      "Validation RMSE:8.552227973937988\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:8.822912216186523\n",
      "Validation RMSE:8.559599876403809\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:8.821107864379883\n",
      "Validation RMSE:8.545491218566895\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:8.82061767578125\n",
      "Validation RMSE:8.565022468566895\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:8.819093704223633\n",
      "Validation RMSE:8.579024314880371\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:8.817625045776367\n",
      "Validation RMSE:8.536092758178711\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:8.816621780395508\n",
      "Validation RMSE:8.537742614746094\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:8.815369606018066\n",
      "Validation RMSE:8.536894798278809\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:8.814055442810059\n",
      "Validation RMSE:8.54020881652832\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:8.812309265136719\n",
      "Validation RMSE:8.548850059509277\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:8.81104564666748\n",
      "Validation RMSE:8.51396656036377\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:8.810064315795898\n",
      "Validation RMSE:8.550658226013184\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:8.80905532836914\n",
      "Validation RMSE:8.536256790161133\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:8.80787181854248\n",
      "Validation RMSE:8.54634952545166\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:8.806526184082031\n",
      "Validation RMSE:8.534931182861328\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:8.805536270141602\n",
      "Validation RMSE:8.516653060913086\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:8.804086685180664\n",
      "Validation RMSE:8.54377555847168\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:8.802580833435059\n",
      "Validation RMSE:8.562721252441406\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:8.801627159118652\n",
      "Validation RMSE:8.577828407287598\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:8.80080795288086\n",
      "Validation RMSE:8.559072494506836\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:8.799962043762207\n",
      "Validation RMSE:8.545949935913086\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:8.798928260803223\n",
      "Validation RMSE:8.525786399841309\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:8.797834396362305\n",
      "Validation RMSE:8.527288436889648\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:8.796375274658203\n",
      "Validation RMSE:8.556619644165039\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:8.795496940612793\n",
      "Validation RMSE:8.555298805236816\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:8.79446792602539\n",
      "Validation RMSE:8.492532730102539\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:8.793642044067383\n",
      "Validation RMSE:8.538429260253906\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:8.7928466796875\n",
      "Validation RMSE:8.486888885498047\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:8.79162883758545\n",
      "Validation RMSE:8.586700439453125\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:8.79015827178955\n",
      "Validation RMSE:8.556066513061523\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:8.789064407348633\n",
      "Validation RMSE:8.549530029296875\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:8.788209915161133\n",
      "Validation RMSE:8.580358505249023\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:8.787373542785645\n",
      "Validation RMSE:8.553506851196289\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:8.786197662353516\n",
      "Validation RMSE:8.551229476928711\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:8.78539752960205\n",
      "Validation RMSE:8.531752586364746\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:8.784472465515137\n",
      "Validation RMSE:8.536039352416992\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:8.783363342285156\n",
      "Validation RMSE:8.571828842163086\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:8.782597541809082\n",
      "Validation RMSE:8.511885643005371\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:8.781216621398926\n",
      "Validation RMSE:8.563508033752441\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:8.780261039733887\n",
      "Validation RMSE:8.53686237335205\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:8.779373168945312\n",
      "Validation RMSE:8.518441200256348\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:8.77854061126709\n",
      "Validation RMSE:8.569024085998535\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:8.777558326721191\n",
      "Validation RMSE:8.528069496154785\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:8.776681900024414\n",
      "Validation RMSE:8.541260719299316\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:8.775720596313477\n",
      "Validation RMSE:8.54134464263916\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:8.774641036987305\n",
      "Validation RMSE:8.534414291381836\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:8.773930549621582\n",
      "Validation RMSE:8.528400421142578\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:8.772974967956543\n",
      "Validation RMSE:8.529976844787598\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:8.772276878356934\n",
      "Validation RMSE:8.536534309387207\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:8.771409034729004\n",
      "Validation RMSE:8.516094207763672\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:8.770438194274902\n",
      "Validation RMSE:8.549424171447754\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:8.76937484741211\n",
      "Validation RMSE:8.533321380615234\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:8.768636703491211\n",
      "Validation RMSE:8.530130386352539\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:8.767950057983398\n",
      "Validation RMSE:8.54327392578125\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:8.766863822937012\n",
      "Validation RMSE:8.537829399108887\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:8.765898704528809\n",
      "Validation RMSE:8.49776554107666\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:8.765019416809082\n",
      "Validation RMSE:8.550029754638672\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:8.764426231384277\n",
      "Validation RMSE:8.513752937316895\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:8.763494491577148\n",
      "Validation RMSE:8.52504825592041\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:8.76281452178955\n",
      "Validation RMSE:8.532369613647461\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:8.761930465698242\n",
      "Validation RMSE:8.54145336151123\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:8.76113510131836\n",
      "Validation RMSE:8.542731285095215\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:8.760428428649902\n",
      "Validation RMSE:8.566031455993652\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:8.759760856628418\n",
      "Validation RMSE:8.519072532653809\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:8.758861541748047\n",
      "Validation RMSE:8.506601333618164\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:8.757914543151855\n",
      "Validation RMSE:8.521828651428223\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:8.757000923156738\n",
      "Validation RMSE:8.510965347290039\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:8.756292343139648\n",
      "Validation RMSE:8.53469467163086\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:8.755252838134766\n",
      "Validation RMSE:8.522642135620117\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:8.754488945007324\n",
      "Validation RMSE:8.520065307617188\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:8.753828048706055\n",
      "Validation RMSE:8.50621223449707\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:8.753000259399414\n",
      "Validation RMSE:8.504737854003906\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:8.752280235290527\n",
      "Validation RMSE:8.535666465759277\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:8.75130558013916\n",
      "Validation RMSE:8.552881240844727\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:8.750757217407227\n",
      "Validation RMSE:8.551680564880371\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:8.749529838562012\n",
      "Validation RMSE:8.556694984436035\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:8.74889087677002\n",
      "Validation RMSE:8.526915550231934\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:8.748329162597656\n",
      "Validation RMSE:8.550581932067871\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:8.747856140136719\n",
      "Validation RMSE:8.501982688903809\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:13700 best loss:8.486888885498047\n",
      "H = 180 Test_RMSE = 8.787442207336426\n"
     ]
    }
   ],
   "source": [
    "lr = 0.00001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H=180\n",
    "\n",
    "Q3_model = MyMLP()\n",
    "model = Q3_model.net(input_shape,output_shape,H,device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)\n",
    "model = Q3_model.train(device,optimizer)\n",
    "test_RMSE = Q3_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crFPVUsGJkDV",
   "metadata": {
    "id": "crFPVUsGJkDV"
   },
   "source": [
    "#### 討論H = 45, 90, 180的Test RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G75OxTQYJm5G",
   "metadata": {
    "id": "G75OxTQYJm5G"
   },
   "source": [
    "在H=180下擁有最好的損失函數，推測原因H=180時擁有最多的neuron，模型經過比較多層的訓練因此擁有最好的預測結果，而在Test RMSE的表現上H=180也表現的最好，但與H=90相差甚少，因此模型可能已經接近local minimum，隨著神經網路增加預測結果只好一點點"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e608486d",
   "metadata": {
    "id": "e608486d"
   },
   "source": [
    "### Q4 (15%)\n",
    "使用Q2的模型設定，考慮 H = 45, 90, 180與Weight Decay = 0.1, 0.2, 0.4的所有組合。模型估計後做表整理Test RMSE。討論H的選擇應為多少較合理?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223f19e5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "223f19e5",
    "outputId": "5e0e7dd8-b7d6-428d-ac16-6e8aa77c2731"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Early Stop==================\n",
      "best step:5600 best loss:8.558300971984863\n",
      "when H = 45, weight decay = 0.1, test_RMSE = 8.884754180908203\n",
      "============Early Stop==================\n",
      "best step:3700 best loss:8.544100761413574\n",
      "when H = 45, weight decay = 0.2, test_RMSE = 9.009414672851562\n",
      "============Early Stop==================\n",
      "best step:4200 best loss:8.558541297912598\n",
      "when H = 45, weight decay = 0.4, test_RMSE = 8.895747184753418\n",
      "============Early Stop==================\n",
      "best step:4500 best loss:8.500055313110352\n",
      "when H = 90, weight decay = 0.1, test_RMSE = 8.944291114807129\n",
      "============Early Stop==================\n",
      "best step:4600 best loss:8.525056838989258\n",
      "when H = 90, weight decay = 0.2, test_RMSE = 9.219419479370117\n",
      "============Early Stop==================\n",
      "best step:4000 best loss:8.500831604003906\n",
      "when H = 90, weight decay = 0.4, test_RMSE = 8.974624633789062\n",
      "============Early Stop==================\n",
      "best step:4000 best loss:8.465256690979004\n",
      "when H = 180, weight decay = 0.1, test_RMSE = 9.256674766540527\n",
      "============Early Stop==================\n",
      "best step:3300 best loss:8.463349342346191\n",
      "when H = 180, weight decay = 0.2, test_RMSE = 9.393705368041992\n",
      "============Early Stop==================\n",
      "best step:3700 best loss:8.497875213623047\n",
      "when H = 180, weight decay = 0.4, test_RMSE = 9.155298233032227\n"
     ]
    }
   ],
   "source": [
    "H_list = [45, 90, 180]\n",
    "weight_list = [0.1, 0.2, 0.4]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.00001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "\n",
    "for H in H_list:\n",
    "    for weight in weight_list:\n",
    "        Q4_model = MyMLP()\n",
    "        model = Q4_model.net(input_shape, output_shape,H,device)\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr = lr, weight_decay = weight)\n",
    "        print(f'Q4MLP(H={H} weight decay ={weight}) is training')\n",
    "        model = Q4_model.train(device, optimizer,verbose=False)\n",
    "        test_RMSE = Q4_model.test(device, testloader)\n",
    "        print(f'when H = {H}, weight decay = {weight}, test_RMSE = {test_RMSE}')  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OoLZweVmKjrV",
   "metadata": {
    "id": "OoLZweVmKjrV"
   },
   "source": [
    "#### 討論H的選擇應為多少較合理?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RgjGhFwlKrxD",
   "metadata": {
    "id": "RgjGhFwlKrxD"
   },
   "source": [
    "從以上結果可知，隨著H增加，test_RMSE呈現上升趨勢，由此可知過複雜的模型造成overfitting，因此以上最好的結果是when H = 45, weight decay = 0.1, test_RMSE = 8.884754180908203"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513bc026",
   "metadata": {
    "id": "513bc026"
   },
   "source": [
    "### Q5 MLP with Dropout (15%)\n",
    "建構一個有Dropout的四層Hidden Layer的MLP。此模型由輸入層開始，第一層由90個Input Features通過線性層轉換為H個Hidden Nodes，通過ReLu Activation Function，之後對Hidden Unit Dropout，機率為0.5。後面各Hidden Lyaer均在ReLu後有Dropout，機率皆為0.5。最後通過一個線性層輸出。所有Hidden Layer的寬度都為H。\n",
    "\n",
    "令H= 90, 使用Adaptive Moment Estimation (Adam)更新參數，設Learning Rate = 0.001，無Weight Decay與Momentum，其他參數使用預設值。畫出模型訓練過程中的Training與Validation RMSE，列出Test RMSE。 並討論訓練過程中Training與Validation RMSE的圖形意義。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b9cd4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "48b9cd4a",
    "outputId": "7162f13e-5ccb-4331-ece2-8ae73752003a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.496915817260742\n",
      "Validation RMSE:9.441896438598633\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.043078422546387\n",
      "Validation RMSE:9.104605674743652\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:9.833292007446289\n",
      "Validation RMSE:8.978920936584473\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:9.697859764099121\n",
      "Validation RMSE:8.935309410095215\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.602527618408203\n",
      "Validation RMSE:8.905191421508789\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.532760620117188\n",
      "Validation RMSE:8.847739219665527\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.479829788208008\n",
      "Validation RMSE:8.812065124511719\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.434534072875977\n",
      "Validation RMSE:8.811443328857422\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.398612976074219\n",
      "Validation RMSE:8.78073787689209\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.366822242736816\n",
      "Validation RMSE:8.785164833068848\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.3421630859375\n",
      "Validation RMSE:8.784465789794922\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.321125984191895\n",
      "Validation RMSE:8.80642032623291\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.302136421203613\n",
      "Validation RMSE:8.777929306030273\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.285347938537598\n",
      "Validation RMSE:8.752630233764648\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.265951156616211\n",
      "Validation RMSE:8.786699295043945\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.252054214477539\n",
      "Validation RMSE:8.731734275817871\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.238608360290527\n",
      "Validation RMSE:8.717052459716797\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.224949836730957\n",
      "Validation RMSE:8.70330810546875\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.215057373046875\n",
      "Validation RMSE:8.714752197265625\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.203385353088379\n",
      "Validation RMSE:8.705363273620605\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.1930513381958\n",
      "Validation RMSE:8.720746994018555\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.184203147888184\n",
      "Validation RMSE:8.716605186462402\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.175312995910645\n",
      "Validation RMSE:8.690526962280273\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.166871070861816\n",
      "Validation RMSE:8.706313133239746\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.157539367675781\n",
      "Validation RMSE:8.725104331970215\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.151956558227539\n",
      "Validation RMSE:8.687012672424316\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.144433975219727\n",
      "Validation RMSE:8.697422981262207\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:9.137718200683594\n",
      "Validation RMSE:8.732075691223145\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:9.13162612915039\n",
      "Validation RMSE:8.706191062927246\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:9.123398780822754\n",
      "Validation RMSE:8.697790145874023\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:9.117259979248047\n",
      "Validation RMSE:8.683024406433105\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:9.111832618713379\n",
      "Validation RMSE:8.686544418334961\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:9.107200622558594\n",
      "Validation RMSE:8.657069206237793\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:9.105182647705078\n",
      "Validation RMSE:8.658111572265625\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:9.097978591918945\n",
      "Validation RMSE:8.658734321594238\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:9.09356689453125\n",
      "Validation RMSE:8.65074634552002\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:9.08997631072998\n",
      "Validation RMSE:8.67170238494873\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:9.084467887878418\n",
      "Validation RMSE:8.664356231689453\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:9.079587936401367\n",
      "Validation RMSE:8.689118385314941\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:9.075309753417969\n",
      "Validation RMSE:8.694230079650879\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:9.071965217590332\n",
      "Validation RMSE:8.676350593566895\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:9.068949699401855\n",
      "Validation RMSE:8.647637367248535\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:9.065671920776367\n",
      "Validation RMSE:8.67074203491211\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:9.061870574951172\n",
      "Validation RMSE:8.655065536499023\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:9.05942153930664\n",
      "Validation RMSE:8.661505699157715\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:9.054973602294922\n",
      "Validation RMSE:8.648343086242676\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:9.050616264343262\n",
      "Validation RMSE:8.653759002685547\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:9.047749519348145\n",
      "Validation RMSE:8.655717849731445\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:9.044388771057129\n",
      "Validation RMSE:8.681427001953125\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:9.042632102966309\n",
      "Validation RMSE:8.656537055969238\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:9.039875030517578\n",
      "Validation RMSE:8.637688636779785\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:9.036073684692383\n",
      "Validation RMSE:8.655003547668457\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:9.033649444580078\n",
      "Validation RMSE:8.632070541381836\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:9.031049728393555\n",
      "Validation RMSE:8.658465385437012\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:9.028584480285645\n",
      "Validation RMSE:8.619478225708008\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:9.026151657104492\n",
      "Validation RMSE:8.635580062866211\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:9.023444175720215\n",
      "Validation RMSE:8.639968872070312\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:9.020910263061523\n",
      "Validation RMSE:8.652475357055664\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:9.0194730758667\n",
      "Validation RMSE:8.654912948608398\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:9.016266822814941\n",
      "Validation RMSE:8.631430625915527\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:9.014236450195312\n",
      "Validation RMSE:8.635446548461914\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:9.012494087219238\n",
      "Validation RMSE:8.637595176696777\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:9.010522842407227\n",
      "Validation RMSE:8.61951732635498\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:9.008612632751465\n",
      "Validation RMSE:8.61347770690918\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:9.006309509277344\n",
      "Validation RMSE:8.658340454101562\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:9.004220962524414\n",
      "Validation RMSE:8.629220962524414\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:9.002129554748535\n",
      "Validation RMSE:8.628936767578125\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:9.000442504882812\n",
      "Validation RMSE:8.6171293258667\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:8.998313903808594\n",
      "Validation RMSE:8.6289701461792\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:8.996332168579102\n",
      "Validation RMSE:8.617730140686035\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:8.994568824768066\n",
      "Validation RMSE:8.635440826416016\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:8.99238395690918\n",
      "Validation RMSE:8.633318901062012\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:8.991283416748047\n",
      "Validation RMSE:8.630555152893066\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:8.989360809326172\n",
      "Validation RMSE:8.643009185791016\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:8.987544059753418\n",
      "Validation RMSE:8.612119674682617\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:8.98617935180664\n",
      "Validation RMSE:8.632909774780273\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:8.984535217285156\n",
      "Validation RMSE:8.623883247375488\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:8.98222541809082\n",
      "Validation RMSE:8.625338554382324\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:8.98107624053955\n",
      "Validation RMSE:8.642419815063477\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:8.97901725769043\n",
      "Validation RMSE:8.620558738708496\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:8.97701644897461\n",
      "Validation RMSE:8.630370140075684\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:8.975634574890137\n",
      "Validation RMSE:8.620841979980469\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:8.975028991699219\n",
      "Validation RMSE:8.611812591552734\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:8.973094940185547\n",
      "Validation RMSE:8.625852584838867\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:8.97146987915039\n",
      "Validation RMSE:8.641740798950195\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:8.969900131225586\n",
      "Validation RMSE:8.616848945617676\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:8.969147682189941\n",
      "Validation RMSE:8.625085830688477\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:8.967673301696777\n",
      "Validation RMSE:8.623042106628418\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:8.965662002563477\n",
      "Validation RMSE:8.635092735290527\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:8.96462345123291\n",
      "Validation RMSE:8.635448455810547\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:8.962835311889648\n",
      "Validation RMSE:8.624344825744629\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:8.962069511413574\n",
      "Validation RMSE:8.62249755859375\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:8.960649490356445\n",
      "Validation RMSE:8.613121032714844\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:8.959527015686035\n",
      "Validation RMSE:8.629085540771484\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:8.958075523376465\n",
      "Validation RMSE:8.609992027282715\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:8.956686019897461\n",
      "Validation RMSE:8.60820198059082\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:8.955395698547363\n",
      "Validation RMSE:8.610718727111816\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:8.954025268554688\n",
      "Validation RMSE:8.615532875061035\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:8.952851295471191\n",
      "Validation RMSE:8.625580787658691\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:8.951671600341797\n",
      "Validation RMSE:8.614884376525879\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:8.950364112854004\n",
      "Validation RMSE:8.621411323547363\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:8.949602127075195\n",
      "Validation RMSE:8.633932113647461\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:8.948708534240723\n",
      "Validation RMSE:8.618733406066895\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:8.947303771972656\n",
      "Validation RMSE:8.609856605529785\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:8.946135520935059\n",
      "Validation RMSE:8.605761528015137\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:8.944734573364258\n",
      "Validation RMSE:8.612335205078125\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:8.943672180175781\n",
      "Validation RMSE:8.609628677368164\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:8.943062782287598\n",
      "Validation RMSE:8.605348587036133\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:8.94182014465332\n",
      "Validation RMSE:8.624098777770996\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:8.941215515136719\n",
      "Validation RMSE:8.595622062683105\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:8.939970970153809\n",
      "Validation RMSE:8.602519989013672\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:8.939032554626465\n",
      "Validation RMSE:8.609626770019531\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:8.937911033630371\n",
      "Validation RMSE:8.604385375976562\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:8.937112808227539\n",
      "Validation RMSE:8.617473602294922\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:8.936415672302246\n",
      "Validation RMSE:8.600488662719727\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:8.935506820678711\n",
      "Validation RMSE:8.613903045654297\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:8.934130668640137\n",
      "Validation RMSE:8.612048149108887\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:8.9330415725708\n",
      "Validation RMSE:8.640406608581543\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:8.932111740112305\n",
      "Validation RMSE:8.630481719970703\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:8.93105697631836\n",
      "Validation RMSE:8.590959548950195\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:8.930429458618164\n",
      "Validation RMSE:8.605725288391113\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:8.929606437683105\n",
      "Validation RMSE:8.613200187683105\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:8.928706169128418\n",
      "Validation RMSE:8.623467445373535\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:8.927949905395508\n",
      "Validation RMSE:8.599485397338867\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:8.927163124084473\n",
      "Validation RMSE:8.590097427368164\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:8.926234245300293\n",
      "Validation RMSE:8.607497215270996\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:8.92553424835205\n",
      "Validation RMSE:8.61690902709961\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:8.924692153930664\n",
      "Validation RMSE:8.610923767089844\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:8.923888206481934\n",
      "Validation RMSE:8.621649742126465\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:8.922941207885742\n",
      "Validation RMSE:8.605554580688477\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:8.921819686889648\n",
      "Validation RMSE:8.593483924865723\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:8.921270370483398\n",
      "Validation RMSE:8.62351131439209\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:8.92067813873291\n",
      "Validation RMSE:8.612391471862793\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:8.919791221618652\n",
      "Validation RMSE:8.598661422729492\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:8.919001579284668\n",
      "Validation RMSE:8.616990089416504\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:8.91826343536377\n",
      "Validation RMSE:8.606948852539062\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:8.917550086975098\n",
      "Validation RMSE:8.584769248962402\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:8.91688346862793\n",
      "Validation RMSE:8.619291305541992\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:8.915898323059082\n",
      "Validation RMSE:8.59356689453125\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:8.915247917175293\n",
      "Validation RMSE:8.611959457397461\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:8.914460182189941\n",
      "Validation RMSE:8.616096496582031\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:8.913722038269043\n",
      "Validation RMSE:8.603827476501465\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:8.91301155090332\n",
      "Validation RMSE:8.598692893981934\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:8.912392616271973\n",
      "Validation RMSE:8.609293937683105\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:8.911945343017578\n",
      "Validation RMSE:8.624565124511719\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:8.91104507446289\n",
      "Validation RMSE:8.606298446655273\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:8.910381317138672\n",
      "Validation RMSE:8.61701488494873\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:8.909285545349121\n",
      "Validation RMSE:8.59176254272461\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:8.908805847167969\n",
      "Validation RMSE:8.600812911987305\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:8.908002853393555\n",
      "Validation RMSE:8.614337921142578\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:8.907780647277832\n",
      "Validation RMSE:8.613092422485352\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:8.9068603515625\n",
      "Validation RMSE:8.599236488342285\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:8.906269073486328\n",
      "Validation RMSE:8.58427619934082\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:8.905673027038574\n",
      "Validation RMSE:8.610930442810059\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:8.904903411865234\n",
      "Validation RMSE:8.601419448852539\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:8.904129981994629\n",
      "Validation RMSE:8.63552474975586\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:8.903707504272461\n",
      "Validation RMSE:8.605780601501465\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:8.903094291687012\n",
      "Validation RMSE:8.593131065368652\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:8.902585983276367\n",
      "Validation RMSE:8.608402252197266\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:8.90163803100586\n",
      "Validation RMSE:8.610329627990723\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:8.900774002075195\n",
      "Validation RMSE:8.577973365783691\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:8.900320053100586\n",
      "Validation RMSE:8.585332870483398\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:8.899925231933594\n",
      "Validation RMSE:8.602459907531738\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:8.89939022064209\n",
      "Validation RMSE:8.606791496276855\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:8.89865779876709\n",
      "Validation RMSE:8.590307235717773\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:8.89826488494873\n",
      "Validation RMSE:8.593039512634277\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:8.89753532409668\n",
      "Validation RMSE:8.611225128173828\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:8.896770477294922\n",
      "Validation RMSE:8.589459419250488\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:8.896239280700684\n",
      "Validation RMSE:8.599112510681152\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:8.895833015441895\n",
      "Validation RMSE:8.593536376953125\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:8.895472526550293\n",
      "Validation RMSE:8.587370872497559\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:8.894932746887207\n",
      "Validation RMSE:8.582062721252441\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:8.894327163696289\n",
      "Validation RMSE:8.60521125793457\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:8.893698692321777\n",
      "Validation RMSE:8.597431182861328\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:8.893311500549316\n",
      "Validation RMSE:8.592090606689453\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:8.892870903015137\n",
      "Validation RMSE:8.581294059753418\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:8.892317771911621\n",
      "Validation RMSE:8.603492736816406\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:8.891715049743652\n",
      "Validation RMSE:8.585227012634277\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:8.89136028289795\n",
      "Validation RMSE:8.600144386291504\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:8.890645980834961\n",
      "Validation RMSE:8.601669311523438\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:8.89019775390625\n",
      "Validation RMSE:8.616347312927246\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:8.88983154296875\n",
      "Validation RMSE:8.597565650939941\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:8.889222145080566\n",
      "Validation RMSE:8.582291603088379\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:8.888720512390137\n",
      "Validation RMSE:8.58924674987793\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:8.888012886047363\n",
      "Validation RMSE:8.597187042236328\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:8.887655258178711\n",
      "Validation RMSE:8.590405464172363\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:8.887184143066406\n",
      "Validation RMSE:8.615208625793457\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:8.886722564697266\n",
      "Validation RMSE:8.592595100402832\n",
      "=====================================\n",
      "Epoch:45 Batch:18900\n",
      "Training RMSE:8.886014938354492\n",
      "Validation RMSE:8.602195739746094\n",
      "=====================================\n",
      "Epoch:45 Batch:19000\n",
      "Training RMSE:8.885594367980957\n",
      "Validation RMSE:8.592597961425781\n",
      "=====================================\n",
      "Epoch:45 Batch:19100\n",
      "Training RMSE:8.884995460510254\n",
      "Validation RMSE:8.58140754699707\n",
      "=====================================\n",
      "Epoch:45 Batch:19200\n",
      "Training RMSE:8.884684562683105\n",
      "Validation RMSE:8.594027519226074\n",
      "=====================================\n",
      "Epoch:46 Batch:19300\n",
      "Training RMSE:8.884234428405762\n",
      "Validation RMSE:8.611021995544434\n",
      "=====================================\n",
      "Epoch:46 Batch:19400\n",
      "Training RMSE:8.883842468261719\n",
      "Validation RMSE:8.581523895263672\n",
      "=====================================\n",
      "Epoch:46 Batch:19500\n",
      "Training RMSE:8.883313179016113\n",
      "Validation RMSE:8.60604190826416\n",
      "=====================================\n",
      "Epoch:46 Batch:19600\n",
      "Training RMSE:8.882712364196777\n",
      "Validation RMSE:8.61923885345459\n",
      "=====================================\n",
      "Epoch:47 Batch:19700\n",
      "Training RMSE:8.882323265075684\n",
      "Validation RMSE:8.58738899230957\n",
      "=====================================\n",
      "Epoch:47 Batch:19800\n",
      "Training RMSE:8.881948471069336\n",
      "Validation RMSE:8.577609062194824\n",
      "=====================================\n",
      "Epoch:47 Batch:19900\n",
      "Training RMSE:8.881064414978027\n",
      "Validation RMSE:8.559676170349121\n",
      "=====================================\n",
      "Epoch:47 Batch:20000\n",
      "Training RMSE:8.880915641784668\n",
      "Validation RMSE:8.599573135375977\n",
      "=====================================\n",
      "Epoch:48 Batch:20100\n",
      "Training RMSE:8.880433082580566\n",
      "Validation RMSE:8.601076126098633\n",
      "=====================================\n",
      "Epoch:48 Batch:20200\n",
      "Training RMSE:8.879827499389648\n",
      "Validation RMSE:8.584325790405273\n",
      "=====================================\n",
      "Epoch:48 Batch:20300\n",
      "Training RMSE:8.879427909851074\n",
      "Validation RMSE:8.605402946472168\n",
      "=====================================\n",
      "Epoch:48 Batch:20400\n",
      "Training RMSE:8.879180908203125\n",
      "Validation RMSE:8.592432022094727\n",
      "=====================================\n",
      "Epoch:49 Batch:20500\n",
      "Training RMSE:8.878791809082031\n",
      "Validation RMSE:8.584746360778809\n",
      "=====================================\n",
      "Epoch:49 Batch:20600\n",
      "Training RMSE:8.87843132019043\n",
      "Validation RMSE:8.610448837280273\n",
      "=====================================\n",
      "Epoch:49 Batch:20700\n",
      "Training RMSE:8.878181457519531\n",
      "Validation RMSE:8.616933822631836\n",
      "=====================================\n",
      "Epoch:49 Batch:20800\n",
      "Training RMSE:8.877582550048828\n",
      "Validation RMSE:8.605913162231445\n",
      "=====================================\n",
      "Epoch:49 Batch:20900\n",
      "Training RMSE:8.877071380615234\n",
      "Validation RMSE:8.622063636779785\n",
      "=====================================\n",
      "Epoch:50 Batch:21000\n",
      "Training RMSE:8.876395225524902\n",
      "Validation RMSE:8.632682800292969\n",
      "=====================================\n",
      "Epoch:50 Batch:21100\n",
      "Training RMSE:8.875997543334961\n",
      "Validation RMSE:8.594181060791016\n",
      "=====================================\n",
      "Epoch:50 Batch:21200\n",
      "Training RMSE:8.875682830810547\n",
      "Validation RMSE:8.592923164367676\n",
      "=====================================\n",
      "Epoch:50 Batch:21300\n",
      "Training RMSE:8.875327110290527\n",
      "Validation RMSE:8.589905738830566\n",
      "=====================================\n",
      "Epoch:51 Batch:21400\n",
      "Training RMSE:8.874814987182617\n",
      "Validation RMSE:8.59215259552002\n",
      "=====================================\n",
      "Epoch:51 Batch:21500\n",
      "Training RMSE:8.874312400817871\n",
      "Validation RMSE:8.610275268554688\n",
      "=====================================\n",
      "Epoch:51 Batch:21600\n",
      "Training RMSE:8.873725891113281\n",
      "Validation RMSE:8.585735321044922\n",
      "=====================================\n",
      "Epoch:51 Batch:21700\n",
      "Training RMSE:8.873544692993164\n",
      "Validation RMSE:8.59520149230957\n",
      "=====================================\n",
      "Epoch:52 Batch:21800\n",
      "Training RMSE:8.873332977294922\n",
      "Validation RMSE:8.57656478881836\n",
      "=====================================\n",
      "Epoch:52 Batch:21900\n",
      "Training RMSE:8.872771263122559\n",
      "Validation RMSE:8.598625183105469\n",
      "=====================================\n",
      "Epoch:52 Batch:22000\n",
      "Training RMSE:8.872387886047363\n",
      "Validation RMSE:8.595477104187012\n",
      "=====================================\n",
      "Epoch:52 Batch:22100\n",
      "Training RMSE:8.872108459472656\n",
      "Validation RMSE:8.590521812438965\n",
      "=====================================\n",
      "Epoch:53 Batch:22200\n",
      "Training RMSE:8.871960639953613\n",
      "Validation RMSE:8.608067512512207\n",
      "=====================================\n",
      "Epoch:53 Batch:22300\n",
      "Training RMSE:8.871370315551758\n",
      "Validation RMSE:8.578254699707031\n",
      "=====================================\n",
      "Epoch:53 Batch:22400\n",
      "Training RMSE:8.87116813659668\n",
      "Validation RMSE:8.588141441345215\n",
      "=====================================\n",
      "Epoch:53 Batch:22500\n",
      "Training RMSE:8.870944023132324\n",
      "Validation RMSE:8.58933162689209\n",
      "=====================================\n",
      "Epoch:54 Batch:22600\n",
      "Training RMSE:8.870450019836426\n",
      "Validation RMSE:8.572919845581055\n",
      "=====================================\n",
      "Epoch:54 Batch:22700\n",
      "Training RMSE:8.869935989379883\n",
      "Validation RMSE:8.574346542358398\n",
      "=====================================\n",
      "Epoch:54 Batch:22800\n",
      "Training RMSE:8.869638442993164\n",
      "Validation RMSE:8.586692810058594\n",
      "=====================================\n",
      "Epoch:54 Batch:22900\n",
      "Training RMSE:8.869297981262207\n",
      "Validation RMSE:8.590971946716309\n",
      "=====================================\n",
      "Epoch:55 Batch:23000\n",
      "Training RMSE:8.868958473205566\n",
      "Validation RMSE:8.58033561706543\n",
      "=====================================\n",
      "Epoch:55 Batch:23100\n",
      "Training RMSE:8.868435859680176\n",
      "Validation RMSE:8.5888032913208\n",
      "=====================================\n",
      "Epoch:55 Batch:23200\n",
      "Training RMSE:8.868066787719727\n",
      "Validation RMSE:8.609196662902832\n",
      "=====================================\n",
      "Epoch:55 Batch:23300\n",
      "Training RMSE:8.867785453796387\n",
      "Validation RMSE:8.603727340698242\n",
      "=====================================\n",
      "Epoch:55 Batch:23400\n",
      "Training RMSE:8.867459297180176\n",
      "Validation RMSE:8.599120140075684\n",
      "=====================================\n",
      "Epoch:56 Batch:23500\n",
      "Training RMSE:8.867033958435059\n",
      "Validation RMSE:8.599166870117188\n",
      "=====================================\n",
      "Epoch:56 Batch:23600\n",
      "Training RMSE:8.86656379699707\n",
      "Validation RMSE:8.57768440246582\n",
      "=====================================\n",
      "Epoch:56 Batch:23700\n",
      "Training RMSE:8.866482734680176\n",
      "Validation RMSE:8.58745288848877\n",
      "=====================================\n",
      "Epoch:56 Batch:23800\n",
      "Training RMSE:8.86609172821045\n",
      "Validation RMSE:8.599481582641602\n",
      "=====================================\n",
      "Epoch:57 Batch:23900\n",
      "Training RMSE:8.865677833557129\n",
      "Validation RMSE:8.586511611938477\n",
      "=====================================\n",
      "Epoch:57 Batch:24000\n",
      "Training RMSE:8.865127563476562\n",
      "Validation RMSE:8.606855392456055\n",
      "=====================================\n",
      "Epoch:57 Batch:24100\n",
      "Training RMSE:8.864776611328125\n",
      "Validation RMSE:8.599161148071289\n",
      "=====================================\n",
      "Epoch:57 Batch:24200\n",
      "Training RMSE:8.864411354064941\n",
      "Validation RMSE:8.600467681884766\n",
      "=====================================\n",
      "Epoch:58 Batch:24300\n",
      "Training RMSE:8.864278793334961\n",
      "Validation RMSE:8.580119132995605\n",
      "=====================================\n",
      "Epoch:58 Batch:24400\n",
      "Training RMSE:8.863626480102539\n",
      "Validation RMSE:8.579561233520508\n",
      "=====================================\n",
      "Epoch:58 Batch:24500\n",
      "Training RMSE:8.863276481628418\n",
      "Validation RMSE:8.608034133911133\n",
      "=====================================\n",
      "Epoch:58 Batch:24600\n",
      "Training RMSE:8.863005638122559\n",
      "Validation RMSE:8.585552215576172\n",
      "=====================================\n",
      "Epoch:59 Batch:24700\n",
      "Training RMSE:8.862870216369629\n",
      "Validation RMSE:8.578667640686035\n",
      "=====================================\n",
      "Epoch:59 Batch:24800\n",
      "Training RMSE:8.862826347351074\n",
      "Validation RMSE:8.58601188659668\n",
      "=====================================\n",
      "Epoch:59 Batch:24900\n",
      "Training RMSE:8.862513542175293\n",
      "Validation RMSE:8.599801063537598\n",
      "=====================================\n",
      "Epoch:59 Batch:25000\n",
      "Training RMSE:8.862080574035645\n",
      "Validation RMSE:8.58952522277832\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:19900 best loss:8.559676170349121\n",
      "H = 90 Test_RMSE = 8.768635749816895\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 90\n",
    "\n",
    "\n",
    "Q5_model = MyMLP()\n",
    "model = Q5_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "model = Q5_model.train(device,optimizer)\n",
    "test_RMSE = Q5_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')\n",
    "Q5_model.plot(H,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Wc_QgvZYKxS2",
   "metadata": {
    "id": "Wc_QgvZYKxS2"
   },
   "source": [
    "### 討論訓練過程中Training與Validation RMSE的圖形意義"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NXD0AUwjK-7-",
   "metadata": {
    "id": "NXD0AUwjK-7-"
   },
   "source": [
    "與前面圖形相比，很明顯有加入Dropout，batch後半段不再出現overfitting，因為Dropout只留下一半的feature來產生output，會導致留下來的feature對output影響結果更強。等到進入validation階段的時候，全部的neuron一起運作就會讓output的結果更好(error更小)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f2123f",
   "metadata": {
    "id": "74f2123f"
   },
   "source": [
    "### Q6 Explore Number of Hidden Units (10%)\n",
    "使用上題的模型，考慮H = 20, 180, 360。 討論H = 20, 45, 180, 360的Test RMSE。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BtYec-deJs35",
   "metadata": {
    "id": "BtYec-deJs35"
   },
   "source": [
    "H = 20時，Test RMSE數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129fb96a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "129fb96a",
    "outputId": "54e9fad0-c05f-425a-80bc-a5f736e86fa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.914627075195312\n",
      "Validation RMSE:10.66990852355957\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.686294555664062\n",
      "Validation RMSE:9.773836135864258\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:10.461749076843262\n",
      "Validation RMSE:9.514527320861816\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:10.30649185180664\n",
      "Validation RMSE:9.35497760772705\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:10.190840721130371\n",
      "Validation RMSE:9.269824981689453\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:10.108076095581055\n",
      "Validation RMSE:9.243973731994629\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:10.050711631774902\n",
      "Validation RMSE:9.194318771362305\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.99825668334961\n",
      "Validation RMSE:9.173192024230957\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.949224472045898\n",
      "Validation RMSE:9.146329879760742\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.914420127868652\n",
      "Validation RMSE:9.12095832824707\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.879379272460938\n",
      "Validation RMSE:9.138766288757324\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.855792999267578\n",
      "Validation RMSE:9.133697509765625\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.826903343200684\n",
      "Validation RMSE:9.087327003479004\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.804409980773926\n",
      "Validation RMSE:9.0774507522583\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.782914161682129\n",
      "Validation RMSE:9.068632125854492\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.765771865844727\n",
      "Validation RMSE:9.050687789916992\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.749263763427734\n",
      "Validation RMSE:9.044750213623047\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.734247207641602\n",
      "Validation RMSE:9.05866813659668\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.71997356414795\n",
      "Validation RMSE:9.053080558776855\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.710972785949707\n",
      "Validation RMSE:9.030834197998047\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.697864532470703\n",
      "Validation RMSE:9.024820327758789\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.68596363067627\n",
      "Validation RMSE:9.038220405578613\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.678877830505371\n",
      "Validation RMSE:9.021206855773926\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.669245719909668\n",
      "Validation RMSE:9.033392906188965\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.658300399780273\n",
      "Validation RMSE:9.002842903137207\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.6502103805542\n",
      "Validation RMSE:9.011322021484375\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.642525672912598\n",
      "Validation RMSE:9.045050621032715\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:9.634082794189453\n",
      "Validation RMSE:8.992154121398926\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:9.625246047973633\n",
      "Validation RMSE:8.999637603759766\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:9.618185043334961\n",
      "Validation RMSE:9.011405944824219\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:9.611390113830566\n",
      "Validation RMSE:9.005149841308594\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:9.6050386428833\n",
      "Validation RMSE:9.03322696685791\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:9.599499702453613\n",
      "Validation RMSE:8.99563217163086\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:9.59365463256836\n",
      "Validation RMSE:9.013047218322754\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:9.588715553283691\n",
      "Validation RMSE:8.996969223022461\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:9.584388732910156\n",
      "Validation RMSE:8.996264457702637\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:9.580510139465332\n",
      "Validation RMSE:8.974763870239258\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:9.573030471801758\n",
      "Validation RMSE:8.98835277557373\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:9.567706108093262\n",
      "Validation RMSE:8.986631393432617\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:9.563843727111816\n",
      "Validation RMSE:8.989677429199219\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:9.559762954711914\n",
      "Validation RMSE:8.986533164978027\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:9.555835723876953\n",
      "Validation RMSE:8.958685874938965\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:9.552404403686523\n",
      "Validation RMSE:8.981489181518555\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:9.54941177368164\n",
      "Validation RMSE:8.98404312133789\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:9.54565143585205\n",
      "Validation RMSE:8.981766700744629\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:9.540915489196777\n",
      "Validation RMSE:8.970976829528809\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:9.536340713500977\n",
      "Validation RMSE:8.978581428527832\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:9.533917427062988\n",
      "Validation RMSE:8.961199760437012\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:9.530579566955566\n",
      "Validation RMSE:8.959870338439941\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:9.527382850646973\n",
      "Validation RMSE:8.969070434570312\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:9.524032592773438\n",
      "Validation RMSE:8.981108665466309\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:9.5217924118042\n",
      "Validation RMSE:8.953907012939453\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:9.518898963928223\n",
      "Validation RMSE:8.973569869995117\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:9.51703929901123\n",
      "Validation RMSE:8.957475662231445\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:9.514694213867188\n",
      "Validation RMSE:8.990943908691406\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:9.511744499206543\n",
      "Validation RMSE:8.957942008972168\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:9.509847640991211\n",
      "Validation RMSE:8.958005905151367\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:9.507879257202148\n",
      "Validation RMSE:8.98630142211914\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:9.505064964294434\n",
      "Validation RMSE:8.986526489257812\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:9.501917839050293\n",
      "Validation RMSE:8.957813262939453\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:9.499529838562012\n",
      "Validation RMSE:8.956440925598145\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:9.497522354125977\n",
      "Validation RMSE:8.949347496032715\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:9.49555778503418\n",
      "Validation RMSE:8.970680236816406\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:9.492474555969238\n",
      "Validation RMSE:8.963127136230469\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:9.491265296936035\n",
      "Validation RMSE:8.962211608886719\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:9.489127159118652\n",
      "Validation RMSE:8.959891319274902\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:9.487319946289062\n",
      "Validation RMSE:8.933698654174805\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:9.485923767089844\n",
      "Validation RMSE:8.937687873840332\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:9.484147071838379\n",
      "Validation RMSE:8.962054252624512\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:9.48170280456543\n",
      "Validation RMSE:8.967501640319824\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:9.48007869720459\n",
      "Validation RMSE:8.942994117736816\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:9.478901863098145\n",
      "Validation RMSE:8.946059226989746\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:9.476741790771484\n",
      "Validation RMSE:8.943411827087402\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:9.475199699401855\n",
      "Validation RMSE:8.956515312194824\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:9.473509788513184\n",
      "Validation RMSE:8.954835891723633\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:9.472464561462402\n",
      "Validation RMSE:8.97285270690918\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:9.471090316772461\n",
      "Validation RMSE:8.9478178024292\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:9.469473838806152\n",
      "Validation RMSE:8.975471496582031\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:9.467411994934082\n",
      "Validation RMSE:8.946147918701172\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:9.46584701538086\n",
      "Validation RMSE:8.936264038085938\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:9.465216636657715\n",
      "Validation RMSE:8.958321571350098\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:9.463644027709961\n",
      "Validation RMSE:8.95538330078125\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:9.462335586547852\n",
      "Validation RMSE:8.959491729736328\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:9.461166381835938\n",
      "Validation RMSE:8.963441848754883\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:9.45992660522461\n",
      "Validation RMSE:8.94599723815918\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:9.458206176757812\n",
      "Validation RMSE:8.958547592163086\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:9.457015037536621\n",
      "Validation RMSE:8.945082664489746\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:9.455531120300293\n",
      "Validation RMSE:8.983120918273926\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:9.454277038574219\n",
      "Validation RMSE:8.952959060668945\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:9.453542709350586\n",
      "Validation RMSE:8.949471473693848\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:9.452434539794922\n",
      "Validation RMSE:8.974387168884277\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:9.450996398925781\n",
      "Validation RMSE:8.94796371459961\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:9.450039863586426\n",
      "Validation RMSE:8.960929870605469\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:9.449015617370605\n",
      "Validation RMSE:8.947702407836914\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:9.447423934936523\n",
      "Validation RMSE:8.923069953918457\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:9.446887016296387\n",
      "Validation RMSE:8.968594551086426\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:9.445972442626953\n",
      "Validation RMSE:8.927423477172852\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:9.444432258605957\n",
      "Validation RMSE:8.956598281860352\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:9.44347858428955\n",
      "Validation RMSE:8.956976890563965\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:9.442558288574219\n",
      "Validation RMSE:8.950156211853027\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:9.44178581237793\n",
      "Validation RMSE:8.969358444213867\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:9.440744400024414\n",
      "Validation RMSE:8.95895004272461\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:9.440096855163574\n",
      "Validation RMSE:8.95604133605957\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:9.439237594604492\n",
      "Validation RMSE:8.9342041015625\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:9.438338279724121\n",
      "Validation RMSE:8.936882972717285\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:9.437500953674316\n",
      "Validation RMSE:8.940452575683594\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:9.436383247375488\n",
      "Validation RMSE:8.918563842773438\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:9.435490608215332\n",
      "Validation RMSE:8.944417953491211\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:9.434625625610352\n",
      "Validation RMSE:8.937176704406738\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:9.433502197265625\n",
      "Validation RMSE:8.931047439575195\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:9.432745933532715\n",
      "Validation RMSE:8.965511322021484\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:9.432025909423828\n",
      "Validation RMSE:8.981690406799316\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:9.431650161743164\n",
      "Validation RMSE:8.953682899475098\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:9.430529594421387\n",
      "Validation RMSE:8.95248031616211\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:9.42982006072998\n",
      "Validation RMSE:8.94790267944336\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:9.429402351379395\n",
      "Validation RMSE:8.950533866882324\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:9.428640365600586\n",
      "Validation RMSE:8.942475318908691\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:9.4279146194458\n",
      "Validation RMSE:8.937728881835938\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:9.427541732788086\n",
      "Validation RMSE:8.936749458312988\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:9.427384376525879\n",
      "Validation RMSE:8.930008888244629\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:9.426214218139648\n",
      "Validation RMSE:8.943934440612793\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:9.425341606140137\n",
      "Validation RMSE:8.969378471374512\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:9.42435359954834\n",
      "Validation RMSE:8.927803039550781\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:9.423765182495117\n",
      "Validation RMSE:8.934747695922852\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:9.42358112335205\n",
      "Validation RMSE:8.926740646362305\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:9.422916412353516\n",
      "Validation RMSE:8.937604904174805\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:9.422321319580078\n",
      "Validation RMSE:8.950362205505371\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:9.421576499938965\n",
      "Validation RMSE:8.937342643737793\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:9.42117977142334\n",
      "Validation RMSE:8.937396049499512\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:9.420302391052246\n",
      "Validation RMSE:8.947690963745117\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:9.419855117797852\n",
      "Validation RMSE:8.937209129333496\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:9.419076919555664\n",
      "Validation RMSE:8.950927734375\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:9.419005393981934\n",
      "Validation RMSE:8.938613891601562\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:9.418091773986816\n",
      "Validation RMSE:8.932039260864258\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:9.41727066040039\n",
      "Validation RMSE:8.929647445678711\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:9.41683578491211\n",
      "Validation RMSE:8.945098876953125\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:9.416057586669922\n",
      "Validation RMSE:8.932705879211426\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:9.41563892364502\n",
      "Validation RMSE:8.934126853942871\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:9.414916038513184\n",
      "Validation RMSE:8.944198608398438\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:9.414618492126465\n",
      "Validation RMSE:8.920557022094727\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:9.413972854614258\n",
      "Validation RMSE:8.921518325805664\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:9.413314819335938\n",
      "Validation RMSE:8.913100242614746\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:9.412805557250977\n",
      "Validation RMSE:8.943999290466309\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:9.412562370300293\n",
      "Validation RMSE:8.943521499633789\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:9.412032127380371\n",
      "Validation RMSE:8.948431968688965\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:9.411140441894531\n",
      "Validation RMSE:8.932936668395996\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:9.41069221496582\n",
      "Validation RMSE:8.970512390136719\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:9.410374641418457\n",
      "Validation RMSE:8.949719429016113\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:9.40991497039795\n",
      "Validation RMSE:8.9512300491333\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:9.409076690673828\n",
      "Validation RMSE:8.920042037963867\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:9.408940315246582\n",
      "Validation RMSE:8.958592414855957\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:9.408514976501465\n",
      "Validation RMSE:8.932936668395996\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:9.408246994018555\n",
      "Validation RMSE:8.911218643188477\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:9.407756805419922\n",
      "Validation RMSE:8.922525405883789\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:9.407278060913086\n",
      "Validation RMSE:8.962455749511719\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:9.406447410583496\n",
      "Validation RMSE:8.92180061340332\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:9.406044960021973\n",
      "Validation RMSE:8.919717788696289\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:9.40566635131836\n",
      "Validation RMSE:8.920488357543945\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:9.405265808105469\n",
      "Validation RMSE:8.927695274353027\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:9.40454387664795\n",
      "Validation RMSE:8.947123527526855\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:9.404507637023926\n",
      "Validation RMSE:8.92195987701416\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:9.403886795043945\n",
      "Validation RMSE:8.944294929504395\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:9.403719902038574\n",
      "Validation RMSE:8.93222427368164\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:9.403266906738281\n",
      "Validation RMSE:8.911911010742188\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:9.402812957763672\n",
      "Validation RMSE:8.935547828674316\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:9.402318000793457\n",
      "Validation RMSE:8.936861038208008\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:9.401983261108398\n",
      "Validation RMSE:8.924224853515625\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:9.401677131652832\n",
      "Validation RMSE:8.92221450805664\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:9.401193618774414\n",
      "Validation RMSE:8.958505630493164\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:9.400798797607422\n",
      "Validation RMSE:8.910906791687012\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:9.400508880615234\n",
      "Validation RMSE:8.948989868164062\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:9.400005340576172\n",
      "Validation RMSE:8.9282865524292\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:9.39940071105957\n",
      "Validation RMSE:8.929747581481934\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:9.398857116699219\n",
      "Validation RMSE:8.926580429077148\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:9.398738861083984\n",
      "Validation RMSE:8.922689437866211\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:9.39863395690918\n",
      "Validation RMSE:8.93499755859375\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:9.39826488494873\n",
      "Validation RMSE:8.961288452148438\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:9.397788047790527\n",
      "Validation RMSE:8.935632705688477\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:9.39720344543457\n",
      "Validation RMSE:8.937623023986816\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:9.396903038024902\n",
      "Validation RMSE:8.929304122924805\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:9.396602630615234\n",
      "Validation RMSE:8.931440353393555\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:9.396400451660156\n",
      "Validation RMSE:8.924112319946289\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:9.395834922790527\n",
      "Validation RMSE:8.935343742370605\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:9.395496368408203\n",
      "Validation RMSE:8.934446334838867\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:9.395130157470703\n",
      "Validation RMSE:8.918596267700195\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:9.394881248474121\n",
      "Validation RMSE:8.954313278198242\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:9.39457893371582\n",
      "Validation RMSE:8.934943199157715\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:9.394070625305176\n",
      "Validation RMSE:8.925558090209961\n",
      "=====================================\n",
      "Epoch:45 Batch:18900\n",
      "Training RMSE:9.393622398376465\n",
      "Validation RMSE:9.000204086303711\n",
      "=====================================\n",
      "Epoch:45 Batch:19000\n",
      "Training RMSE:9.393167495727539\n",
      "Validation RMSE:8.92138671875\n",
      "=====================================\n",
      "Epoch:45 Batch:19100\n",
      "Training RMSE:9.392939567565918\n",
      "Validation RMSE:8.94577407836914\n",
      "=====================================\n",
      "Epoch:45 Batch:19200\n",
      "Training RMSE:9.39273452758789\n",
      "Validation RMSE:8.90844440460205\n",
      "=====================================\n",
      "Epoch:46 Batch:19300\n",
      "Training RMSE:9.392406463623047\n",
      "Validation RMSE:8.944344520568848\n",
      "=====================================\n",
      "Epoch:46 Batch:19400\n",
      "Training RMSE:9.392207145690918\n",
      "Validation RMSE:8.937580108642578\n",
      "=====================================\n",
      "Epoch:46 Batch:19500\n",
      "Training RMSE:9.391847610473633\n",
      "Validation RMSE:8.905933380126953\n",
      "=====================================\n",
      "Epoch:46 Batch:19600\n",
      "Training RMSE:9.391234397888184\n",
      "Validation RMSE:8.92577838897705\n",
      "=====================================\n",
      "Epoch:47 Batch:19700\n",
      "Training RMSE:9.391081809997559\n",
      "Validation RMSE:8.92424488067627\n",
      "=====================================\n",
      "Epoch:47 Batch:19800\n",
      "Training RMSE:9.390738487243652\n",
      "Validation RMSE:8.922005653381348\n",
      "=====================================\n",
      "Epoch:47 Batch:19900\n",
      "Training RMSE:9.390556335449219\n",
      "Validation RMSE:8.946697235107422\n",
      "=====================================\n",
      "Epoch:47 Batch:20000\n",
      "Training RMSE:9.390191078186035\n",
      "Validation RMSE:8.928262710571289\n",
      "=====================================\n",
      "Epoch:48 Batch:20100\n",
      "Training RMSE:9.389702796936035\n",
      "Validation RMSE:8.933450698852539\n",
      "=====================================\n",
      "Epoch:48 Batch:20200\n",
      "Training RMSE:9.389521598815918\n",
      "Validation RMSE:8.914904594421387\n",
      "=====================================\n",
      "Epoch:48 Batch:20300\n",
      "Training RMSE:9.389220237731934\n",
      "Validation RMSE:8.937440872192383\n",
      "=====================================\n",
      "Epoch:48 Batch:20400\n",
      "Training RMSE:9.389200210571289\n",
      "Validation RMSE:8.927460670471191\n",
      "=====================================\n",
      "Epoch:49 Batch:20500\n",
      "Training RMSE:9.38876724243164\n",
      "Validation RMSE:8.913261413574219\n",
      "=====================================\n",
      "Epoch:49 Batch:20600\n",
      "Training RMSE:9.38845157623291\n",
      "Validation RMSE:8.94112491607666\n",
      "=====================================\n",
      "Epoch:49 Batch:20700\n",
      "Training RMSE:9.388071060180664\n",
      "Validation RMSE:8.935718536376953\n",
      "=====================================\n",
      "Epoch:49 Batch:20800\n",
      "Training RMSE:9.388004302978516\n",
      "Validation RMSE:8.952305793762207\n",
      "=====================================\n",
      "Epoch:49 Batch:20900\n",
      "Training RMSE:9.387609481811523\n",
      "Validation RMSE:8.933420181274414\n",
      "=====================================\n",
      "Epoch:50 Batch:21000\n",
      "Training RMSE:9.38729190826416\n",
      "Validation RMSE:8.931963920593262\n",
      "=====================================\n",
      "Epoch:50 Batch:21100\n",
      "Training RMSE:9.387105941772461\n",
      "Validation RMSE:8.931046485900879\n",
      "=====================================\n",
      "Epoch:50 Batch:21200\n",
      "Training RMSE:9.386966705322266\n",
      "Validation RMSE:8.932442665100098\n",
      "=====================================\n",
      "Epoch:50 Batch:21300\n",
      "Training RMSE:9.386527061462402\n",
      "Validation RMSE:8.957381248474121\n",
      "=====================================\n",
      "Epoch:51 Batch:21400\n",
      "Training RMSE:9.386293411254883\n",
      "Validation RMSE:8.92420482635498\n",
      "=====================================\n",
      "Epoch:51 Batch:21500\n",
      "Training RMSE:9.38582706451416\n",
      "Validation RMSE:8.929716110229492\n",
      "=====================================\n",
      "Epoch:51 Batch:21600\n",
      "Training RMSE:9.38596248626709\n",
      "Validation RMSE:8.936285972595215\n",
      "=====================================\n",
      "Epoch:51 Batch:21700\n",
      "Training RMSE:9.385575294494629\n",
      "Validation RMSE:8.919900894165039\n",
      "=====================================\n",
      "Epoch:52 Batch:21800\n",
      "Training RMSE:9.385340690612793\n",
      "Validation RMSE:8.92533016204834\n",
      "=====================================\n",
      "Epoch:52 Batch:21900\n",
      "Training RMSE:9.385454177856445\n",
      "Validation RMSE:8.926386833190918\n",
      "=====================================\n",
      "Epoch:52 Batch:22000\n",
      "Training RMSE:9.385247230529785\n",
      "Validation RMSE:8.916407585144043\n",
      "=====================================\n",
      "Epoch:52 Batch:22100\n",
      "Training RMSE:9.384521484375\n",
      "Validation RMSE:8.92688274383545\n",
      "=====================================\n",
      "Epoch:53 Batch:22200\n",
      "Training RMSE:9.384200096130371\n",
      "Validation RMSE:8.970429420471191\n",
      "=====================================\n",
      "Epoch:53 Batch:22300\n",
      "Training RMSE:9.383687973022461\n",
      "Validation RMSE:8.941433906555176\n",
      "=====================================\n",
      "Epoch:53 Batch:22400\n",
      "Training RMSE:9.383685111999512\n",
      "Validation RMSE:8.920832633972168\n",
      "=====================================\n",
      "Epoch:53 Batch:22500\n",
      "Training RMSE:9.383403778076172\n",
      "Validation RMSE:8.92260456085205\n",
      "=====================================\n",
      "Epoch:54 Batch:22600\n",
      "Training RMSE:9.383237838745117\n",
      "Validation RMSE:8.939886093139648\n",
      "=====================================\n",
      "Epoch:54 Batch:22700\n",
      "Training RMSE:9.383000373840332\n",
      "Validation RMSE:8.941814422607422\n",
      "=====================================\n",
      "Epoch:54 Batch:22800\n",
      "Training RMSE:9.382479667663574\n",
      "Validation RMSE:8.935281753540039\n",
      "=====================================\n",
      "Epoch:54 Batch:22900\n",
      "Training RMSE:9.382328987121582\n",
      "Validation RMSE:8.93979263305664\n",
      "=====================================\n",
      "Epoch:55 Batch:23000\n",
      "Training RMSE:9.382364273071289\n",
      "Validation RMSE:8.926980018615723\n",
      "=====================================\n",
      "Epoch:55 Batch:23100\n",
      "Training RMSE:9.38211441040039\n",
      "Validation RMSE:8.954116821289062\n",
      "=====================================\n",
      "Epoch:55 Batch:23200\n",
      "Training RMSE:9.381890296936035\n",
      "Validation RMSE:8.930423736572266\n",
      "=====================================\n",
      "Epoch:55 Batch:23300\n",
      "Training RMSE:9.381654739379883\n",
      "Validation RMSE:8.926244735717773\n",
      "=====================================\n",
      "Epoch:55 Batch:23400\n",
      "Training RMSE:9.381271362304688\n",
      "Validation RMSE:8.922404289245605\n",
      "=====================================\n",
      "Epoch:56 Batch:23500\n",
      "Training RMSE:9.381163597106934\n",
      "Validation RMSE:8.93337631225586\n",
      "=====================================\n",
      "Epoch:56 Batch:23600\n",
      "Training RMSE:9.380919456481934\n",
      "Validation RMSE:8.932604789733887\n",
      "=====================================\n",
      "Epoch:56 Batch:23700\n",
      "Training RMSE:9.380743980407715\n",
      "Validation RMSE:8.9315185546875\n",
      "=====================================\n",
      "Epoch:56 Batch:23800\n",
      "Training RMSE:9.380574226379395\n",
      "Validation RMSE:8.943177223205566\n",
      "=====================================\n",
      "Epoch:57 Batch:23900\n",
      "Training RMSE:9.380245208740234\n",
      "Validation RMSE:8.927757263183594\n",
      "=====================================\n",
      "Epoch:57 Batch:24000\n",
      "Training RMSE:9.380189895629883\n",
      "Validation RMSE:8.943490982055664\n",
      "=====================================\n",
      "Epoch:57 Batch:24100\n",
      "Training RMSE:9.379981994628906\n",
      "Validation RMSE:8.93276596069336\n",
      "=====================================\n",
      "Epoch:57 Batch:24200\n",
      "Training RMSE:9.379731178283691\n",
      "Validation RMSE:8.937240600585938\n",
      "=====================================\n",
      "Epoch:58 Batch:24300\n",
      "Training RMSE:9.379539489746094\n",
      "Validation RMSE:8.927650451660156\n",
      "=====================================\n",
      "Epoch:58 Batch:24400\n",
      "Training RMSE:9.379197120666504\n",
      "Validation RMSE:8.929022789001465\n",
      "=====================================\n",
      "Epoch:58 Batch:24500\n",
      "Training RMSE:9.379067420959473\n",
      "Validation RMSE:8.925372123718262\n",
      "=====================================\n",
      "Epoch:58 Batch:24600\n",
      "Training RMSE:9.378959655761719\n",
      "Validation RMSE:8.949069023132324\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:19500 best loss:8.905933380126953\n",
      "H = 20 Test_RMSE = 9.007262229919434\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 20\n",
    "\n",
    "Q6_model = MyMLP()\n",
    "model = Q6_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "model = Q6_model.train(device,optimizer)\n",
    "test_RMSE = Q6_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "csLyPrWJKAwA",
   "metadata": {
    "id": "csLyPrWJKAwA"
   },
   "source": [
    "H = 45時，Test RMSE數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LjLx4OXpJ83f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjLx4OXpJ83f",
    "outputId": "5bf7ec02-3604-47fc-ba0c-62982d5594c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.689420700073242\n",
      "Validation RMSE:9.80948257446289\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.279781341552734\n",
      "Validation RMSE:9.320520401000977\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:10.049967765808105\n",
      "Validation RMSE:9.13329792022705\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:9.915108680725098\n",
      "Validation RMSE:9.097502708435059\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.81801986694336\n",
      "Validation RMSE:9.017497062683105\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.746317863464355\n",
      "Validation RMSE:8.994589805603027\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.69255256652832\n",
      "Validation RMSE:8.974111557006836\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.646522521972656\n",
      "Validation RMSE:8.937273025512695\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.604504585266113\n",
      "Validation RMSE:8.917245864868164\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.575363159179688\n",
      "Validation RMSE:8.915721893310547\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.54417610168457\n",
      "Validation RMSE:8.928070068359375\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.520062446594238\n",
      "Validation RMSE:8.890727043151855\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.501867294311523\n",
      "Validation RMSE:8.873560905456543\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.484068870544434\n",
      "Validation RMSE:8.873456954956055\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.462044715881348\n",
      "Validation RMSE:8.852945327758789\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.446159362792969\n",
      "Validation RMSE:8.89948558807373\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.428425788879395\n",
      "Validation RMSE:8.87378215789795\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.415369987487793\n",
      "Validation RMSE:8.892274856567383\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.402270317077637\n",
      "Validation RMSE:8.831151962280273\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.391044616699219\n",
      "Validation RMSE:8.828910827636719\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.382497787475586\n",
      "Validation RMSE:8.842022895812988\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.372295379638672\n",
      "Validation RMSE:8.836862564086914\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.365267753601074\n",
      "Validation RMSE:8.824657440185547\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.355234146118164\n",
      "Validation RMSE:8.860770225524902\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.346931457519531\n",
      "Validation RMSE:8.807087898254395\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.340600967407227\n",
      "Validation RMSE:8.817444801330566\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.330704689025879\n",
      "Validation RMSE:8.819151878356934\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:9.323610305786133\n",
      "Validation RMSE:8.817938804626465\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:9.31806468963623\n",
      "Validation RMSE:8.8089017868042\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:9.31270980834961\n",
      "Validation RMSE:8.804342269897461\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:9.308295249938965\n",
      "Validation RMSE:8.832894325256348\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:9.302796363830566\n",
      "Validation RMSE:8.805315017700195\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:9.296562194824219\n",
      "Validation RMSE:8.808536529541016\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:9.29050064086914\n",
      "Validation RMSE:8.791812896728516\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:9.284753799438477\n",
      "Validation RMSE:8.826652526855469\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:9.281231880187988\n",
      "Validation RMSE:8.801348686218262\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:9.27613353729248\n",
      "Validation RMSE:8.827783584594727\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:9.273050308227539\n",
      "Validation RMSE:8.788392066955566\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:9.269158363342285\n",
      "Validation RMSE:8.800559043884277\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:9.263930320739746\n",
      "Validation RMSE:8.807701110839844\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:9.261380195617676\n",
      "Validation RMSE:8.806861877441406\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:9.257664680480957\n",
      "Validation RMSE:8.804113388061523\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:9.25284481048584\n",
      "Validation RMSE:8.809895515441895\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:9.250066757202148\n",
      "Validation RMSE:8.803218841552734\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:9.246757507324219\n",
      "Validation RMSE:8.799707412719727\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:9.244029998779297\n",
      "Validation RMSE:8.804811477661133\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:9.240862846374512\n",
      "Validation RMSE:8.78275203704834\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:9.237626075744629\n",
      "Validation RMSE:8.812017440795898\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:9.2349853515625\n",
      "Validation RMSE:8.787805557250977\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:9.232439041137695\n",
      "Validation RMSE:8.775568962097168\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:9.228731155395508\n",
      "Validation RMSE:8.780556678771973\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:9.225027084350586\n",
      "Validation RMSE:8.764023780822754\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:9.22364330291748\n",
      "Validation RMSE:8.779363632202148\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:9.221078872680664\n",
      "Validation RMSE:8.786428451538086\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:9.218072891235352\n",
      "Validation RMSE:8.783858299255371\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:9.215678215026855\n",
      "Validation RMSE:8.786591529846191\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:9.214231491088867\n",
      "Validation RMSE:8.77532958984375\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:9.21200942993164\n",
      "Validation RMSE:8.79883098602295\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:9.209731101989746\n",
      "Validation RMSE:8.77005672454834\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:9.207633972167969\n",
      "Validation RMSE:8.819266319274902\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:9.205733299255371\n",
      "Validation RMSE:8.792241096496582\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:9.20350456237793\n",
      "Validation RMSE:8.785262107849121\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:9.202059745788574\n",
      "Validation RMSE:8.770901679992676\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:9.199533462524414\n",
      "Validation RMSE:8.761283874511719\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:9.19887638092041\n",
      "Validation RMSE:8.771632194519043\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:9.19633960723877\n",
      "Validation RMSE:8.78835391998291\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:9.194608688354492\n",
      "Validation RMSE:8.782243728637695\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:9.192319869995117\n",
      "Validation RMSE:8.768250465393066\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:9.19091510772705\n",
      "Validation RMSE:8.786270141601562\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:9.188651084899902\n",
      "Validation RMSE:8.787344932556152\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:9.187529563903809\n",
      "Validation RMSE:8.782208442687988\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:9.186314582824707\n",
      "Validation RMSE:8.789999961853027\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:9.184197425842285\n",
      "Validation RMSE:8.757187843322754\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:9.18338394165039\n",
      "Validation RMSE:8.749736785888672\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:9.18134880065918\n",
      "Validation RMSE:8.761407852172852\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:9.179844856262207\n",
      "Validation RMSE:8.77033805847168\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:9.17834186553955\n",
      "Validation RMSE:8.793082237243652\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:9.177217483520508\n",
      "Validation RMSE:8.773308753967285\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:9.175366401672363\n",
      "Validation RMSE:8.808389663696289\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:9.173654556274414\n",
      "Validation RMSE:8.754817008972168\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:9.17270565032959\n",
      "Validation RMSE:8.786117553710938\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:9.171704292297363\n",
      "Validation RMSE:8.75363540649414\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:9.17033863067627\n",
      "Validation RMSE:8.76478385925293\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:9.16895580291748\n",
      "Validation RMSE:8.754376411437988\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:9.167840957641602\n",
      "Validation RMSE:8.736532211303711\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:9.16688346862793\n",
      "Validation RMSE:8.774453163146973\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:9.16502571105957\n",
      "Validation RMSE:8.73630142211914\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:9.163843154907227\n",
      "Validation RMSE:8.76517105102539\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:9.162901878356934\n",
      "Validation RMSE:8.76357650756836\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:9.161752700805664\n",
      "Validation RMSE:8.744619369506836\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:9.160135269165039\n",
      "Validation RMSE:8.747437477111816\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:9.159645080566406\n",
      "Validation RMSE:8.758692741394043\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:9.158596992492676\n",
      "Validation RMSE:8.742918968200684\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:9.158056259155273\n",
      "Validation RMSE:8.742841720581055\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:9.156963348388672\n",
      "Validation RMSE:8.763679504394531\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:9.155275344848633\n",
      "Validation RMSE:8.744518280029297\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:9.15457534790039\n",
      "Validation RMSE:8.757347106933594\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:9.153497695922852\n",
      "Validation RMSE:8.761887550354004\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:9.152609825134277\n",
      "Validation RMSE:8.777395248413086\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:9.151372909545898\n",
      "Validation RMSE:8.731080055236816\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:9.15076732635498\n",
      "Validation RMSE:8.74913501739502\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:9.149904251098633\n",
      "Validation RMSE:8.73095703125\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:9.14882755279541\n",
      "Validation RMSE:8.763075828552246\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:9.147578239440918\n",
      "Validation RMSE:8.762874603271484\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:9.146906852722168\n",
      "Validation RMSE:8.729680061340332\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:9.145936012268066\n",
      "Validation RMSE:8.769999504089355\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:9.145345687866211\n",
      "Validation RMSE:8.75154972076416\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:9.144055366516113\n",
      "Validation RMSE:8.763326644897461\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:9.143068313598633\n",
      "Validation RMSE:8.743019104003906\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:9.142265319824219\n",
      "Validation RMSE:8.755616188049316\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:9.141549110412598\n",
      "Validation RMSE:8.75850772857666\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:9.14046859741211\n",
      "Validation RMSE:8.745591163635254\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:9.139632225036621\n",
      "Validation RMSE:8.733402252197266\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:9.13912296295166\n",
      "Validation RMSE:8.750360488891602\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:9.137869834899902\n",
      "Validation RMSE:8.751906394958496\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:9.137263298034668\n",
      "Validation RMSE:8.732661247253418\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:9.136566162109375\n",
      "Validation RMSE:8.74653434753418\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:9.135530471801758\n",
      "Validation RMSE:8.741490364074707\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:9.13497543334961\n",
      "Validation RMSE:8.752387046813965\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:9.134586334228516\n",
      "Validation RMSE:8.760485649108887\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:9.133635520935059\n",
      "Validation RMSE:8.724858283996582\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:9.132896423339844\n",
      "Validation RMSE:8.725345611572266\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:9.132219314575195\n",
      "Validation RMSE:8.757152557373047\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:9.131414413452148\n",
      "Validation RMSE:8.764642715454102\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:9.130840301513672\n",
      "Validation RMSE:8.731793403625488\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:9.13015079498291\n",
      "Validation RMSE:8.74765682220459\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:9.129488945007324\n",
      "Validation RMSE:8.764439582824707\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:9.128868103027344\n",
      "Validation RMSE:8.73849105834961\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:9.128181457519531\n",
      "Validation RMSE:8.728809356689453\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:9.12772274017334\n",
      "Validation RMSE:8.749909400939941\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:9.127077102661133\n",
      "Validation RMSE:8.761347770690918\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:9.126774787902832\n",
      "Validation RMSE:8.736041069030762\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:9.126180648803711\n",
      "Validation RMSE:8.758650779724121\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:9.125299453735352\n",
      "Validation RMSE:8.755085945129395\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:9.124611854553223\n",
      "Validation RMSE:8.762832641601562\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:9.123868942260742\n",
      "Validation RMSE:8.772913932800293\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:9.123457908630371\n",
      "Validation RMSE:8.73036003112793\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:9.122577667236328\n",
      "Validation RMSE:8.754316329956055\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:9.1217679977417\n",
      "Validation RMSE:8.743419647216797\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:9.121014595031738\n",
      "Validation RMSE:8.764680862426758\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:9.120575904846191\n",
      "Validation RMSE:8.752233505249023\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:9.120414733886719\n",
      "Validation RMSE:8.728429794311523\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:9.119489669799805\n",
      "Validation RMSE:8.729217529296875\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:9.119319915771484\n",
      "Validation RMSE:8.749480247497559\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:9.119006156921387\n",
      "Validation RMSE:8.763830184936523\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:9.118294715881348\n",
      "Validation RMSE:8.752328872680664\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:9.117464065551758\n",
      "Validation RMSE:8.736979484558105\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:9.11689567565918\n",
      "Validation RMSE:8.73991584777832\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:9.11638069152832\n",
      "Validation RMSE:8.737529754638672\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:9.11568546295166\n",
      "Validation RMSE:8.745460510253906\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:9.115396499633789\n",
      "Validation RMSE:8.76470947265625\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:9.114960670471191\n",
      "Validation RMSE:8.772685050964355\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:9.114579200744629\n",
      "Validation RMSE:8.722696304321289\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:9.11421012878418\n",
      "Validation RMSE:8.749979972839355\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:9.113639831542969\n",
      "Validation RMSE:8.721217155456543\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:9.113399505615234\n",
      "Validation RMSE:8.733738899230957\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:9.112893104553223\n",
      "Validation RMSE:8.740157127380371\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:9.112277030944824\n",
      "Validation RMSE:8.765584945678711\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:9.111701011657715\n",
      "Validation RMSE:8.742853164672852\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:9.111352920532227\n",
      "Validation RMSE:8.750848770141602\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:9.110608100891113\n",
      "Validation RMSE:8.766666412353516\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:9.109991073608398\n",
      "Validation RMSE:8.748613357543945\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:9.109640121459961\n",
      "Validation RMSE:8.760425567626953\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:9.109383583068848\n",
      "Validation RMSE:8.731821060180664\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:9.108872413635254\n",
      "Validation RMSE:8.763789176940918\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:9.108240127563477\n",
      "Validation RMSE:8.758562088012695\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:9.107793807983398\n",
      "Validation RMSE:8.741226196289062\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:9.107349395751953\n",
      "Validation RMSE:8.730158805847168\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:9.106756210327148\n",
      "Validation RMSE:8.726861953735352\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:9.106303215026855\n",
      "Validation RMSE:8.768933296203613\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:9.10617446899414\n",
      "Validation RMSE:8.747330665588379\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:9.105612754821777\n",
      "Validation RMSE:8.742321014404297\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:9.105178833007812\n",
      "Validation RMSE:8.736807823181152\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:9.10499382019043\n",
      "Validation RMSE:8.743627548217773\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:9.104440689086914\n",
      "Validation RMSE:8.735044479370117\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:9.10392951965332\n",
      "Validation RMSE:8.751871109008789\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:9.103692054748535\n",
      "Validation RMSE:8.735331535339355\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:9.103020668029785\n",
      "Validation RMSE:8.735931396484375\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:9.102762222290039\n",
      "Validation RMSE:8.758435249328613\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:9.10244083404541\n",
      "Validation RMSE:8.730291366577148\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:9.102118492126465\n",
      "Validation RMSE:8.7479829788208\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:9.101560592651367\n",
      "Validation RMSE:8.75916576385498\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:9.101150512695312\n",
      "Validation RMSE:8.73213005065918\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:9.10084056854248\n",
      "Validation RMSE:8.74012565612793\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:9.100492477416992\n",
      "Validation RMSE:8.741419792175293\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:9.100176811218262\n",
      "Validation RMSE:8.740077018737793\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:9.099862098693848\n",
      "Validation RMSE:8.761696815490723\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:9.099503517150879\n",
      "Validation RMSE:8.761313438415527\n",
      "=====================================\n",
      "Epoch:45 Batch:18900\n",
      "Training RMSE:9.098931312561035\n",
      "Validation RMSE:8.749469757080078\n",
      "=====================================\n",
      "Epoch:45 Batch:19000\n",
      "Training RMSE:9.098587036132812\n",
      "Validation RMSE:8.739134788513184\n",
      "=====================================\n",
      "Epoch:45 Batch:19100\n",
      "Training RMSE:9.098262786865234\n",
      "Validation RMSE:8.743135452270508\n",
      "=====================================\n",
      "Epoch:45 Batch:19200\n",
      "Training RMSE:9.097943305969238\n",
      "Validation RMSE:8.731436729431152\n",
      "=====================================\n",
      "Epoch:46 Batch:19300\n",
      "Training RMSE:9.097530364990234\n",
      "Validation RMSE:8.749882698059082\n",
      "=====================================\n",
      "Epoch:46 Batch:19400\n",
      "Training RMSE:9.097086906433105\n",
      "Validation RMSE:8.743425369262695\n",
      "=====================================\n",
      "Epoch:46 Batch:19500\n",
      "Training RMSE:9.096785545349121\n",
      "Validation RMSE:8.747575759887695\n",
      "=====================================\n",
      "Epoch:46 Batch:19600\n",
      "Training RMSE:9.096471786499023\n",
      "Validation RMSE:8.75089168548584\n",
      "=====================================\n",
      "Epoch:47 Batch:19700\n",
      "Training RMSE:9.096146583557129\n",
      "Validation RMSE:8.740966796875\n",
      "=====================================\n",
      "Epoch:47 Batch:19800\n",
      "Training RMSE:9.095881462097168\n",
      "Validation RMSE:8.741991996765137\n",
      "=====================================\n",
      "Epoch:47 Batch:19900\n",
      "Training RMSE:9.095635414123535\n",
      "Validation RMSE:8.752681732177734\n",
      "=====================================\n",
      "Epoch:47 Batch:20000\n",
      "Training RMSE:9.094680786132812\n",
      "Validation RMSE:8.729660987854004\n",
      "=====================================\n",
      "Epoch:48 Batch:20100\n",
      "Training RMSE:9.094560623168945\n",
      "Validation RMSE:8.722079277038574\n",
      "=====================================\n",
      "Epoch:48 Batch:20200\n",
      "Training RMSE:9.09401798248291\n",
      "Validation RMSE:8.700304985046387\n",
      "=====================================\n",
      "Epoch:48 Batch:20300\n",
      "Training RMSE:9.093924522399902\n",
      "Validation RMSE:8.734989166259766\n",
      "=====================================\n",
      "Epoch:48 Batch:20400\n",
      "Training RMSE:9.093454360961914\n",
      "Validation RMSE:8.7512845993042\n",
      "=====================================\n",
      "Epoch:49 Batch:20500\n",
      "Training RMSE:9.093201637268066\n",
      "Validation RMSE:8.766582489013672\n",
      "=====================================\n",
      "Epoch:49 Batch:20600\n",
      "Training RMSE:9.093005180358887\n",
      "Validation RMSE:8.771533966064453\n",
      "=====================================\n",
      "Epoch:49 Batch:20700\n",
      "Training RMSE:9.092567443847656\n",
      "Validation RMSE:8.762711524963379\n",
      "=====================================\n",
      "Epoch:49 Batch:20800\n",
      "Training RMSE:9.092162132263184\n",
      "Validation RMSE:8.73330020904541\n",
      "=====================================\n",
      "Epoch:49 Batch:20900\n",
      "Training RMSE:9.091883659362793\n",
      "Validation RMSE:8.731988906860352\n",
      "=====================================\n",
      "Epoch:50 Batch:21000\n",
      "Training RMSE:9.09150505065918\n",
      "Validation RMSE:8.744199752807617\n",
      "=====================================\n",
      "Epoch:50 Batch:21100\n",
      "Training RMSE:9.091204643249512\n",
      "Validation RMSE:8.735130310058594\n",
      "=====================================\n",
      "Epoch:50 Batch:21200\n",
      "Training RMSE:9.09095573425293\n",
      "Validation RMSE:8.718232154846191\n",
      "=====================================\n",
      "Epoch:50 Batch:21300\n",
      "Training RMSE:9.090655326843262\n",
      "Validation RMSE:8.742366790771484\n",
      "=====================================\n",
      "Epoch:51 Batch:21400\n",
      "Training RMSE:9.090311050415039\n",
      "Validation RMSE:8.753706932067871\n",
      "=====================================\n",
      "Epoch:51 Batch:21500\n",
      "Training RMSE:9.089923858642578\n",
      "Validation RMSE:8.752516746520996\n",
      "=====================================\n",
      "Epoch:51 Batch:21600\n",
      "Training RMSE:9.089573860168457\n",
      "Validation RMSE:8.73293685913086\n",
      "=====================================\n",
      "Epoch:51 Batch:21700\n",
      "Training RMSE:9.089455604553223\n",
      "Validation RMSE:8.739242553710938\n",
      "=====================================\n",
      "Epoch:52 Batch:21800\n",
      "Training RMSE:9.089279174804688\n",
      "Validation RMSE:8.752057075500488\n",
      "=====================================\n",
      "Epoch:52 Batch:21900\n",
      "Training RMSE:9.089031219482422\n",
      "Validation RMSE:8.734563827514648\n",
      "=====================================\n",
      "Epoch:52 Batch:22000\n",
      "Training RMSE:9.088547706604004\n",
      "Validation RMSE:8.716699600219727\n",
      "=====================================\n",
      "Epoch:52 Batch:22100\n",
      "Training RMSE:9.088225364685059\n",
      "Validation RMSE:8.736024856567383\n",
      "=====================================\n",
      "Epoch:53 Batch:22200\n",
      "Training RMSE:9.087867736816406\n",
      "Validation RMSE:8.74528694152832\n",
      "=====================================\n",
      "Epoch:53 Batch:22300\n",
      "Training RMSE:9.087769508361816\n",
      "Validation RMSE:8.722427368164062\n",
      "=====================================\n",
      "Epoch:53 Batch:22400\n",
      "Training RMSE:9.087385177612305\n",
      "Validation RMSE:8.718052864074707\n",
      "=====================================\n",
      "Epoch:53 Batch:22500\n",
      "Training RMSE:9.087162971496582\n",
      "Validation RMSE:8.736135482788086\n",
      "=====================================\n",
      "Epoch:54 Batch:22600\n",
      "Training RMSE:9.086870193481445\n",
      "Validation RMSE:8.722060203552246\n",
      "=====================================\n",
      "Epoch:54 Batch:22700\n",
      "Training RMSE:9.086501121520996\n",
      "Validation RMSE:8.721595764160156\n",
      "=====================================\n",
      "Epoch:54 Batch:22800\n",
      "Training RMSE:9.086247444152832\n",
      "Validation RMSE:8.73625373840332\n",
      "=====================================\n",
      "Epoch:54 Batch:22900\n",
      "Training RMSE:9.085750579833984\n",
      "Validation RMSE:8.7227144241333\n",
      "=====================================\n",
      "Epoch:55 Batch:23000\n",
      "Training RMSE:9.085692405700684\n",
      "Validation RMSE:8.721545219421387\n",
      "=====================================\n",
      "Epoch:55 Batch:23100\n",
      "Training RMSE:9.085456848144531\n",
      "Validation RMSE:8.726799011230469\n",
      "=====================================\n",
      "Epoch:55 Batch:23200\n",
      "Training RMSE:9.08497428894043\n",
      "Validation RMSE:8.740065574645996\n",
      "=====================================\n",
      "Epoch:55 Batch:23300\n",
      "Training RMSE:9.084692001342773\n",
      "Validation RMSE:8.73236083984375\n",
      "=====================================\n",
      "Epoch:55 Batch:23400\n",
      "Training RMSE:9.084686279296875\n",
      "Validation RMSE:8.716644287109375\n",
      "=====================================\n",
      "Epoch:56 Batch:23500\n",
      "Training RMSE:9.084394454956055\n",
      "Validation RMSE:8.74167251586914\n",
      "=====================================\n",
      "Epoch:56 Batch:23600\n",
      "Training RMSE:9.084161758422852\n",
      "Validation RMSE:8.72523021697998\n",
      "=====================================\n",
      "Epoch:56 Batch:23700\n",
      "Training RMSE:9.083895683288574\n",
      "Validation RMSE:8.746132850646973\n",
      "=====================================\n",
      "Epoch:56 Batch:23800\n",
      "Training RMSE:9.08354377746582\n",
      "Validation RMSE:8.734131813049316\n",
      "=====================================\n",
      "Epoch:57 Batch:23900\n",
      "Training RMSE:9.083189964294434\n",
      "Validation RMSE:8.727158546447754\n",
      "=====================================\n",
      "Epoch:57 Batch:24000\n",
      "Training RMSE:9.082953453063965\n",
      "Validation RMSE:8.720155715942383\n",
      "=====================================\n",
      "Epoch:57 Batch:24100\n",
      "Training RMSE:9.082664489746094\n",
      "Validation RMSE:8.747446060180664\n",
      "=====================================\n",
      "Epoch:57 Batch:24200\n",
      "Training RMSE:9.082473754882812\n",
      "Validation RMSE:8.732762336730957\n",
      "=====================================\n",
      "Epoch:58 Batch:24300\n",
      "Training RMSE:9.08217716217041\n",
      "Validation RMSE:8.723326683044434\n",
      "=====================================\n",
      "Epoch:58 Batch:24400\n",
      "Training RMSE:9.081955909729004\n",
      "Validation RMSE:8.697358131408691\n",
      "=====================================\n",
      "Epoch:58 Batch:24500\n",
      "Training RMSE:9.081774711608887\n",
      "Validation RMSE:8.743341445922852\n",
      "=====================================\n",
      "Epoch:58 Batch:24600\n",
      "Training RMSE:9.081722259521484\n",
      "Validation RMSE:8.729057312011719\n",
      "=====================================\n",
      "Epoch:59 Batch:24700\n",
      "Training RMSE:9.08119010925293\n",
      "Validation RMSE:8.750715255737305\n",
      "=====================================\n",
      "Epoch:59 Batch:24800\n",
      "Training RMSE:9.081079483032227\n",
      "Validation RMSE:8.751490592956543\n",
      "=====================================\n",
      "Epoch:59 Batch:24900\n",
      "Training RMSE:9.08077621459961\n",
      "Validation RMSE:8.732647895812988\n",
      "=====================================\n",
      "Epoch:59 Batch:25000\n",
      "Training RMSE:9.080589294433594\n",
      "Validation RMSE:8.712207794189453\n",
      "=====================================\n",
      "Epoch:60 Batch:25100\n",
      "Training RMSE:9.080350875854492\n",
      "Validation RMSE:8.716740608215332\n",
      "=====================================\n",
      "Epoch:60 Batch:25200\n",
      "Training RMSE:9.080132484436035\n",
      "Validation RMSE:8.72064208984375\n",
      "=====================================\n",
      "Epoch:60 Batch:25300\n",
      "Training RMSE:9.079792022705078\n",
      "Validation RMSE:8.729334831237793\n",
      "=====================================\n",
      "Epoch:60 Batch:25400\n",
      "Training RMSE:9.079446792602539\n",
      "Validation RMSE:8.729841232299805\n",
      "=====================================\n",
      "Epoch:61 Batch:25500\n",
      "Training RMSE:9.079246520996094\n",
      "Validation RMSE:8.72964096069336\n",
      "=====================================\n",
      "Epoch:61 Batch:25600\n",
      "Training RMSE:9.078993797302246\n",
      "Validation RMSE:8.754594802856445\n",
      "=====================================\n",
      "Epoch:61 Batch:25700\n",
      "Training RMSE:9.078954696655273\n",
      "Validation RMSE:8.731476783752441\n",
      "=====================================\n",
      "Epoch:61 Batch:25800\n",
      "Training RMSE:9.078779220581055\n",
      "Validation RMSE:8.723165512084961\n",
      "=====================================\n",
      "Epoch:61 Batch:25900\n",
      "Training RMSE:9.078446388244629\n",
      "Validation RMSE:8.733747482299805\n",
      "=====================================\n",
      "Epoch:62 Batch:26000\n",
      "Training RMSE:9.078250885009766\n",
      "Validation RMSE:8.715439796447754\n",
      "=====================================\n",
      "Epoch:62 Batch:26100\n",
      "Training RMSE:9.077977180480957\n",
      "Validation RMSE:8.741443634033203\n",
      "=====================================\n",
      "Epoch:62 Batch:26200\n",
      "Training RMSE:9.07762622833252\n",
      "Validation RMSE:8.742565155029297\n",
      "=====================================\n",
      "Epoch:62 Batch:26300\n",
      "Training RMSE:9.077404975891113\n",
      "Validation RMSE:8.728525161743164\n",
      "=====================================\n",
      "Epoch:63 Batch:26400\n",
      "Training RMSE:9.077073097229004\n",
      "Validation RMSE:8.734570503234863\n",
      "=====================================\n",
      "Epoch:63 Batch:26500\n",
      "Training RMSE:9.076841354370117\n",
      "Validation RMSE:8.737203598022461\n",
      "=====================================\n",
      "Epoch:63 Batch:26600\n",
      "Training RMSE:9.07654094696045\n",
      "Validation RMSE:8.740754127502441\n",
      "=====================================\n",
      "Epoch:63 Batch:26700\n",
      "Training RMSE:9.076489448547363\n",
      "Validation RMSE:8.748274803161621\n",
      "=====================================\n",
      "Epoch:64 Batch:26800\n",
      "Training RMSE:9.076193809509277\n",
      "Validation RMSE:8.71965217590332\n",
      "=====================================\n",
      "Epoch:64 Batch:26900\n",
      "Training RMSE:9.075933456420898\n",
      "Validation RMSE:8.738327980041504\n",
      "=====================================\n",
      "Epoch:64 Batch:27000\n",
      "Training RMSE:9.075616836547852\n",
      "Validation RMSE:8.767364501953125\n",
      "=====================================\n",
      "Epoch:64 Batch:27100\n",
      "Training RMSE:9.075663566589355\n",
      "Validation RMSE:8.749176979064941\n",
      "=====================================\n",
      "Epoch:65 Batch:27200\n",
      "Training RMSE:9.075425148010254\n",
      "Validation RMSE:8.729233741760254\n",
      "=====================================\n",
      "Epoch:65 Batch:27300\n",
      "Training RMSE:9.075085639953613\n",
      "Validation RMSE:8.716259002685547\n",
      "=====================================\n",
      "Epoch:65 Batch:27400\n",
      "Training RMSE:9.075045585632324\n",
      "Validation RMSE:8.724740028381348\n",
      "=====================================\n",
      "Epoch:65 Batch:27500\n",
      "Training RMSE:9.074801445007324\n",
      "Validation RMSE:8.722919464111328\n",
      "=====================================\n",
      "Epoch:66 Batch:27600\n",
      "Training RMSE:9.074435234069824\n",
      "Validation RMSE:8.761302947998047\n",
      "=====================================\n",
      "Epoch:66 Batch:27700\n",
      "Training RMSE:9.074165344238281\n",
      "Validation RMSE:8.723526000976562\n",
      "=====================================\n",
      "Epoch:66 Batch:27800\n",
      "Training RMSE:9.07396411895752\n",
      "Validation RMSE:8.744465827941895\n",
      "=====================================\n",
      "Epoch:66 Batch:27900\n",
      "Training RMSE:9.074044227600098\n",
      "Validation RMSE:8.729555130004883\n",
      "=====================================\n",
      "Epoch:66 Batch:28000\n",
      "Training RMSE:9.073699951171875\n",
      "Validation RMSE:8.75279712677002\n",
      "=====================================\n",
      "Epoch:67 Batch:28100\n",
      "Training RMSE:9.073508262634277\n",
      "Validation RMSE:8.727704048156738\n",
      "=====================================\n",
      "Epoch:67 Batch:28200\n",
      "Training RMSE:9.073474884033203\n",
      "Validation RMSE:8.732806205749512\n",
      "=====================================\n",
      "Epoch:67 Batch:28300\n",
      "Training RMSE:9.073205947875977\n",
      "Validation RMSE:8.723003387451172\n",
      "=====================================\n",
      "Epoch:67 Batch:28400\n",
      "Training RMSE:9.072809219360352\n",
      "Validation RMSE:8.720907211303711\n",
      "=====================================\n",
      "Epoch:68 Batch:28500\n",
      "Training RMSE:9.072690963745117\n",
      "Validation RMSE:8.741222381591797\n",
      "=====================================\n",
      "Epoch:68 Batch:28600\n",
      "Training RMSE:9.072409629821777\n",
      "Validation RMSE:8.742749214172363\n",
      "=====================================\n",
      "Epoch:68 Batch:28700\n",
      "Training RMSE:9.072193145751953\n",
      "Validation RMSE:8.723113059997559\n",
      "=====================================\n",
      "Epoch:68 Batch:28800\n",
      "Training RMSE:9.071953773498535\n",
      "Validation RMSE:8.726822853088379\n",
      "=====================================\n",
      "Epoch:69 Batch:28900\n",
      "Training RMSE:9.071720123291016\n",
      "Validation RMSE:8.723130226135254\n",
      "=====================================\n",
      "Epoch:69 Batch:29000\n",
      "Training RMSE:9.071517944335938\n",
      "Validation RMSE:8.769064903259277\n",
      "=====================================\n",
      "Epoch:69 Batch:29100\n",
      "Training RMSE:9.071531295776367\n",
      "Validation RMSE:8.7366304397583\n",
      "=====================================\n",
      "Epoch:69 Batch:29200\n",
      "Training RMSE:9.071460723876953\n",
      "Validation RMSE:8.74482250213623\n",
      "=====================================\n",
      "Epoch:70 Batch:29300\n",
      "Training RMSE:9.071206092834473\n",
      "Validation RMSE:8.725202560424805\n",
      "=====================================\n",
      "Epoch:70 Batch:29400\n",
      "Training RMSE:9.07096004486084\n",
      "Validation RMSE:8.725018501281738\n",
      "=====================================\n",
      "Epoch:70 Batch:29500\n",
      "Training RMSE:9.070854187011719\n",
      "Validation RMSE:8.722759246826172\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:24400 best loss:8.697358131408691\n",
      "H = 45 Test_RMSE = 8.866884231567383\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 45\n",
    "\n",
    "Q6_model = MyMLP()\n",
    "model = Q6_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "model = Q6_model.train(device,optimizer)\n",
    "test_RMSE = Q6_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yU9CGpgjKLCE",
   "metadata": {
    "id": "yU9CGpgjKLCE"
   },
   "source": [
    "H = 180時，Test RMSE數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "redrCdE5KHlN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "redrCdE5KHlN",
    "outputId": "e61a1683-2851-4e2a-e2ae-1dea8728e3f1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.20142650604248\n",
      "Validation RMSE:9.26504898071289\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:9.790473937988281\n",
      "Validation RMSE:8.989136695861816\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:9.616174697875977\n",
      "Validation RMSE:8.885553359985352\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:9.496101379394531\n",
      "Validation RMSE:8.8313570022583\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.411355972290039\n",
      "Validation RMSE:8.837593078613281\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.356754302978516\n",
      "Validation RMSE:8.81417465209961\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.310369491577148\n",
      "Validation RMSE:8.816288948059082\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.274052619934082\n",
      "Validation RMSE:8.74799633026123\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.24262523651123\n",
      "Validation RMSE:8.756890296936035\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.216052055358887\n",
      "Validation RMSE:8.731582641601562\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.194977760314941\n",
      "Validation RMSE:8.720885276794434\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.170372009277344\n",
      "Validation RMSE:8.7004976272583\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.149102210998535\n",
      "Validation RMSE:8.685757637023926\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.12918758392334\n",
      "Validation RMSE:8.70468807220459\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.116935729980469\n",
      "Validation RMSE:8.66933822631836\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.10297966003418\n",
      "Validation RMSE:8.671747207641602\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.091756820678711\n",
      "Validation RMSE:8.684318542480469\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.079113960266113\n",
      "Validation RMSE:8.63852310180664\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.068292617797852\n",
      "Validation RMSE:8.719097137451172\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.060186386108398\n",
      "Validation RMSE:8.64657211303711\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.049665451049805\n",
      "Validation RMSE:8.655573844909668\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.040507316589355\n",
      "Validation RMSE:8.644621849060059\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.030731201171875\n",
      "Validation RMSE:8.651368141174316\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.02242374420166\n",
      "Validation RMSE:8.636517524719238\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.015889167785645\n",
      "Validation RMSE:8.613704681396484\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.00905704498291\n",
      "Validation RMSE:8.636438369750977\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.001167297363281\n",
      "Validation RMSE:8.643502235412598\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:8.994382858276367\n",
      "Validation RMSE:8.605575561523438\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:8.98827075958252\n",
      "Validation RMSE:8.640226364135742\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:8.98255443572998\n",
      "Validation RMSE:8.607702255249023\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:8.976149559020996\n",
      "Validation RMSE:8.619953155517578\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:8.971831321716309\n",
      "Validation RMSE:8.621665000915527\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:8.966416358947754\n",
      "Validation RMSE:8.602259635925293\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:8.960569381713867\n",
      "Validation RMSE:8.585148811340332\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:8.955872535705566\n",
      "Validation RMSE:8.610465049743652\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:8.950600624084473\n",
      "Validation RMSE:8.609443664550781\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:8.947722434997559\n",
      "Validation RMSE:8.601305961608887\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:8.94333267211914\n",
      "Validation RMSE:8.584522247314453\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:8.938480377197266\n",
      "Validation RMSE:8.56332778930664\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:8.934110641479492\n",
      "Validation RMSE:8.611753463745117\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:8.930193901062012\n",
      "Validation RMSE:8.59815788269043\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:8.926286697387695\n",
      "Validation RMSE:8.564939498901367\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:8.922544479370117\n",
      "Validation RMSE:8.578766822814941\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:8.91928768157959\n",
      "Validation RMSE:8.610210418701172\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:8.916032791137695\n",
      "Validation RMSE:8.570195198059082\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:8.91195011138916\n",
      "Validation RMSE:8.589533805847168\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:8.907949447631836\n",
      "Validation RMSE:8.565576553344727\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:8.904229164123535\n",
      "Validation RMSE:8.594575881958008\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:8.901862144470215\n",
      "Validation RMSE:8.582406044006348\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:8.899260520935059\n",
      "Validation RMSE:8.573394775390625\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:8.89551830291748\n",
      "Validation RMSE:8.563386917114258\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:8.892902374267578\n",
      "Validation RMSE:8.574984550476074\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:8.890299797058105\n",
      "Validation RMSE:8.549513816833496\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:8.887073516845703\n",
      "Validation RMSE:8.589497566223145\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:8.883721351623535\n",
      "Validation RMSE:8.56921672821045\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:8.880803108215332\n",
      "Validation RMSE:8.589736938476562\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:8.877727508544922\n",
      "Validation RMSE:8.566245079040527\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:8.874967575073242\n",
      "Validation RMSE:8.5596923828125\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:8.8719482421875\n",
      "Validation RMSE:8.563502311706543\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:8.869426727294922\n",
      "Validation RMSE:8.569515228271484\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:8.867147445678711\n",
      "Validation RMSE:8.585182189941406\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:8.864582061767578\n",
      "Validation RMSE:8.590625762939453\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:8.86253547668457\n",
      "Validation RMSE:8.568541526794434\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:8.85997486114502\n",
      "Validation RMSE:8.579148292541504\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:8.857342720031738\n",
      "Validation RMSE:8.566405296325684\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:8.855213165283203\n",
      "Validation RMSE:8.554895401000977\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:8.852980613708496\n",
      "Validation RMSE:8.559187889099121\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:8.850430488586426\n",
      "Validation RMSE:8.545862197875977\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:8.84864616394043\n",
      "Validation RMSE:8.593794822692871\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:8.846461296081543\n",
      "Validation RMSE:8.55180549621582\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:8.844653129577637\n",
      "Validation RMSE:8.575224876403809\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:8.841737747192383\n",
      "Validation RMSE:8.566797256469727\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:8.840509414672852\n",
      "Validation RMSE:8.549978256225586\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:8.83863353729248\n",
      "Validation RMSE:8.531020164489746\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:8.83706283569336\n",
      "Validation RMSE:8.537405967712402\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:8.834968566894531\n",
      "Validation RMSE:8.54932689666748\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:8.832991600036621\n",
      "Validation RMSE:8.536913871765137\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:8.830970764160156\n",
      "Validation RMSE:8.58449649810791\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:8.829373359680176\n",
      "Validation RMSE:8.54810619354248\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:8.827352523803711\n",
      "Validation RMSE:8.5442476272583\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:8.825876235961914\n",
      "Validation RMSE:8.56493854522705\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:8.823870658874512\n",
      "Validation RMSE:8.548359870910645\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:8.82189655303955\n",
      "Validation RMSE:8.564746856689453\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:8.820691108703613\n",
      "Validation RMSE:8.520252227783203\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:8.818164825439453\n",
      "Validation RMSE:8.550219535827637\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:8.816546440124512\n",
      "Validation RMSE:8.5386381149292\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:8.815196990966797\n",
      "Validation RMSE:8.538848876953125\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:8.814275741577148\n",
      "Validation RMSE:8.53154182434082\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:8.812581062316895\n",
      "Validation RMSE:8.549127578735352\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:8.811250686645508\n",
      "Validation RMSE:8.537843704223633\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:8.809728622436523\n",
      "Validation RMSE:8.551527976989746\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:8.808187484741211\n",
      "Validation RMSE:8.54887866973877\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:8.806549072265625\n",
      "Validation RMSE:8.53149127960205\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:8.80510425567627\n",
      "Validation RMSE:8.53380298614502\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:8.803895950317383\n",
      "Validation RMSE:8.56838607788086\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:8.802101135253906\n",
      "Validation RMSE:8.545056343078613\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:8.800797462463379\n",
      "Validation RMSE:8.521010398864746\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:8.799517631530762\n",
      "Validation RMSE:8.515558242797852\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:8.797472953796387\n",
      "Validation RMSE:8.554986953735352\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:8.796246528625488\n",
      "Validation RMSE:8.555099487304688\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:8.794844627380371\n",
      "Validation RMSE:8.521148681640625\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:8.793462753295898\n",
      "Validation RMSE:8.529679298400879\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:8.792253494262695\n",
      "Validation RMSE:8.52281665802002\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:8.790946960449219\n",
      "Validation RMSE:8.512149810791016\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:8.789299964904785\n",
      "Validation RMSE:8.516984939575195\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:8.787833213806152\n",
      "Validation RMSE:8.508832931518555\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:8.787078857421875\n",
      "Validation RMSE:8.516279220581055\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:8.78560733795166\n",
      "Validation RMSE:8.514881134033203\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:8.784271240234375\n",
      "Validation RMSE:8.533198356628418\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:8.78257942199707\n",
      "Validation RMSE:8.541250228881836\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:8.781097412109375\n",
      "Validation RMSE:8.541248321533203\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:8.78041934967041\n",
      "Validation RMSE:8.516788482666016\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:8.779111862182617\n",
      "Validation RMSE:8.51587200164795\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:8.77772045135498\n",
      "Validation RMSE:8.536096572875977\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:8.776610374450684\n",
      "Validation RMSE:8.53206729888916\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:8.775435447692871\n",
      "Validation RMSE:8.521031379699707\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:8.774428367614746\n",
      "Validation RMSE:8.5440673828125\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:8.77298355102539\n",
      "Validation RMSE:8.53065299987793\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:8.771695137023926\n",
      "Validation RMSE:8.52739143371582\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:8.770869255065918\n",
      "Validation RMSE:8.542619705200195\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:8.76970386505127\n",
      "Validation RMSE:8.502159118652344\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:8.768296241760254\n",
      "Validation RMSE:8.55229377746582\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:8.767204284667969\n",
      "Validation RMSE:8.538507461547852\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:8.766351699829102\n",
      "Validation RMSE:8.53266429901123\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:8.765523910522461\n",
      "Validation RMSE:8.504569053649902\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:8.764734268188477\n",
      "Validation RMSE:8.52593994140625\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:8.76314640045166\n",
      "Validation RMSE:8.520687103271484\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:8.762350082397461\n",
      "Validation RMSE:8.5564603805542\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:8.761448860168457\n",
      "Validation RMSE:8.519927024841309\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:8.759997367858887\n",
      "Validation RMSE:8.533827781677246\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:8.759049415588379\n",
      "Validation RMSE:8.539910316467285\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:8.758135795593262\n",
      "Validation RMSE:8.517435073852539\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:8.757314682006836\n",
      "Validation RMSE:8.526269912719727\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:8.756156921386719\n",
      "Validation RMSE:8.500024795532227\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:8.755456924438477\n",
      "Validation RMSE:8.491500854492188\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:8.75434398651123\n",
      "Validation RMSE:8.527913093566895\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:8.753185272216797\n",
      "Validation RMSE:8.524226188659668\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:8.752327919006348\n",
      "Validation RMSE:8.532693862915039\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:8.750945091247559\n",
      "Validation RMSE:8.531228065490723\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:8.750296592712402\n",
      "Validation RMSE:8.541823387145996\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:8.749532699584961\n",
      "Validation RMSE:8.541805267333984\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:8.748342514038086\n",
      "Validation RMSE:8.532689094543457\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:8.747136116027832\n",
      "Validation RMSE:8.520068168640137\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:8.746702194213867\n",
      "Validation RMSE:8.537469863891602\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:8.745715141296387\n",
      "Validation RMSE:8.514090538024902\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:8.744660377502441\n",
      "Validation RMSE:8.532585144042969\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:8.743590354919434\n",
      "Validation RMSE:8.49600887298584\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:8.742637634277344\n",
      "Validation RMSE:8.527313232421875\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:8.741644859313965\n",
      "Validation RMSE:8.51626205444336\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:8.7409029006958\n",
      "Validation RMSE:8.532931327819824\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:8.740042686462402\n",
      "Validation RMSE:8.517635345458984\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:8.739055633544922\n",
      "Validation RMSE:8.528130531311035\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:8.73857593536377\n",
      "Validation RMSE:8.538220405578613\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:8.737791061401367\n",
      "Validation RMSE:8.528026580810547\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:8.736681938171387\n",
      "Validation RMSE:8.511672973632812\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:8.735718727111816\n",
      "Validation RMSE:8.523825645446777\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:8.734882354736328\n",
      "Validation RMSE:8.51950740814209\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:8.7340726852417\n",
      "Validation RMSE:8.516412734985352\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:8.733433723449707\n",
      "Validation RMSE:8.523826599121094\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:8.732583045959473\n",
      "Validation RMSE:8.520570755004883\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:8.731491088867188\n",
      "Validation RMSE:8.495818138122559\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:8.730710983276367\n",
      "Validation RMSE:8.497916221618652\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:8.730109214782715\n",
      "Validation RMSE:8.500551223754883\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:8.729156494140625\n",
      "Validation RMSE:8.531961441040039\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:8.728281021118164\n",
      "Validation RMSE:8.517467498779297\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:8.727652549743652\n",
      "Validation RMSE:8.513435363769531\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:8.726879119873047\n",
      "Validation RMSE:8.500246047973633\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:8.72607421875\n",
      "Validation RMSE:8.500617980957031\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:8.725268363952637\n",
      "Validation RMSE:8.512203216552734\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:8.72439193725586\n",
      "Validation RMSE:8.54123306274414\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:8.723526000976562\n",
      "Validation RMSE:8.52020263671875\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:8.722824096679688\n",
      "Validation RMSE:8.49731159210205\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:8.722244262695312\n",
      "Validation RMSE:8.526786804199219\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:8.72152042388916\n",
      "Validation RMSE:8.495728492736816\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:8.720723152160645\n",
      "Validation RMSE:8.510125160217285\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:8.719752311706543\n",
      "Validation RMSE:8.494976043701172\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:8.719181060791016\n",
      "Validation RMSE:8.486656188964844\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:8.71843147277832\n",
      "Validation RMSE:8.47261905670166\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:8.717812538146973\n",
      "Validation RMSE:8.512690544128418\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:8.717007637023926\n",
      "Validation RMSE:8.493062019348145\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:8.71630573272705\n",
      "Validation RMSE:8.479798316955566\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:8.715791702270508\n",
      "Validation RMSE:8.51832389831543\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:8.714852333068848\n",
      "Validation RMSE:8.482094764709473\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:8.714195251464844\n",
      "Validation RMSE:8.497090339660645\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:8.713418960571289\n",
      "Validation RMSE:8.484806060791016\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:8.712847709655762\n",
      "Validation RMSE:8.49341106414795\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:8.711934089660645\n",
      "Validation RMSE:8.506255149841309\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:8.711311340332031\n",
      "Validation RMSE:8.473017692565918\n",
      "=====================================\n",
      "Epoch:45 Batch:18900\n",
      "Training RMSE:8.71044635772705\n",
      "Validation RMSE:8.520416259765625\n",
      "=====================================\n",
      "Epoch:45 Batch:19000\n",
      "Training RMSE:8.709831237792969\n",
      "Validation RMSE:8.525813102722168\n",
      "=====================================\n",
      "Epoch:45 Batch:19100\n",
      "Training RMSE:8.70935344696045\n",
      "Validation RMSE:8.49459171295166\n",
      "=====================================\n",
      "Epoch:45 Batch:19200\n",
      "Training RMSE:8.708654403686523\n",
      "Validation RMSE:8.502397537231445\n",
      "=====================================\n",
      "Epoch:46 Batch:19300\n",
      "Training RMSE:8.708243370056152\n",
      "Validation RMSE:8.485372543334961\n",
      "=====================================\n",
      "Epoch:46 Batch:19400\n",
      "Training RMSE:8.707509994506836\n",
      "Validation RMSE:8.489563941955566\n",
      "=====================================\n",
      "Epoch:46 Batch:19500\n",
      "Training RMSE:8.706658363342285\n",
      "Validation RMSE:8.474514961242676\n",
      "=====================================\n",
      "Epoch:46 Batch:19600\n",
      "Training RMSE:8.706144332885742\n",
      "Validation RMSE:8.52401351928711\n",
      "=====================================\n",
      "Epoch:47 Batch:19700\n",
      "Training RMSE:8.705540657043457\n",
      "Validation RMSE:8.50145149230957\n",
      "=====================================\n",
      "Epoch:47 Batch:19800\n",
      "Training RMSE:8.704658508300781\n",
      "Validation RMSE:8.5126371383667\n",
      "=====================================\n",
      "Epoch:47 Batch:19900\n",
      "Training RMSE:8.70407772064209\n",
      "Validation RMSE:8.509992599487305\n",
      "=====================================\n",
      "Epoch:47 Batch:20000\n",
      "Training RMSE:8.703592300415039\n",
      "Validation RMSE:8.49868106842041\n",
      "=====================================\n",
      "Epoch:48 Batch:20100\n",
      "Training RMSE:8.7030029296875\n",
      "Validation RMSE:8.486445426940918\n",
      "=====================================\n",
      "Epoch:48 Batch:20200\n",
      "Training RMSE:8.702210426330566\n",
      "Validation RMSE:8.489919662475586\n",
      "=====================================\n",
      "Epoch:48 Batch:20300\n",
      "Training RMSE:8.701615333557129\n",
      "Validation RMSE:8.48376178741455\n",
      "=====================================\n",
      "Epoch:48 Batch:20400\n",
      "Training RMSE:8.70094108581543\n",
      "Validation RMSE:8.4989595413208\n",
      "=====================================\n",
      "Epoch:49 Batch:20500\n",
      "Training RMSE:8.700247764587402\n",
      "Validation RMSE:8.514973640441895\n",
      "=====================================\n",
      "Epoch:49 Batch:20600\n",
      "Training RMSE:8.69931411743164\n",
      "Validation RMSE:8.505775451660156\n",
      "=====================================\n",
      "Epoch:49 Batch:20700\n",
      "Training RMSE:8.6986722946167\n",
      "Validation RMSE:8.482490539550781\n",
      "=====================================\n",
      "Epoch:49 Batch:20800\n",
      "Training RMSE:8.69810962677002\n",
      "Validation RMSE:8.503087997436523\n",
      "=====================================\n",
      "Epoch:49 Batch:20900\n",
      "Training RMSE:8.69773006439209\n",
      "Validation RMSE:8.499173164367676\n",
      "=====================================\n",
      "Epoch:50 Batch:21000\n",
      "Training RMSE:8.697196006774902\n",
      "Validation RMSE:8.5007905960083\n",
      "=====================================\n",
      "Epoch:50 Batch:21100\n",
      "Training RMSE:8.696462631225586\n",
      "Validation RMSE:8.48697566986084\n",
      "=====================================\n",
      "Epoch:50 Batch:21200\n",
      "Training RMSE:8.695940017700195\n",
      "Validation RMSE:8.491939544677734\n",
      "=====================================\n",
      "Epoch:50 Batch:21300\n",
      "Training RMSE:8.695347785949707\n",
      "Validation RMSE:8.481531143188477\n",
      "=====================================\n",
      "Epoch:51 Batch:21400\n",
      "Training RMSE:8.694376945495605\n",
      "Validation RMSE:8.477832794189453\n",
      "=====================================\n",
      "Epoch:51 Batch:21500\n",
      "Training RMSE:8.693918228149414\n",
      "Validation RMSE:8.502360343933105\n",
      "=====================================\n",
      "Epoch:51 Batch:21600\n",
      "Training RMSE:8.693408966064453\n",
      "Validation RMSE:8.495495796203613\n",
      "=====================================\n",
      "Epoch:51 Batch:21700\n",
      "Training RMSE:8.69299030303955\n",
      "Validation RMSE:8.497611999511719\n",
      "=====================================\n",
      "Epoch:52 Batch:21800\n",
      "Training RMSE:8.692387580871582\n",
      "Validation RMSE:8.502306938171387\n",
      "=====================================\n",
      "Epoch:52 Batch:21900\n",
      "Training RMSE:8.691683769226074\n",
      "Validation RMSE:8.483598709106445\n",
      "=====================================\n",
      "Epoch:52 Batch:22000\n",
      "Training RMSE:8.69128131866455\n",
      "Validation RMSE:8.489888191223145\n",
      "=====================================\n",
      "Epoch:52 Batch:22100\n",
      "Training RMSE:8.690791130065918\n",
      "Validation RMSE:8.505599021911621\n",
      "=====================================\n",
      "Epoch:53 Batch:22200\n",
      "Training RMSE:8.690363883972168\n",
      "Validation RMSE:8.51913833618164\n",
      "=====================================\n",
      "Epoch:53 Batch:22300\n",
      "Training RMSE:8.689793586730957\n",
      "Validation RMSE:8.524617195129395\n",
      "=====================================\n",
      "Epoch:53 Batch:22400\n",
      "Training RMSE:8.689289093017578\n",
      "Validation RMSE:8.503264427185059\n",
      "=====================================\n",
      "Epoch:53 Batch:22500\n",
      "Training RMSE:8.688763618469238\n",
      "Validation RMSE:8.502364158630371\n",
      "=====================================\n",
      "Epoch:54 Batch:22600\n",
      "Training RMSE:8.687994956970215\n",
      "Validation RMSE:8.497986793518066\n",
      "=====================================\n",
      "Epoch:54 Batch:22700\n",
      "Training RMSE:8.687382698059082\n",
      "Validation RMSE:8.50391674041748\n",
      "=====================================\n",
      "Epoch:54 Batch:22800\n",
      "Training RMSE:8.6869478225708\n",
      "Validation RMSE:8.511959075927734\n",
      "=====================================\n",
      "Epoch:54 Batch:22900\n",
      "Training RMSE:8.686514854431152\n",
      "Validation RMSE:8.493754386901855\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:17800 best loss:8.47261905670166\n",
      "H = 180 Test_RMSE = 8.746196746826172\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 180\n",
    "\n",
    "Q6_model = MyMLP()\n",
    "model = Q6_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "model = Q6_model.train(device,optimizer)\n",
    "test_RMSE = Q6_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TsBPRzthKPiw",
   "metadata": {
    "id": "TsBPRzthKPiw"
   },
   "source": [
    "H = 360時，Test RMSE數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Cmr_xeQnKSoD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cmr_xeQnKSoD",
    "outputId": "795ea008-5599-4905-9c79-04e2169f035b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:9.93531608581543\n",
      "Validation RMSE:9.108758926391602\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:9.599625587463379\n",
      "Validation RMSE:8.979246139526367\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:9.462857246398926\n",
      "Validation RMSE:8.857792854309082\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:9.378317832946777\n",
      "Validation RMSE:8.845174789428711\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.306299209594727\n",
      "Validation RMSE:8.800986289978027\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.250448226928711\n",
      "Validation RMSE:8.762139320373535\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.207507133483887\n",
      "Validation RMSE:8.804971694946289\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.17929458618164\n",
      "Validation RMSE:8.704842567443848\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.148954391479492\n",
      "Validation RMSE:8.658984184265137\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.120766639709473\n",
      "Validation RMSE:8.669395446777344\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.096237182617188\n",
      "Validation RMSE:8.706174850463867\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.078707695007324\n",
      "Validation RMSE:8.659379959106445\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.060439109802246\n",
      "Validation RMSE:8.683429718017578\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.047508239746094\n",
      "Validation RMSE:8.661705017089844\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.030366897583008\n",
      "Validation RMSE:8.65451717376709\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.018357276916504\n",
      "Validation RMSE:8.610090255737305\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.00445556640625\n",
      "Validation RMSE:8.639458656311035\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:8.994762420654297\n",
      "Validation RMSE:8.621652603149414\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:8.98511791229248\n",
      "Validation RMSE:8.605264663696289\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:8.973204612731934\n",
      "Validation RMSE:8.597627639770508\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:8.962075233459473\n",
      "Validation RMSE:8.598544120788574\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:8.950542449951172\n",
      "Validation RMSE:8.599104881286621\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:8.944082260131836\n",
      "Validation RMSE:8.568904876708984\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:8.93422794342041\n",
      "Validation RMSE:8.571639060974121\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:8.927888870239258\n",
      "Validation RMSE:8.599876403808594\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:8.920738220214844\n",
      "Validation RMSE:8.568686485290527\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:8.913962364196777\n",
      "Validation RMSE:8.566803932189941\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:8.905052185058594\n",
      "Validation RMSE:8.571755409240723\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:8.899469375610352\n",
      "Validation RMSE:8.582050323486328\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:8.892656326293945\n",
      "Validation RMSE:8.562426567077637\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:8.885276794433594\n",
      "Validation RMSE:8.555564880371094\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:8.880221366882324\n",
      "Validation RMSE:8.54592227935791\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:8.87523365020752\n",
      "Validation RMSE:8.557165145874023\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:8.868392944335938\n",
      "Validation RMSE:8.529768943786621\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:8.863652229309082\n",
      "Validation RMSE:8.57043170928955\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:8.85668659210205\n",
      "Validation RMSE:8.540834426879883\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:8.853290557861328\n",
      "Validation RMSE:8.523785591125488\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:8.84875774383545\n",
      "Validation RMSE:8.533696174621582\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:8.842558860778809\n",
      "Validation RMSE:8.561100959777832\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:8.837859153747559\n",
      "Validation RMSE:8.529541015625\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:8.834001541137695\n",
      "Validation RMSE:8.506077766418457\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:8.829710006713867\n",
      "Validation RMSE:8.520992279052734\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:8.82433032989502\n",
      "Validation RMSE:8.536977767944336\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:8.820956230163574\n",
      "Validation RMSE:8.544793128967285\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:8.81668758392334\n",
      "Validation RMSE:8.525834083557129\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:8.8135404586792\n",
      "Validation RMSE:8.533571243286133\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:8.808877944946289\n",
      "Validation RMSE:8.536237716674805\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:8.804924964904785\n",
      "Validation RMSE:8.48086929321289\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:8.800835609436035\n",
      "Validation RMSE:8.484498023986816\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:8.797406196594238\n",
      "Validation RMSE:8.517324447631836\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:8.793795585632324\n",
      "Validation RMSE:8.5538969039917\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:8.789402961730957\n",
      "Validation RMSE:8.50399112701416\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:8.785284996032715\n",
      "Validation RMSE:8.542082786560059\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:8.783090591430664\n",
      "Validation RMSE:8.492088317871094\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:8.778417587280273\n",
      "Validation RMSE:8.504423141479492\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:8.774764060974121\n",
      "Validation RMSE:8.521076202392578\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:8.771726608276367\n",
      "Validation RMSE:8.494873046875\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:8.768669128417969\n",
      "Validation RMSE:8.488872528076172\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:8.765156745910645\n",
      "Validation RMSE:8.488296508789062\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:8.761926651000977\n",
      "Validation RMSE:8.505847930908203\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:8.759121894836426\n",
      "Validation RMSE:8.48116397857666\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:8.75632095336914\n",
      "Validation RMSE:8.45002555847168\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:8.753077507019043\n",
      "Validation RMSE:8.53098201751709\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:8.749805450439453\n",
      "Validation RMSE:8.483111381530762\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:8.746560096740723\n",
      "Validation RMSE:8.489646911621094\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:8.7437744140625\n",
      "Validation RMSE:8.47932243347168\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:8.741250991821289\n",
      "Validation RMSE:8.47545051574707\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:8.738609313964844\n",
      "Validation RMSE:8.475318908691406\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:8.736047744750977\n",
      "Validation RMSE:8.51721477508545\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:8.73342514038086\n",
      "Validation RMSE:8.492775917053223\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:8.73071575164795\n",
      "Validation RMSE:8.49899959564209\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:8.727768898010254\n",
      "Validation RMSE:8.448698043823242\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:8.725187301635742\n",
      "Validation RMSE:8.480587005615234\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:8.722599029541016\n",
      "Validation RMSE:8.489880561828613\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:8.71989917755127\n",
      "Validation RMSE:8.465203285217285\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:8.717350959777832\n",
      "Validation RMSE:8.47677993774414\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:8.715237617492676\n",
      "Validation RMSE:8.476908683776855\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:8.713553428649902\n",
      "Validation RMSE:8.470965385437012\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:8.710921287536621\n",
      "Validation RMSE:8.490747451782227\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:8.708025932312012\n",
      "Validation RMSE:8.461777687072754\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:8.705745697021484\n",
      "Validation RMSE:8.482455253601074\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:8.704097747802734\n",
      "Validation RMSE:8.467151641845703\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:8.701935768127441\n",
      "Validation RMSE:8.456254005432129\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:8.698822021484375\n",
      "Validation RMSE:8.447261810302734\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:8.695610046386719\n",
      "Validation RMSE:8.443492889404297\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:8.69286060333252\n",
      "Validation RMSE:8.48218059539795\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:8.691427230834961\n",
      "Validation RMSE:8.448927879333496\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:8.689659118652344\n",
      "Validation RMSE:8.4238862991333\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:8.687517166137695\n",
      "Validation RMSE:8.442957878112793\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:8.684829711914062\n",
      "Validation RMSE:8.47242259979248\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:8.682791709899902\n",
      "Validation RMSE:8.484743118286133\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:8.681191444396973\n",
      "Validation RMSE:8.457773208618164\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:8.678722381591797\n",
      "Validation RMSE:8.45952033996582\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:8.676497459411621\n",
      "Validation RMSE:8.463293075561523\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:8.674226760864258\n",
      "Validation RMSE:8.45371150970459\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:8.672722816467285\n",
      "Validation RMSE:8.44930362701416\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:8.670635223388672\n",
      "Validation RMSE:8.432941436767578\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:8.668933868408203\n",
      "Validation RMSE:8.49145221710205\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:8.667243957519531\n",
      "Validation RMSE:8.45355224609375\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:8.665166854858398\n",
      "Validation RMSE:8.451757431030273\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:8.662277221679688\n",
      "Validation RMSE:8.466421127319336\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:8.660140037536621\n",
      "Validation RMSE:8.442330360412598\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:8.658316612243652\n",
      "Validation RMSE:8.452668190002441\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:8.656707763671875\n",
      "Validation RMSE:8.462044715881348\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:8.654545783996582\n",
      "Validation RMSE:8.456573486328125\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:8.652554512023926\n",
      "Validation RMSE:8.441405296325684\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:8.650705337524414\n",
      "Validation RMSE:8.463852882385254\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:8.64869499206543\n",
      "Validation RMSE:8.45109748840332\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:8.647377967834473\n",
      "Validation RMSE:8.414937973022461\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:8.645874977111816\n",
      "Validation RMSE:8.423124313354492\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:8.644054412841797\n",
      "Validation RMSE:8.455450057983398\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:8.642217636108398\n",
      "Validation RMSE:8.421229362487793\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:8.640584945678711\n",
      "Validation RMSE:8.411075592041016\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:8.638947486877441\n",
      "Validation RMSE:8.45694637298584\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:8.636713027954102\n",
      "Validation RMSE:8.426129341125488\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:8.635209083557129\n",
      "Validation RMSE:8.430830001831055\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:8.63352108001709\n",
      "Validation RMSE:8.418834686279297\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:8.63188648223877\n",
      "Validation RMSE:8.443319320678711\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:8.630287170410156\n",
      "Validation RMSE:8.429350852966309\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:8.628486633300781\n",
      "Validation RMSE:8.445575714111328\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:8.626840591430664\n",
      "Validation RMSE:8.440716743469238\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:8.625028610229492\n",
      "Validation RMSE:8.424631118774414\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:8.623162269592285\n",
      "Validation RMSE:8.431066513061523\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:8.621703147888184\n",
      "Validation RMSE:8.443320274353027\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:8.620393753051758\n",
      "Validation RMSE:8.427337646484375\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:8.618414878845215\n",
      "Validation RMSE:8.415515899658203\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:8.616536140441895\n",
      "Validation RMSE:8.424921035766602\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:8.614889144897461\n",
      "Validation RMSE:8.435468673706055\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:8.613487243652344\n",
      "Validation RMSE:8.441450119018555\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:8.612051963806152\n",
      "Validation RMSE:8.451133728027344\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:8.610514640808105\n",
      "Validation RMSE:8.431469917297363\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:8.609101295471191\n",
      "Validation RMSE:8.441893577575684\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:8.607867240905762\n",
      "Validation RMSE:8.402830123901367\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:8.606488227844238\n",
      "Validation RMSE:8.422538757324219\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:8.604500770568848\n",
      "Validation RMSE:8.442220687866211\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:8.603116035461426\n",
      "Validation RMSE:8.42789363861084\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:8.601358413696289\n",
      "Validation RMSE:8.436759948730469\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:8.600081443786621\n",
      "Validation RMSE:8.409234046936035\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:8.598867416381836\n",
      "Validation RMSE:8.423498153686523\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:8.59722900390625\n",
      "Validation RMSE:8.42089557647705\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:8.59553337097168\n",
      "Validation RMSE:8.408215522766113\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:8.594350814819336\n",
      "Validation RMSE:8.459269523620605\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:8.592504501342773\n",
      "Validation RMSE:8.402653694152832\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:8.590993881225586\n",
      "Validation RMSE:8.412182807922363\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:8.589604377746582\n",
      "Validation RMSE:8.420256614685059\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:8.588531494140625\n",
      "Validation RMSE:8.4285888671875\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:8.586894989013672\n",
      "Validation RMSE:8.424089431762695\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:8.585678100585938\n",
      "Validation RMSE:8.414602279663086\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:8.584348678588867\n",
      "Validation RMSE:8.395306587219238\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:8.582892417907715\n",
      "Validation RMSE:8.424651145935059\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:8.5817232131958\n",
      "Validation RMSE:8.406266212463379\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:8.579989433288574\n",
      "Validation RMSE:8.417149543762207\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:8.578692436218262\n",
      "Validation RMSE:8.416741371154785\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:8.57734489440918\n",
      "Validation RMSE:8.40515422821045\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:8.57603645324707\n",
      "Validation RMSE:8.409872055053711\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:8.574578285217285\n",
      "Validation RMSE:8.425779342651367\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:8.573229789733887\n",
      "Validation RMSE:8.421374320983887\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:8.57217025756836\n",
      "Validation RMSE:8.41119384765625\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:8.570993423461914\n",
      "Validation RMSE:8.426996231079102\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:8.569777488708496\n",
      "Validation RMSE:8.395829200744629\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:8.56875228881836\n",
      "Validation RMSE:8.423568725585938\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:8.56757640838623\n",
      "Validation RMSE:8.431708335876465\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:8.566298484802246\n",
      "Validation RMSE:8.40943717956543\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:8.565072059631348\n",
      "Validation RMSE:8.454828262329102\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:8.563277244567871\n",
      "Validation RMSE:8.386224746704102\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:8.562272071838379\n",
      "Validation RMSE:8.43980884552002\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:8.561376571655273\n",
      "Validation RMSE:8.409322738647461\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:8.559835433959961\n",
      "Validation RMSE:8.441122055053711\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:8.558502197265625\n",
      "Validation RMSE:8.443183898925781\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:8.557376861572266\n",
      "Validation RMSE:8.410721778869629\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:8.55654239654541\n",
      "Validation RMSE:8.39303970336914\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:8.555480003356934\n",
      "Validation RMSE:8.440065383911133\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:8.55421257019043\n",
      "Validation RMSE:8.435214042663574\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:8.55281925201416\n",
      "Validation RMSE:8.423675537109375\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:8.55163288116455\n",
      "Validation RMSE:8.437363624572754\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:8.550372123718262\n",
      "Validation RMSE:8.420228004455566\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:8.549073219299316\n",
      "Validation RMSE:8.400259971618652\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:8.548015594482422\n",
      "Validation RMSE:8.411410331726074\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:8.546839714050293\n",
      "Validation RMSE:8.451618194580078\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:8.545594215393066\n",
      "Validation RMSE:8.44373607635498\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:8.544358253479004\n",
      "Validation RMSE:8.392563819885254\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:8.543360710144043\n",
      "Validation RMSE:8.40717887878418\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:8.54244613647461\n",
      "Validation RMSE:8.402006149291992\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:8.541285514831543\n",
      "Validation RMSE:8.412179946899414\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:8.539900779724121\n",
      "Validation RMSE:8.428393363952637\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:8.538935661315918\n",
      "Validation RMSE:8.414175033569336\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:8.537964820861816\n",
      "Validation RMSE:8.407079696655273\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:8.536883354187012\n",
      "Validation RMSE:8.379168510437012\n",
      "=====================================\n",
      "Epoch:45 Batch:18900\n",
      "Training RMSE:8.53576946258545\n",
      "Validation RMSE:8.409855842590332\n",
      "=====================================\n",
      "Epoch:45 Batch:19000\n",
      "Training RMSE:8.534613609313965\n",
      "Validation RMSE:8.410303115844727\n",
      "=====================================\n",
      "Epoch:45 Batch:19100\n",
      "Training RMSE:8.533123016357422\n",
      "Validation RMSE:8.402677536010742\n",
      "=====================================\n",
      "Epoch:45 Batch:19200\n",
      "Training RMSE:8.532456398010254\n",
      "Validation RMSE:8.391554832458496\n",
      "=====================================\n",
      "Epoch:46 Batch:19300\n",
      "Training RMSE:8.531075477600098\n",
      "Validation RMSE:8.43245792388916\n",
      "=====================================\n",
      "Epoch:46 Batch:19400\n",
      "Training RMSE:8.52983570098877\n",
      "Validation RMSE:8.40817642211914\n",
      "=====================================\n",
      "Epoch:46 Batch:19500\n",
      "Training RMSE:8.529160499572754\n",
      "Validation RMSE:8.392121315002441\n",
      "=====================================\n",
      "Epoch:46 Batch:19600\n",
      "Training RMSE:8.528170585632324\n",
      "Validation RMSE:8.399195671081543\n",
      "=====================================\n",
      "Epoch:47 Batch:19700\n",
      "Training RMSE:8.527056694030762\n",
      "Validation RMSE:8.381482124328613\n",
      "=====================================\n",
      "Epoch:47 Batch:19800\n",
      "Training RMSE:8.525745391845703\n",
      "Validation RMSE:8.412376403808594\n",
      "=====================================\n",
      "Epoch:47 Batch:19900\n",
      "Training RMSE:8.52471923828125\n",
      "Validation RMSE:8.39657974243164\n",
      "=====================================\n",
      "Epoch:47 Batch:20000\n",
      "Training RMSE:8.523883819580078\n",
      "Validation RMSE:8.404735565185547\n",
      "=====================================\n",
      "Epoch:48 Batch:20100\n",
      "Training RMSE:8.523101806640625\n",
      "Validation RMSE:8.432353973388672\n",
      "=====================================\n",
      "Epoch:48 Batch:20200\n",
      "Training RMSE:8.521889686584473\n",
      "Validation RMSE:8.411293029785156\n",
      "=====================================\n",
      "Epoch:48 Batch:20300\n",
      "Training RMSE:8.521146774291992\n",
      "Validation RMSE:8.407060623168945\n",
      "=====================================\n",
      "Epoch:48 Batch:20400\n",
      "Training RMSE:8.519970893859863\n",
      "Validation RMSE:8.432212829589844\n",
      "=====================================\n",
      "Epoch:49 Batch:20500\n",
      "Training RMSE:8.51895809173584\n",
      "Validation RMSE:8.39063549041748\n",
      "=====================================\n",
      "Epoch:49 Batch:20600\n",
      "Training RMSE:8.517802238464355\n",
      "Validation RMSE:8.423011779785156\n",
      "=====================================\n",
      "Epoch:49 Batch:20700\n",
      "Training RMSE:8.5169038772583\n",
      "Validation RMSE:8.37000846862793\n",
      "=====================================\n",
      "Epoch:49 Batch:20800\n",
      "Training RMSE:8.516125679016113\n",
      "Validation RMSE:8.389447212219238\n",
      "=====================================\n",
      "Epoch:49 Batch:20900\n",
      "Training RMSE:8.514863014221191\n",
      "Validation RMSE:8.377908706665039\n",
      "=====================================\n",
      "Epoch:50 Batch:21000\n",
      "Training RMSE:8.513497352600098\n",
      "Validation RMSE:8.395179748535156\n",
      "=====================================\n",
      "Epoch:50 Batch:21100\n",
      "Training RMSE:8.51268196105957\n",
      "Validation RMSE:8.394362449645996\n",
      "=====================================\n",
      "Epoch:50 Batch:21200\n",
      "Training RMSE:8.511788368225098\n",
      "Validation RMSE:8.437178611755371\n",
      "=====================================\n",
      "Epoch:50 Batch:21300\n",
      "Training RMSE:8.510857582092285\n",
      "Validation RMSE:8.422965049743652\n",
      "=====================================\n",
      "Epoch:51 Batch:21400\n",
      "Training RMSE:8.509860038757324\n",
      "Validation RMSE:8.375055313110352\n",
      "=====================================\n",
      "Epoch:51 Batch:21500\n",
      "Training RMSE:8.508703231811523\n",
      "Validation RMSE:8.403887748718262\n",
      "=====================================\n",
      "Epoch:51 Batch:21600\n",
      "Training RMSE:8.507685661315918\n",
      "Validation RMSE:8.418708801269531\n",
      "=====================================\n",
      "Epoch:51 Batch:21700\n",
      "Training RMSE:8.506753921508789\n",
      "Validation RMSE:8.382719039916992\n",
      "=====================================\n",
      "Epoch:52 Batch:21800\n",
      "Training RMSE:8.50578784942627\n",
      "Validation RMSE:8.385332107543945\n",
      "=====================================\n",
      "Epoch:52 Batch:21900\n",
      "Training RMSE:8.504836082458496\n",
      "Validation RMSE:8.411004066467285\n",
      "=====================================\n",
      "Epoch:52 Batch:22000\n",
      "Training RMSE:8.503814697265625\n",
      "Validation RMSE:8.402978897094727\n",
      "=====================================\n",
      "Epoch:52 Batch:22100\n",
      "Training RMSE:8.503152847290039\n",
      "Validation RMSE:8.389727592468262\n",
      "=====================================\n",
      "Epoch:53 Batch:22200\n",
      "Training RMSE:8.502359390258789\n",
      "Validation RMSE:8.403914451599121\n",
      "=====================================\n",
      "Epoch:53 Batch:22300\n",
      "Training RMSE:8.501363754272461\n",
      "Validation RMSE:8.399699211120605\n",
      "=====================================\n",
      "Epoch:53 Batch:22400\n",
      "Training RMSE:8.500513076782227\n",
      "Validation RMSE:8.380346298217773\n",
      "=====================================\n",
      "Epoch:53 Batch:22500\n",
      "Training RMSE:8.499443054199219\n",
      "Validation RMSE:8.398656845092773\n",
      "=====================================\n",
      "Epoch:54 Batch:22600\n",
      "Training RMSE:8.498255729675293\n",
      "Validation RMSE:8.386473655700684\n",
      "=====================================\n",
      "Epoch:54 Batch:22700\n",
      "Training RMSE:8.49727725982666\n",
      "Validation RMSE:8.392719268798828\n",
      "=====================================\n",
      "Epoch:54 Batch:22800\n",
      "Training RMSE:8.496232986450195\n",
      "Validation RMSE:8.42372989654541\n",
      "=====================================\n",
      "Epoch:54 Batch:22900\n",
      "Training RMSE:8.495391845703125\n",
      "Validation RMSE:8.393141746520996\n",
      "=====================================\n",
      "Epoch:55 Batch:23000\n",
      "Training RMSE:8.494770050048828\n",
      "Validation RMSE:8.397825241088867\n",
      "=====================================\n",
      "Epoch:55 Batch:23100\n",
      "Training RMSE:8.493653297424316\n",
      "Validation RMSE:8.384858131408691\n",
      "=====================================\n",
      "Epoch:55 Batch:23200\n",
      "Training RMSE:8.492757797241211\n",
      "Validation RMSE:8.406932830810547\n",
      "=====================================\n",
      "Epoch:55 Batch:23300\n",
      "Training RMSE:8.491939544677734\n",
      "Validation RMSE:8.368008613586426\n",
      "=====================================\n",
      "Epoch:55 Batch:23400\n",
      "Training RMSE:8.491203308105469\n",
      "Validation RMSE:8.426426887512207\n",
      "=====================================\n",
      "Epoch:56 Batch:23500\n",
      "Training RMSE:8.490280151367188\n",
      "Validation RMSE:8.398984909057617\n",
      "=====================================\n",
      "Epoch:56 Batch:23600\n",
      "Training RMSE:8.489409446716309\n",
      "Validation RMSE:8.444260597229004\n",
      "=====================================\n",
      "Epoch:56 Batch:23700\n",
      "Training RMSE:8.488553047180176\n",
      "Validation RMSE:8.397027015686035\n",
      "=====================================\n",
      "Epoch:56 Batch:23800\n",
      "Training RMSE:8.487497329711914\n",
      "Validation RMSE:8.37492847442627\n",
      "=====================================\n",
      "Epoch:57 Batch:23900\n",
      "Training RMSE:8.486465454101562\n",
      "Validation RMSE:8.37269401550293\n",
      "=====================================\n",
      "Epoch:57 Batch:24000\n",
      "Training RMSE:8.485576629638672\n",
      "Validation RMSE:8.433655738830566\n",
      "=====================================\n",
      "Epoch:57 Batch:24100\n",
      "Training RMSE:8.48495864868164\n",
      "Validation RMSE:8.390021324157715\n",
      "=====================================\n",
      "Epoch:57 Batch:24200\n",
      "Training RMSE:8.483939170837402\n",
      "Validation RMSE:8.437917709350586\n",
      "=====================================\n",
      "Epoch:58 Batch:24300\n",
      "Training RMSE:8.482919692993164\n",
      "Validation RMSE:8.411474227905273\n",
      "=====================================\n",
      "Epoch:58 Batch:24400\n",
      "Training RMSE:8.481977462768555\n",
      "Validation RMSE:8.390209197998047\n",
      "=====================================\n",
      "Epoch:58 Batch:24500\n",
      "Training RMSE:8.48132038116455\n",
      "Validation RMSE:8.425359725952148\n",
      "=====================================\n",
      "Epoch:58 Batch:24600\n",
      "Training RMSE:8.480551719665527\n",
      "Validation RMSE:8.39527416229248\n",
      "=====================================\n",
      "Epoch:59 Batch:24700\n",
      "Training RMSE:8.479717254638672\n",
      "Validation RMSE:8.416922569274902\n",
      "=====================================\n",
      "Epoch:59 Batch:24800\n",
      "Training RMSE:8.478677749633789\n",
      "Validation RMSE:8.397669792175293\n",
      "=====================================\n",
      "Epoch:59 Batch:24900\n",
      "Training RMSE:8.477697372436523\n",
      "Validation RMSE:8.397626876831055\n",
      "=====================================\n",
      "Epoch:59 Batch:25000\n",
      "Training RMSE:8.477009773254395\n",
      "Validation RMSE:8.403937339782715\n",
      "=====================================\n",
      "Epoch:60 Batch:25100\n",
      "Training RMSE:8.476204872131348\n",
      "Validation RMSE:8.390873908996582\n",
      "=====================================\n",
      "Epoch:60 Batch:25200\n",
      "Training RMSE:8.475459098815918\n",
      "Validation RMSE:8.406294822692871\n",
      "=====================================\n",
      "Epoch:60 Batch:25300\n",
      "Training RMSE:8.474708557128906\n",
      "Validation RMSE:8.420171737670898\n",
      "=====================================\n",
      "Epoch:60 Batch:25400\n",
      "Training RMSE:8.473861694335938\n",
      "Validation RMSE:8.381746292114258\n",
      "=====================================\n",
      "Epoch:61 Batch:25500\n",
      "Training RMSE:8.473000526428223\n",
      "Validation RMSE:8.404606819152832\n",
      "=====================================\n",
      "Epoch:61 Batch:25600\n",
      "Training RMSE:8.472186088562012\n",
      "Validation RMSE:8.402137756347656\n",
      "=====================================\n",
      "Epoch:61 Batch:25700\n",
      "Training RMSE:8.471263885498047\n",
      "Validation RMSE:8.425125122070312\n",
      "=====================================\n",
      "Epoch:61 Batch:25800\n",
      "Training RMSE:8.470471382141113\n",
      "Validation RMSE:8.420552253723145\n",
      "=====================================\n",
      "Epoch:61 Batch:25900\n",
      "Training RMSE:8.469574928283691\n",
      "Validation RMSE:8.404873847961426\n",
      "=====================================\n",
      "Epoch:62 Batch:26000\n",
      "Training RMSE:8.468744277954102\n",
      "Validation RMSE:8.40557861328125\n",
      "=====================================\n",
      "Epoch:62 Batch:26100\n",
      "Training RMSE:8.467921257019043\n",
      "Validation RMSE:8.394591331481934\n",
      "=====================================\n",
      "Epoch:62 Batch:26200\n",
      "Training RMSE:8.4671049118042\n",
      "Validation RMSE:8.399806022644043\n",
      "=====================================\n",
      "Epoch:62 Batch:26300\n",
      "Training RMSE:8.466351509094238\n",
      "Validation RMSE:8.416974067687988\n",
      "=====================================\n",
      "Epoch:63 Batch:26400\n",
      "Training RMSE:8.46556568145752\n",
      "Validation RMSE:8.417583465576172\n",
      "=====================================\n",
      "Epoch:63 Batch:26500\n",
      "Training RMSE:8.464646339416504\n",
      "Validation RMSE:8.416119575500488\n",
      "=====================================\n",
      "Epoch:63 Batch:26600\n",
      "Training RMSE:8.46405029296875\n",
      "Validation RMSE:8.40784740447998\n",
      "=====================================\n",
      "Epoch:63 Batch:26700\n",
      "Training RMSE:8.463374137878418\n",
      "Validation RMSE:8.39758586883545\n",
      "=====================================\n",
      "Epoch:64 Batch:26800\n",
      "Training RMSE:8.462623596191406\n",
      "Validation RMSE:8.382603645324707\n",
      "=====================================\n",
      "Epoch:64 Batch:26900\n",
      "Training RMSE:8.461930274963379\n",
      "Validation RMSE:8.401123046875\n",
      "=====================================\n",
      "Epoch:64 Batch:27000\n",
      "Training RMSE:8.461175918579102\n",
      "Validation RMSE:8.429059028625488\n",
      "=====================================\n",
      "Epoch:64 Batch:27100\n",
      "Training RMSE:8.460319519042969\n",
      "Validation RMSE:8.395309448242188\n",
      "=====================================\n",
      "Epoch:65 Batch:27200\n",
      "Training RMSE:8.459397315979004\n",
      "Validation RMSE:8.359891891479492\n",
      "=====================================\n",
      "Epoch:65 Batch:27300\n",
      "Training RMSE:8.458645820617676\n",
      "Validation RMSE:8.404294967651367\n",
      "=====================================\n",
      "Epoch:65 Batch:27400\n",
      "Training RMSE:8.457983016967773\n",
      "Validation RMSE:8.408615112304688\n",
      "=====================================\n",
      "Epoch:65 Batch:27500\n",
      "Training RMSE:8.457340240478516\n",
      "Validation RMSE:8.388747215270996\n",
      "=====================================\n",
      "Epoch:66 Batch:27600\n",
      "Training RMSE:8.456526756286621\n",
      "Validation RMSE:8.391035079956055\n",
      "=====================================\n",
      "Epoch:66 Batch:27700\n",
      "Training RMSE:8.45567798614502\n",
      "Validation RMSE:8.390626907348633\n",
      "=====================================\n",
      "Epoch:66 Batch:27800\n",
      "Training RMSE:8.454883575439453\n",
      "Validation RMSE:8.398804664611816\n",
      "=====================================\n",
      "Epoch:66 Batch:27900\n",
      "Training RMSE:8.454160690307617\n",
      "Validation RMSE:8.39787769317627\n",
      "=====================================\n",
      "Epoch:66 Batch:28000\n",
      "Training RMSE:8.45344352722168\n",
      "Validation RMSE:8.385303497314453\n",
      "=====================================\n",
      "Epoch:67 Batch:28100\n",
      "Training RMSE:8.45252799987793\n",
      "Validation RMSE:8.389357566833496\n",
      "=====================================\n",
      "Epoch:67 Batch:28200\n",
      "Training RMSE:8.451727867126465\n",
      "Validation RMSE:8.377880096435547\n",
      "=====================================\n",
      "Epoch:67 Batch:28300\n",
      "Training RMSE:8.45112419128418\n",
      "Validation RMSE:8.42504596710205\n",
      "=====================================\n",
      "Epoch:67 Batch:28400\n",
      "Training RMSE:8.450462341308594\n",
      "Validation RMSE:8.402908325195312\n",
      "=====================================\n",
      "Epoch:68 Batch:28500\n",
      "Training RMSE:8.449844360351562\n",
      "Validation RMSE:8.404706001281738\n",
      "=====================================\n",
      "Epoch:68 Batch:28600\n",
      "Training RMSE:8.448952674865723\n",
      "Validation RMSE:8.41051959991455\n",
      "=====================================\n",
      "Epoch:68 Batch:28700\n",
      "Training RMSE:8.447976112365723\n",
      "Validation RMSE:8.389795303344727\n",
      "=====================================\n",
      "Epoch:68 Batch:28800\n",
      "Training RMSE:8.447524070739746\n",
      "Validation RMSE:8.401932716369629\n",
      "=====================================\n",
      "Epoch:69 Batch:28900\n",
      "Training RMSE:8.446911811828613\n",
      "Validation RMSE:8.369690895080566\n",
      "=====================================\n",
      "Epoch:69 Batch:29000\n",
      "Training RMSE:8.446200370788574\n",
      "Validation RMSE:8.392064094543457\n",
      "=====================================\n",
      "Epoch:69 Batch:29100\n",
      "Training RMSE:8.445514678955078\n",
      "Validation RMSE:8.376411437988281\n",
      "=====================================\n",
      "Epoch:69 Batch:29200\n",
      "Training RMSE:8.444733619689941\n",
      "Validation RMSE:8.419044494628906\n",
      "=====================================\n",
      "Epoch:70 Batch:29300\n",
      "Training RMSE:8.44404125213623\n",
      "Validation RMSE:8.3920259475708\n",
      "=====================================\n",
      "Epoch:70 Batch:29400\n",
      "Training RMSE:8.443361282348633\n",
      "Validation RMSE:8.384163856506348\n",
      "=====================================\n",
      "Epoch:70 Batch:29500\n",
      "Training RMSE:8.442594528198242\n",
      "Validation RMSE:8.408085823059082\n",
      "=====================================\n",
      "Epoch:70 Batch:29600\n",
      "Training RMSE:8.441987037658691\n",
      "Validation RMSE:8.395564079284668\n",
      "=====================================\n",
      "Epoch:71 Batch:29700\n",
      "Training RMSE:8.441353797912598\n",
      "Validation RMSE:8.383269309997559\n",
      "=====================================\n",
      "Epoch:71 Batch:29800\n",
      "Training RMSE:8.440595626831055\n",
      "Validation RMSE:8.370745658874512\n",
      "=====================================\n",
      "Epoch:71 Batch:29900\n",
      "Training RMSE:8.439895629882812\n",
      "Validation RMSE:8.406248092651367\n",
      "=====================================\n",
      "Epoch:71 Batch:30000\n",
      "Training RMSE:8.439278602600098\n",
      "Validation RMSE:8.398991584777832\n",
      "=====================================\n",
      "Epoch:72 Batch:30100\n",
      "Training RMSE:8.438457489013672\n",
      "Validation RMSE:8.37991714477539\n",
      "=====================================\n",
      "Epoch:72 Batch:30200\n",
      "Training RMSE:8.437713623046875\n",
      "Validation RMSE:8.415826797485352\n",
      "=====================================\n",
      "Epoch:72 Batch:30300\n",
      "Training RMSE:8.43710708618164\n",
      "Validation RMSE:8.4083833694458\n",
      "=====================================\n",
      "Epoch:72 Batch:30400\n",
      "Training RMSE:8.436315536499023\n",
      "Validation RMSE:8.37230396270752\n",
      "=====================================\n",
      "Epoch:72 Batch:30500\n",
      "Training RMSE:8.435564041137695\n",
      "Validation RMSE:8.4088773727417\n",
      "=====================================\n",
      "Epoch:73 Batch:30600\n",
      "Training RMSE:8.434636116027832\n",
      "Validation RMSE:8.396403312683105\n",
      "=====================================\n",
      "Epoch:73 Batch:30700\n",
      "Training RMSE:8.434004783630371\n",
      "Validation RMSE:8.387930870056152\n",
      "=====================================\n",
      "Epoch:73 Batch:30800\n",
      "Training RMSE:8.433431625366211\n",
      "Validation RMSE:8.379961967468262\n",
      "=====================================\n",
      "Epoch:73 Batch:30900\n",
      "Training RMSE:8.432798385620117\n",
      "Validation RMSE:8.40191650390625\n",
      "=====================================\n",
      "Epoch:74 Batch:31000\n",
      "Training RMSE:8.431938171386719\n",
      "Validation RMSE:8.3869047164917\n",
      "=====================================\n",
      "Epoch:74 Batch:31100\n",
      "Training RMSE:8.43120002746582\n",
      "Validation RMSE:8.417699813842773\n",
      "=====================================\n",
      "Epoch:74 Batch:31200\n",
      "Training RMSE:8.430439949035645\n",
      "Validation RMSE:8.390912055969238\n",
      "=====================================\n",
      "Epoch:74 Batch:31300\n",
      "Training RMSE:8.429845809936523\n",
      "Validation RMSE:8.399922370910645\n",
      "=====================================\n",
      "Epoch:75 Batch:31400\n",
      "Training RMSE:8.429216384887695\n",
      "Validation RMSE:8.420682907104492\n",
      "=====================================\n",
      "Epoch:75 Batch:31500\n",
      "Training RMSE:8.428449630737305\n",
      "Validation RMSE:8.36961841583252\n",
      "=====================================\n",
      "Epoch:75 Batch:31600\n",
      "Training RMSE:8.427754402160645\n",
      "Validation RMSE:8.379931449890137\n",
      "=====================================\n",
      "Epoch:75 Batch:31700\n",
      "Training RMSE:8.427210807800293\n",
      "Validation RMSE:8.382128715515137\n",
      "=====================================\n",
      "Epoch:76 Batch:31800\n",
      "Training RMSE:8.426322937011719\n",
      "Validation RMSE:8.415754318237305\n",
      "=====================================\n",
      "Epoch:76 Batch:31900\n",
      "Training RMSE:8.425799369812012\n",
      "Validation RMSE:8.385143280029297\n",
      "=====================================\n",
      "Epoch:76 Batch:32000\n",
      "Training RMSE:8.42509937286377\n",
      "Validation RMSE:8.36009693145752\n",
      "=====================================\n",
      "Epoch:76 Batch:32100\n",
      "Training RMSE:8.424492835998535\n",
      "Validation RMSE:8.39189624786377\n",
      "=====================================\n",
      "Epoch:77 Batch:32200\n",
      "Training RMSE:8.423833847045898\n",
      "Validation RMSE:8.436367988586426\n",
      "=====================================\n",
      "Epoch:77 Batch:32300\n",
      "Training RMSE:8.423126220703125\n",
      "Validation RMSE:8.405752182006836\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:27200 best loss:8.359891891479492\n",
      "H = 360 Test_RMSE = 8.769277572631836\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 360\n",
    "\n",
    "Q6_model = MyMLP()\n",
    "model = Q6_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "model = Q6_model.train(device,optimizer)\n",
    "test_RMSE = Q6_model.test(device,testloader)\n",
    "print(f'H = {H} Test_RMSE = {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7qhWJt3ZK70M",
   "metadata": {
    "id": "7qhWJt3ZK70M"
   },
   "source": [
    "### 討論H = 20, 45, 180, 360的Test RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kSMJusU6K9Wl",
   "metadata": {
    "id": "kSMJusU6K9Wl"
   },
   "source": [
    "在使用Dropout和Adam演算法之後，模型不再出現overfitting， 然而H 最大的時候，並不是 RMSE 最小、預測能力最好的時候。是剛好介在中間的 H = 180 得到了最好的成績。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b6cb83",
   "metadata": {
    "id": "45b6cb83"
   },
   "source": [
    "### Q7 L2 + L1 Loss (15%)\n",
    "我們前面的小題皆是使用SSE，也就是L2 Loss。一個改善模型訓練的方式是使用多種類似的Loss，以線性組合的方式建構Loss Function。請使用Q5中的MLP with Dropout模型 (H = 90)，並以L2 + L1 Loss訓練模型。這個Loss的定義如下:\n",
    "\n",
    "$$\n",
    "loss(\\mathbf{y}, \\hat{\\mathbf{y}}) = z \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + (1 - z) \\sum_{i = 1}^n | y_i - \\hat{y}_i |,\n",
    "$$\n",
    "其中z為實數且$0 <=z <= 1$。\n",
    "\n",
    "使用z = 0.5。並以Adam訓練模型。畫出Training and Validation RMSE，並報告Test RMSE。注意這裡繪圖時應使用RMSE而不是這個特殊的Loss。\n",
    "\n",
    "另外，使用z = 0.0, 0.1, 0.9, 1.0訓練模型(不須提供訓練過程的Loss圖形)，統整各個z值下的Test RMSE並討論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7b654",
   "metadata": {
    "id": "4aa7b654"
   },
   "outputs": [],
   "source": [
    "class Q7MLP(MyMLP):\n",
    "    def __init__(self,z):\n",
    "        super().__init__()\n",
    "        self.z = z\n",
    "   \n",
    "    def loss(self,y_pred,y_true):\n",
    "        Q7loss = self.z*(((y_pred-y_true)**2).sum())+(1-self.z)*((torch.abs(y_pred-y_true)).sum())\n",
    "        return Q7loss\n",
    "    \n",
    "    def train(self,device,optimizer,verbose=True):\n",
    "        step_count = 0\n",
    "        total_loss = 0\n",
    "        for epoch_idx in range(self.epoch):\n",
    "            for batch_idx, (inputs, targets) in enumerate(subtrainloader):\n",
    "                targets = targets.reshape((-1, 1))\n",
    "                self.model.train()\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                step_count += 1\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.loss(outputs,targets)\n",
    "                loss_SSE = self.sse(outputs, targets)\n",
    "                loss_MSE = self.mse(outputs,targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss_MSE\n",
    "                if step_count % self.calc_rmse_intvls == 0:\n",
    "                    rmse_train = ((total_loss / step_count)**0.5).cpu().detach().numpy()\n",
    "                    rmse_valid =self.calc_rmse(device,validloader,self.model).cpu().detach().numpy()\n",
    "                    self.train_rmse_lst.append(rmse_train)\n",
    "                    self.valid_rmse_lst.append(rmse_valid)\n",
    "                    if verbose:\n",
    "                      print(f'Epoch:{epoch_idx} Batch:{step_count}')\n",
    "                      print(f'Training RMSE:{rmse_train}')\n",
    "                      print(f'Validation RMSE:{rmse_valid}')\n",
    "                      print('=====================================')\n",
    "                      #total_loss = 0\n",
    "                    if rmse_valid < self.best_loss:\n",
    "                        self.best_loss = rmse_valid\n",
    "                        self.best_step_count = step_count\n",
    "                        self.best_model = self.model \n",
    "                    elif step_count - self.best_step_count > self.patience:\n",
    "                        self.end_step = step_count\n",
    "                        print('============Early Stop==================')\n",
    "                        print(f'best step:{self.best_step_count} best loss:{self.best_loss}')\n",
    "                        return self.best_model  \n",
    "        self.end_step = step_count\n",
    "        return self.best_model\n",
    "    \n",
    "    def plot(self,H,dropout=False):\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(self.train_rmse_lst, label='subtrain')\n",
    "        plt.plot(self.valid_rmse_lst, label='valid')\n",
    "        plt.title(f'MLP with Four Hidden Layers and L2+L1 Loss (H = {H}, z = {self.z}, dropout = {dropout})')\n",
    "        plt.xlabel('batchs (100 per)')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bJSvIE6yLLlH",
   "metadata": {
    "id": "bJSvIE6yLLlH"
   },
   "source": [
    "畫出Training and Validation RMSE，並報告Test RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ef8ae8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "c3ef8ae8",
    "outputId": "8c16d0e7-1a1e-4a13-a9dd-33713332dc76"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.502344131469727\n",
      "Validation RMSE:9.456080436706543\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.051857948303223\n",
      "Validation RMSE:9.094910621643066\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:9.834074020385742\n",
      "Validation RMSE:9.027044296264648\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:9.69693374633789\n",
      "Validation RMSE:8.91567325592041\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:9.5947847366333\n",
      "Validation RMSE:8.894513130187988\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:9.522619247436523\n",
      "Validation RMSE:8.861030578613281\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.474925994873047\n",
      "Validation RMSE:8.843263626098633\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.433830261230469\n",
      "Validation RMSE:8.798746109008789\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.394797325134277\n",
      "Validation RMSE:8.815794944763184\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.366596221923828\n",
      "Validation RMSE:8.804056167602539\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.340600967407227\n",
      "Validation RMSE:8.768866539001465\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.316764831542969\n",
      "Validation RMSE:8.75594711303711\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.296229362487793\n",
      "Validation RMSE:8.779455184936523\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.273685455322266\n",
      "Validation RMSE:8.731273651123047\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.258726119995117\n",
      "Validation RMSE:8.728172302246094\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.24345588684082\n",
      "Validation RMSE:8.72447395324707\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.23147201538086\n",
      "Validation RMSE:8.720585823059082\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.221366882324219\n",
      "Validation RMSE:8.712868690490723\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.205759048461914\n",
      "Validation RMSE:8.742594718933105\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.195286750793457\n",
      "Validation RMSE:8.711445808410645\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.186281204223633\n",
      "Validation RMSE:8.740431785583496\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.177007675170898\n",
      "Validation RMSE:8.718021392822266\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.165732383728027\n",
      "Validation RMSE:8.69157886505127\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.158331871032715\n",
      "Validation RMSE:8.717204093933105\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.15107536315918\n",
      "Validation RMSE:8.694409370422363\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.143143653869629\n",
      "Validation RMSE:8.688617706298828\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.137304306030273\n",
      "Validation RMSE:8.679971694946289\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:9.130577087402344\n",
      "Validation RMSE:8.684309959411621\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:9.123295783996582\n",
      "Validation RMSE:8.693207740783691\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:9.117654800415039\n",
      "Validation RMSE:8.669757843017578\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:9.111714363098145\n",
      "Validation RMSE:8.662846565246582\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:9.106706619262695\n",
      "Validation RMSE:8.673473358154297\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:9.100340843200684\n",
      "Validation RMSE:8.675521850585938\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:9.09653091430664\n",
      "Validation RMSE:8.68529224395752\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:9.09090805053711\n",
      "Validation RMSE:8.722314834594727\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:9.084677696228027\n",
      "Validation RMSE:8.657257080078125\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:9.081798553466797\n",
      "Validation RMSE:8.661975860595703\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:9.077398300170898\n",
      "Validation RMSE:8.685601234436035\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:9.07320499420166\n",
      "Validation RMSE:8.663891792297363\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:9.068023681640625\n",
      "Validation RMSE:8.695479393005371\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:9.064537048339844\n",
      "Validation RMSE:8.663956642150879\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:9.062503814697266\n",
      "Validation RMSE:8.642759323120117\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:9.0590181350708\n",
      "Validation RMSE:8.644718170166016\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:9.054829597473145\n",
      "Validation RMSE:8.6881103515625\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:9.051178932189941\n",
      "Validation RMSE:8.650456428527832\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:9.049239158630371\n",
      "Validation RMSE:8.661141395568848\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:9.046669006347656\n",
      "Validation RMSE:8.665587425231934\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:9.042142868041992\n",
      "Validation RMSE:8.659826278686523\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:9.038552284240723\n",
      "Validation RMSE:8.657203674316406\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:9.035543441772461\n",
      "Validation RMSE:8.673624992370605\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:9.033677101135254\n",
      "Validation RMSE:8.620142936706543\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:9.031097412109375\n",
      "Validation RMSE:8.647165298461914\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:9.027892112731934\n",
      "Validation RMSE:8.63085651397705\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:9.025050163269043\n",
      "Validation RMSE:8.640191078186035\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:9.02220630645752\n",
      "Validation RMSE:8.659903526306152\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:9.020136833190918\n",
      "Validation RMSE:8.628129005432129\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:9.0169677734375\n",
      "Validation RMSE:8.67718505859375\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:9.015054702758789\n",
      "Validation RMSE:8.647326469421387\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:9.012407302856445\n",
      "Validation RMSE:8.661788940429688\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:9.010250091552734\n",
      "Validation RMSE:8.62002182006836\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:9.008487701416016\n",
      "Validation RMSE:8.632338523864746\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:9.006481170654297\n",
      "Validation RMSE:8.629426002502441\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:9.004023551940918\n",
      "Validation RMSE:8.633339881896973\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:9.002084732055664\n",
      "Validation RMSE:8.644207954406738\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:9.000139236450195\n",
      "Validation RMSE:8.630877494812012\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:8.997842788696289\n",
      "Validation RMSE:8.647942543029785\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:8.995238304138184\n",
      "Validation RMSE:8.640124320983887\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:8.99316120147705\n",
      "Validation RMSE:8.627296447753906\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:8.991472244262695\n",
      "Validation RMSE:8.624448776245117\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:8.989051818847656\n",
      "Validation RMSE:8.65727424621582\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:8.988151550292969\n",
      "Validation RMSE:8.631441116333008\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:8.986590385437012\n",
      "Validation RMSE:8.638762474060059\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:8.984434127807617\n",
      "Validation RMSE:8.633222579956055\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:8.982819557189941\n",
      "Validation RMSE:8.619671821594238\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:8.981243133544922\n",
      "Validation RMSE:8.631218910217285\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:8.979212760925293\n",
      "Validation RMSE:8.636094093322754\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:8.977222442626953\n",
      "Validation RMSE:8.643457412719727\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:8.976025581359863\n",
      "Validation RMSE:8.613571166992188\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:8.974482536315918\n",
      "Validation RMSE:8.6389799118042\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:8.97222900390625\n",
      "Validation RMSE:8.621373176574707\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:8.970809936523438\n",
      "Validation RMSE:8.60128116607666\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:8.96977710723877\n",
      "Validation RMSE:8.621274948120117\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:8.968085289001465\n",
      "Validation RMSE:8.639979362487793\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:8.96674919128418\n",
      "Validation RMSE:8.609824180603027\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:8.96541690826416\n",
      "Validation RMSE:8.635087013244629\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:8.964015007019043\n",
      "Validation RMSE:8.609801292419434\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:8.962389945983887\n",
      "Validation RMSE:8.614291191101074\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:8.961244583129883\n",
      "Validation RMSE:8.625104904174805\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:8.960360527038574\n",
      "Validation RMSE:8.621540069580078\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:8.958953857421875\n",
      "Validation RMSE:8.634004592895508\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:8.957756042480469\n",
      "Validation RMSE:8.614897727966309\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:8.956292152404785\n",
      "Validation RMSE:8.604355812072754\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:8.954863548278809\n",
      "Validation RMSE:8.615715980529785\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:8.95313835144043\n",
      "Validation RMSE:8.603594779968262\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:8.952018737792969\n",
      "Validation RMSE:8.61866283416748\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:8.951165199279785\n",
      "Validation RMSE:8.614848136901855\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:8.950618743896484\n",
      "Validation RMSE:8.606863975524902\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:8.949246406555176\n",
      "Validation RMSE:8.609519004821777\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:8.948097229003906\n",
      "Validation RMSE:8.624357223510742\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:8.946549415588379\n",
      "Validation RMSE:8.619416236877441\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:8.945258140563965\n",
      "Validation RMSE:8.616174697875977\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:8.944201469421387\n",
      "Validation RMSE:8.612692832946777\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:8.942740440368652\n",
      "Validation RMSE:8.607135772705078\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:8.941793441772461\n",
      "Validation RMSE:8.606850624084473\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:8.941011428833008\n",
      "Validation RMSE:8.634398460388184\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:8.939762115478516\n",
      "Validation RMSE:8.599161148071289\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:8.938776969909668\n",
      "Validation RMSE:8.598609924316406\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:8.937615394592285\n",
      "Validation RMSE:8.62497329711914\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:8.936927795410156\n",
      "Validation RMSE:8.600800514221191\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:8.936030387878418\n",
      "Validation RMSE:8.611987113952637\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:8.934723854064941\n",
      "Validation RMSE:8.613429069519043\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:8.934062957763672\n",
      "Validation RMSE:8.624969482421875\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:8.93288803100586\n",
      "Validation RMSE:8.61064338684082\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:8.930971145629883\n",
      "Validation RMSE:8.602672576904297\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:8.930137634277344\n",
      "Validation RMSE:8.625015258789062\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:8.929555892944336\n",
      "Validation RMSE:8.607338905334473\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:8.929170608520508\n",
      "Validation RMSE:8.591946601867676\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:8.92805290222168\n",
      "Validation RMSE:8.605348587036133\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:8.926877975463867\n",
      "Validation RMSE:8.61924934387207\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:8.926331520080566\n",
      "Validation RMSE:8.619388580322266\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:8.925201416015625\n",
      "Validation RMSE:8.586930274963379\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:8.924240112304688\n",
      "Validation RMSE:8.5875825881958\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:8.923632621765137\n",
      "Validation RMSE:8.611654281616211\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:8.922445297241211\n",
      "Validation RMSE:8.60988712310791\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:8.922086715698242\n",
      "Validation RMSE:8.608162879943848\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:8.920828819274902\n",
      "Validation RMSE:8.598132133483887\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:8.920170783996582\n",
      "Validation RMSE:8.615181922912598\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:8.9193754196167\n",
      "Validation RMSE:8.600939750671387\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:8.91887378692627\n",
      "Validation RMSE:8.615829467773438\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:8.91768741607666\n",
      "Validation RMSE:8.607842445373535\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:8.916974067687988\n",
      "Validation RMSE:8.6019926071167\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:8.916388511657715\n",
      "Validation RMSE:8.588199615478516\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:8.915672302246094\n",
      "Validation RMSE:8.592792510986328\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:8.914534568786621\n",
      "Validation RMSE:8.606721878051758\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:8.913476943969727\n",
      "Validation RMSE:8.610774993896484\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:8.912981986999512\n",
      "Validation RMSE:8.604863166809082\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:8.912090301513672\n",
      "Validation RMSE:8.60576343536377\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:8.911478042602539\n",
      "Validation RMSE:8.595613479614258\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:8.910540580749512\n",
      "Validation RMSE:8.60173225402832\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:8.909732818603516\n",
      "Validation RMSE:8.614089012145996\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:8.909116744995117\n",
      "Validation RMSE:8.60457992553711\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:8.908642768859863\n",
      "Validation RMSE:8.591586112976074\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:8.90772533416748\n",
      "Validation RMSE:8.60511589050293\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:8.907156944274902\n",
      "Validation RMSE:8.588201522827148\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:8.90604019165039\n",
      "Validation RMSE:8.576093673706055\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:8.905710220336914\n",
      "Validation RMSE:8.586223602294922\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:8.90512466430664\n",
      "Validation RMSE:8.598130226135254\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:8.9038724899292\n",
      "Validation RMSE:8.581341743469238\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:8.903525352478027\n",
      "Validation RMSE:8.608258247375488\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:8.90290641784668\n",
      "Validation RMSE:8.582261085510254\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:8.901955604553223\n",
      "Validation RMSE:8.574200630187988\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:8.901392936706543\n",
      "Validation RMSE:8.574315071105957\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:8.901152610778809\n",
      "Validation RMSE:8.584848403930664\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:8.90031909942627\n",
      "Validation RMSE:8.579846382141113\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:8.899492263793945\n",
      "Validation RMSE:8.606430053710938\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:8.898693084716797\n",
      "Validation RMSE:8.574882507324219\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:8.898297309875488\n",
      "Validation RMSE:8.586416244506836\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:8.897674560546875\n",
      "Validation RMSE:8.604562759399414\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:8.897093772888184\n",
      "Validation RMSE:8.597966194152832\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:8.896623611450195\n",
      "Validation RMSE:8.590885162353516\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:8.895707130432129\n",
      "Validation RMSE:8.603203773498535\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:8.894970893859863\n",
      "Validation RMSE:8.609042167663574\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:8.89466381072998\n",
      "Validation RMSE:8.5881929397583\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:8.894120216369629\n",
      "Validation RMSE:8.610249519348145\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:8.893341064453125\n",
      "Validation RMSE:8.597640037536621\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:8.892573356628418\n",
      "Validation RMSE:8.613052368164062\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:8.892110824584961\n",
      "Validation RMSE:8.605525970458984\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:8.89137077331543\n",
      "Validation RMSE:8.596960067749023\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:8.890944480895996\n",
      "Validation RMSE:8.58399486541748\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:8.890414237976074\n",
      "Validation RMSE:8.584470748901367\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:8.890195846557617\n",
      "Validation RMSE:8.611443519592285\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:8.889507293701172\n",
      "Validation RMSE:8.606222152709961\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:8.888847351074219\n",
      "Validation RMSE:8.601044654846191\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:8.888520240783691\n",
      "Validation RMSE:8.600207328796387\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:8.887951850891113\n",
      "Validation RMSE:8.594926834106445\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:8.887155532836914\n",
      "Validation RMSE:8.591829299926758\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:8.887093544006348\n",
      "Validation RMSE:8.595808029174805\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:8.88660717010498\n",
      "Validation RMSE:8.593509674072266\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:8.885799407958984\n",
      "Validation RMSE:8.584229469299316\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:8.88529109954834\n",
      "Validation RMSE:8.568791389465332\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:8.88477611541748\n",
      "Validation RMSE:8.598434448242188\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:8.883970260620117\n",
      "Validation RMSE:8.575032234191895\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:8.883553504943848\n",
      "Validation RMSE:8.588363647460938\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:8.883159637451172\n",
      "Validation RMSE:8.604570388793945\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:8.882969856262207\n",
      "Validation RMSE:8.592832565307617\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:8.882594108581543\n",
      "Validation RMSE:8.587855339050293\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:8.881821632385254\n",
      "Validation RMSE:8.599161148071289\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:8.881287574768066\n",
      "Validation RMSE:8.615133285522461\n",
      "=====================================\n",
      "Epoch:45 Batch:18900\n",
      "Training RMSE:8.880796432495117\n",
      "Validation RMSE:8.581440925598145\n",
      "=====================================\n",
      "Epoch:45 Batch:19000\n",
      "Training RMSE:8.880203247070312\n",
      "Validation RMSE:8.57827377319336\n",
      "=====================================\n",
      "Epoch:45 Batch:19100\n",
      "Training RMSE:8.879968643188477\n",
      "Validation RMSE:8.584908485412598\n",
      "=====================================\n",
      "Epoch:45 Batch:19200\n",
      "Training RMSE:8.87950611114502\n",
      "Validation RMSE:8.58914852142334\n",
      "=====================================\n",
      "Epoch:46 Batch:19300\n",
      "Training RMSE:8.878628730773926\n",
      "Validation RMSE:8.605487823486328\n",
      "=====================================\n",
      "Epoch:46 Batch:19400\n",
      "Training RMSE:8.877904891967773\n",
      "Validation RMSE:8.585586547851562\n",
      "=====================================\n",
      "Epoch:46 Batch:19500\n",
      "Training RMSE:8.877693176269531\n",
      "Validation RMSE:8.592890739440918\n",
      "=====================================\n",
      "Epoch:46 Batch:19600\n",
      "Training RMSE:8.877506256103516\n",
      "Validation RMSE:8.579059600830078\n",
      "=====================================\n",
      "Epoch:47 Batch:19700\n",
      "Training RMSE:8.877056121826172\n",
      "Validation RMSE:8.582947731018066\n",
      "=====================================\n",
      "Epoch:47 Batch:19800\n",
      "Training RMSE:8.876441955566406\n",
      "Validation RMSE:8.581072807312012\n",
      "=====================================\n",
      "Epoch:47 Batch:19900\n",
      "Training RMSE:8.875849723815918\n",
      "Validation RMSE:8.579093933105469\n",
      "=====================================\n",
      "Epoch:47 Batch:20000\n",
      "Training RMSE:8.875499725341797\n",
      "Validation RMSE:8.58753776550293\n",
      "=====================================\n",
      "Epoch:48 Batch:20100\n",
      "Training RMSE:8.87509822845459\n",
      "Validation RMSE:8.591533660888672\n",
      "=====================================\n",
      "Epoch:48 Batch:20200\n",
      "Training RMSE:8.874468803405762\n",
      "Validation RMSE:8.591090202331543\n",
      "=====================================\n",
      "Epoch:48 Batch:20300\n",
      "Training RMSE:8.873862266540527\n",
      "Validation RMSE:8.597137451171875\n",
      "=====================================\n",
      "Epoch:48 Batch:20400\n",
      "Training RMSE:8.873537063598633\n",
      "Validation RMSE:8.58178424835205\n",
      "=====================================\n",
      "Epoch:49 Batch:20500\n",
      "Training RMSE:8.873324394226074\n",
      "Validation RMSE:8.588255882263184\n",
      "=====================================\n",
      "Epoch:49 Batch:20600\n",
      "Training RMSE:8.872785568237305\n",
      "Validation RMSE:8.58184814453125\n",
      "=====================================\n",
      "Epoch:49 Batch:20700\n",
      "Training RMSE:8.872322082519531\n",
      "Validation RMSE:8.568486213684082\n",
      "=====================================\n",
      "Epoch:49 Batch:20800\n",
      "Training RMSE:8.871975898742676\n",
      "Validation RMSE:8.583330154418945\n",
      "=====================================\n",
      "Epoch:49 Batch:20900\n",
      "Training RMSE:8.871661186218262\n",
      "Validation RMSE:8.586471557617188\n",
      "=====================================\n",
      "Epoch:50 Batch:21000\n",
      "Training RMSE:8.871416091918945\n",
      "Validation RMSE:8.589614868164062\n",
      "=====================================\n",
      "Epoch:50 Batch:21100\n",
      "Training RMSE:8.871037483215332\n",
      "Validation RMSE:8.580066680908203\n",
      "=====================================\n",
      "Epoch:50 Batch:21200\n",
      "Training RMSE:8.870532035827637\n",
      "Validation RMSE:8.606517791748047\n",
      "=====================================\n",
      "Epoch:50 Batch:21300\n",
      "Training RMSE:8.870072364807129\n",
      "Validation RMSE:8.587608337402344\n",
      "=====================================\n",
      "Epoch:51 Batch:21400\n",
      "Training RMSE:8.869499206542969\n",
      "Validation RMSE:8.602049827575684\n",
      "=====================================\n",
      "Epoch:51 Batch:21500\n",
      "Training RMSE:8.869217872619629\n",
      "Validation RMSE:8.573126792907715\n",
      "=====================================\n",
      "Epoch:51 Batch:21600\n",
      "Training RMSE:8.868829727172852\n",
      "Validation RMSE:8.586748123168945\n",
      "=====================================\n",
      "Epoch:51 Batch:21700\n",
      "Training RMSE:8.868345260620117\n",
      "Validation RMSE:8.585535049438477\n",
      "=====================================\n",
      "Epoch:52 Batch:21800\n",
      "Training RMSE:8.867964744567871\n",
      "Validation RMSE:8.577607154846191\n",
      "=====================================\n",
      "Epoch:52 Batch:21900\n",
      "Training RMSE:8.867084503173828\n",
      "Validation RMSE:8.588789939880371\n",
      "=====================================\n",
      "Epoch:52 Batch:22000\n",
      "Training RMSE:8.866959571838379\n",
      "Validation RMSE:8.577999114990234\n",
      "=====================================\n",
      "Epoch:52 Batch:22100\n",
      "Training RMSE:8.866606712341309\n",
      "Validation RMSE:8.587512969970703\n",
      "=====================================\n",
      "Epoch:53 Batch:22200\n",
      "Training RMSE:8.866329193115234\n",
      "Validation RMSE:8.602147102355957\n",
      "=====================================\n",
      "Epoch:53 Batch:22300\n",
      "Training RMSE:8.865743637084961\n",
      "Validation RMSE:8.597895622253418\n",
      "=====================================\n",
      "Epoch:53 Batch:22400\n",
      "Training RMSE:8.86539077758789\n",
      "Validation RMSE:8.582242965698242\n",
      "=====================================\n",
      "Epoch:53 Batch:22500\n",
      "Training RMSE:8.865347862243652\n",
      "Validation RMSE:8.568304061889648\n",
      "=====================================\n",
      "Epoch:54 Batch:22600\n",
      "Training RMSE:8.864876747131348\n",
      "Validation RMSE:8.576960563659668\n",
      "=====================================\n",
      "Epoch:54 Batch:22700\n",
      "Training RMSE:8.864469528198242\n",
      "Validation RMSE:8.577017784118652\n",
      "=====================================\n",
      "Epoch:54 Batch:22800\n",
      "Training RMSE:8.863839149475098\n",
      "Validation RMSE:8.596709251403809\n",
      "=====================================\n",
      "Epoch:54 Batch:22900\n",
      "Training RMSE:8.863556861877441\n",
      "Validation RMSE:8.589969635009766\n",
      "=====================================\n",
      "Epoch:55 Batch:23000\n",
      "Training RMSE:8.863237380981445\n",
      "Validation RMSE:8.584540367126465\n",
      "=====================================\n",
      "Epoch:55 Batch:23100\n",
      "Training RMSE:8.86292552947998\n",
      "Validation RMSE:8.583731651306152\n",
      "=====================================\n",
      "Epoch:55 Batch:23200\n",
      "Training RMSE:8.862580299377441\n",
      "Validation RMSE:8.600895881652832\n",
      "=====================================\n",
      "Epoch:55 Batch:23300\n",
      "Training RMSE:8.862167358398438\n",
      "Validation RMSE:8.578930854797363\n",
      "=====================================\n",
      "Epoch:55 Batch:23400\n",
      "Training RMSE:8.861860275268555\n",
      "Validation RMSE:8.580479621887207\n",
      "=====================================\n",
      "Epoch:56 Batch:23500\n",
      "Training RMSE:8.86142349243164\n",
      "Validation RMSE:8.578280448913574\n",
      "=====================================\n",
      "Epoch:56 Batch:23600\n",
      "Training RMSE:8.861145973205566\n",
      "Validation RMSE:8.580857276916504\n",
      "=====================================\n",
      "Epoch:56 Batch:23700\n",
      "Training RMSE:8.860818862915039\n",
      "Validation RMSE:8.582003593444824\n",
      "=====================================\n",
      "Epoch:56 Batch:23800\n",
      "Training RMSE:8.860289573669434\n",
      "Validation RMSE:8.568290710449219\n",
      "=====================================\n",
      "Epoch:57 Batch:23900\n",
      "Training RMSE:8.859705924987793\n",
      "Validation RMSE:8.583390235900879\n",
      "=====================================\n",
      "Epoch:57 Batch:24000\n",
      "Training RMSE:8.85924243927002\n",
      "Validation RMSE:8.578384399414062\n",
      "=====================================\n",
      "Epoch:57 Batch:24100\n",
      "Training RMSE:8.858819007873535\n",
      "Validation RMSE:8.565619468688965\n",
      "=====================================\n",
      "Epoch:57 Batch:24200\n",
      "Training RMSE:8.8585844039917\n",
      "Validation RMSE:8.590970039367676\n",
      "=====================================\n",
      "Epoch:58 Batch:24300\n",
      "Training RMSE:8.858382225036621\n",
      "Validation RMSE:8.560033798217773\n",
      "=====================================\n",
      "Epoch:58 Batch:24400\n",
      "Training RMSE:8.858017921447754\n",
      "Validation RMSE:8.593229293823242\n",
      "=====================================\n",
      "Epoch:58 Batch:24500\n",
      "Training RMSE:8.85779094696045\n",
      "Validation RMSE:8.551358222961426\n",
      "=====================================\n",
      "Epoch:58 Batch:24600\n",
      "Training RMSE:8.857492446899414\n",
      "Validation RMSE:8.582320213317871\n",
      "=====================================\n",
      "Epoch:59 Batch:24700\n",
      "Training RMSE:8.857237815856934\n",
      "Validation RMSE:8.573324203491211\n",
      "=====================================\n",
      "Epoch:59 Batch:24800\n",
      "Training RMSE:8.856679916381836\n",
      "Validation RMSE:8.575553894042969\n",
      "=====================================\n",
      "Epoch:59 Batch:24900\n",
      "Training RMSE:8.856522560119629\n",
      "Validation RMSE:8.568931579589844\n",
      "=====================================\n",
      "Epoch:59 Batch:25000\n",
      "Training RMSE:8.856368064880371\n",
      "Validation RMSE:8.572668075561523\n",
      "=====================================\n",
      "Epoch:60 Batch:25100\n",
      "Training RMSE:8.855979919433594\n",
      "Validation RMSE:8.584136009216309\n",
      "=====================================\n",
      "Epoch:60 Batch:25200\n",
      "Training RMSE:8.855633735656738\n",
      "Validation RMSE:8.570138931274414\n",
      "=====================================\n",
      "Epoch:60 Batch:25300\n",
      "Training RMSE:8.855326652526855\n",
      "Validation RMSE:8.583035469055176\n",
      "=====================================\n",
      "Epoch:60 Batch:25400\n",
      "Training RMSE:8.855114936828613\n",
      "Validation RMSE:8.569265365600586\n",
      "=====================================\n",
      "Epoch:61 Batch:25500\n",
      "Training RMSE:8.854804039001465\n",
      "Validation RMSE:8.586563110351562\n",
      "=====================================\n",
      "Epoch:61 Batch:25600\n",
      "Training RMSE:8.8545503616333\n",
      "Validation RMSE:8.573060989379883\n",
      "=====================================\n",
      "Epoch:61 Batch:25700\n",
      "Training RMSE:8.854318618774414\n",
      "Validation RMSE:8.566910743713379\n",
      "=====================================\n",
      "Epoch:61 Batch:25800\n",
      "Training RMSE:8.853951454162598\n",
      "Validation RMSE:8.567066192626953\n",
      "=====================================\n",
      "Epoch:61 Batch:25900\n",
      "Training RMSE:8.85356616973877\n",
      "Validation RMSE:8.585138320922852\n",
      "=====================================\n",
      "Epoch:62 Batch:26000\n",
      "Training RMSE:8.853181838989258\n",
      "Validation RMSE:8.580471992492676\n",
      "=====================================\n",
      "Epoch:62 Batch:26100\n",
      "Training RMSE:8.853140830993652\n",
      "Validation RMSE:8.565823554992676\n",
      "=====================================\n",
      "Epoch:62 Batch:26200\n",
      "Training RMSE:8.852706909179688\n",
      "Validation RMSE:8.60069751739502\n",
      "=====================================\n",
      "Epoch:62 Batch:26300\n",
      "Training RMSE:8.852433204650879\n",
      "Validation RMSE:8.578165054321289\n",
      "=====================================\n",
      "Epoch:63 Batch:26400\n",
      "Training RMSE:8.851934432983398\n",
      "Validation RMSE:8.574188232421875\n",
      "=====================================\n",
      "Epoch:63 Batch:26500\n",
      "Training RMSE:8.85159969329834\n",
      "Validation RMSE:8.589774131774902\n",
      "=====================================\n",
      "Epoch:63 Batch:26600\n",
      "Training RMSE:8.851390838623047\n",
      "Validation RMSE:8.564767837524414\n",
      "=====================================\n",
      "Epoch:63 Batch:26700\n",
      "Training RMSE:8.851005554199219\n",
      "Validation RMSE:8.570314407348633\n",
      "=====================================\n",
      "Epoch:64 Batch:26800\n",
      "Training RMSE:8.850648880004883\n",
      "Validation RMSE:8.570998191833496\n",
      "=====================================\n",
      "Epoch:64 Batch:26900\n",
      "Training RMSE:8.850228309631348\n",
      "Validation RMSE:8.571161270141602\n",
      "=====================================\n",
      "Epoch:64 Batch:27000\n",
      "Training RMSE:8.849996566772461\n",
      "Validation RMSE:8.57112979888916\n",
      "=====================================\n",
      "Epoch:64 Batch:27100\n",
      "Training RMSE:8.849703788757324\n",
      "Validation RMSE:8.576769828796387\n",
      "=====================================\n",
      "Epoch:65 Batch:27200\n",
      "Training RMSE:8.849591255187988\n",
      "Validation RMSE:8.573365211486816\n",
      "=====================================\n",
      "Epoch:65 Batch:27300\n",
      "Training RMSE:8.849251747131348\n",
      "Validation RMSE:8.55953598022461\n",
      "=====================================\n",
      "Epoch:65 Batch:27400\n",
      "Training RMSE:8.848857879638672\n",
      "Validation RMSE:8.569700241088867\n",
      "=====================================\n",
      "Epoch:65 Batch:27500\n",
      "Training RMSE:8.848621368408203\n",
      "Validation RMSE:8.586923599243164\n",
      "=====================================\n",
      "Epoch:66 Batch:27600\n",
      "Training RMSE:8.848427772521973\n",
      "Validation RMSE:8.576781272888184\n",
      "=====================================\n",
      "Epoch:66 Batch:27700\n",
      "Training RMSE:8.848118782043457\n",
      "Validation RMSE:8.582014083862305\n",
      "=====================================\n",
      "Epoch:66 Batch:27800\n",
      "Training RMSE:8.84780502319336\n",
      "Validation RMSE:8.576024055480957\n",
      "=====================================\n",
      "Epoch:66 Batch:27900\n",
      "Training RMSE:8.847593307495117\n",
      "Validation RMSE:8.58625316619873\n",
      "=====================================\n",
      "Epoch:66 Batch:28000\n",
      "Training RMSE:8.847166061401367\n",
      "Validation RMSE:8.577123641967773\n",
      "=====================================\n",
      "Epoch:67 Batch:28100\n",
      "Training RMSE:8.846722602844238\n",
      "Validation RMSE:8.582071304321289\n",
      "=====================================\n",
      "Epoch:67 Batch:28200\n",
      "Training RMSE:8.846405029296875\n",
      "Validation RMSE:8.590605735778809\n",
      "=====================================\n",
      "Epoch:67 Batch:28300\n",
      "Training RMSE:8.846269607543945\n",
      "Validation RMSE:8.588106155395508\n",
      "=====================================\n",
      "Epoch:67 Batch:28400\n",
      "Training RMSE:8.846074104309082\n",
      "Validation RMSE:8.591554641723633\n",
      "=====================================\n",
      "Epoch:68 Batch:28500\n",
      "Training RMSE:8.845734596252441\n",
      "Validation RMSE:8.563716888427734\n",
      "=====================================\n",
      "Epoch:68 Batch:28600\n",
      "Training RMSE:8.845596313476562\n",
      "Validation RMSE:8.560184478759766\n",
      "=====================================\n",
      "Epoch:68 Batch:28700\n",
      "Training RMSE:8.845184326171875\n",
      "Validation RMSE:8.569476127624512\n",
      "=====================================\n",
      "Epoch:68 Batch:28800\n",
      "Training RMSE:8.844820976257324\n",
      "Validation RMSE:8.596940994262695\n",
      "=====================================\n",
      "Epoch:69 Batch:28900\n",
      "Training RMSE:8.844565391540527\n",
      "Validation RMSE:8.570953369140625\n",
      "=====================================\n",
      "Epoch:69 Batch:29000\n",
      "Training RMSE:8.84427261352539\n",
      "Validation RMSE:8.566320419311523\n",
      "=====================================\n",
      "Epoch:69 Batch:29100\n",
      "Training RMSE:8.844030380249023\n",
      "Validation RMSE:8.587199211120605\n",
      "=====================================\n",
      "Epoch:69 Batch:29200\n",
      "Training RMSE:8.843774795532227\n",
      "Validation RMSE:8.583008766174316\n",
      "=====================================\n",
      "Epoch:70 Batch:29300\n",
      "Training RMSE:8.84355640411377\n",
      "Validation RMSE:8.557235717773438\n",
      "=====================================\n",
      "Epoch:70 Batch:29400\n",
      "Training RMSE:8.843361854553223\n",
      "Validation RMSE:8.572266578674316\n",
      "=====================================\n",
      "Epoch:70 Batch:29500\n",
      "Training RMSE:8.842862129211426\n",
      "Validation RMSE:8.56240463256836\n",
      "=====================================\n",
      "Epoch:70 Batch:29600\n",
      "Training RMSE:8.842728614807129\n",
      "Validation RMSE:8.586028099060059\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:24500 best loss:8.551358222961426\n",
      "H = 90 z =0.5 Test_RMSE = 8.773019790649414\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 90\n",
    "z=0.5\n",
    "\n",
    "Q7_model = Q7MLP(z)\n",
    "model = Q7_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "model = Q7_model.train(device,optimizer,True)\n",
    "Q7test_RMSE = Q7_model.test(device,testloader)\n",
    "print(f'H = {H} z ={z} Test_RMSE = {Q7test_RMSE}')\n",
    "Q7_model.plot(H,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NX6kZGIpLQir",
   "metadata": {
    "id": "NX6kZGIpLQir"
   },
   "source": [
    "使用z = 0.0, 0.1, 0.9, 1.0訓練模型(不須提供訓練過程的Loss圖形)，統整各個z值下的Test RMSE並討論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xYKf4aRsLEk4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xYKf4aRsLEk4",
    "outputId": "0695bac9-f58b-4e6b-9f55-3b127fc02c7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q7MLP(H=90 z =0.0) is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Early Stop==================\n",
      "best step:12300 best loss:8.851669311523438\n",
      "H = 90 z =0.0 Test_RMSE = 9.015715599060059\n",
      "===============================================\n",
      "Q7MLP(H=90 z =0.1) is training\n",
      "============Early Stop==================\n",
      "best step:25800 best loss:8.5739107131958\n",
      "H = 90 z =0.1 Test_RMSE = 8.770198822021484\n",
      "===============================================\n",
      "Q7MLP(H=90 z =0.5) is training\n",
      "============Early Stop==================\n",
      "best step:15500 best loss:8.569818496704102\n",
      "H = 90 z =0.5 Test_RMSE = 8.768113136291504\n",
      "===============================================\n",
      "Q7MLP(H=90 z =0.9) is training\n",
      "============Early Stop==================\n",
      "best step:17200 best loss:8.57918643951416\n",
      "H = 90 z =0.9 Test_RMSE = 8.7907075881958\n",
      "===============================================\n",
      "Q7MLP(H=90 z =1.0) is training\n",
      "============Early Stop==================\n",
      "best step:12200 best loss:8.566431045532227\n",
      "H = 90 z =1.0 Test_RMSE = 8.776327133178711\n",
      "===============================================\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "z_list = [0.0, 0.1, 0.5, 0.9, 1.0]\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 90\n",
    "z=0.5\n",
    "\n",
    "for z in z_list:\n",
    "    Q7_model = Q7MLP(z)\n",
    "    model = Q7_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    print(f'Q7MLP(H={H} z ={z}) is training')\n",
    "    model = Q7_model.train(device,optimizer,verbose=False)\n",
    "    test_RMSE = Q7_model.test(device,testloader)\n",
    "    print(f'H = {H} z ={z} Test_RMSE = {test_RMSE}')\n",
    "    print('===============================================')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "snBU4_aHoYGU",
   "metadata": {
    "id": "snBU4_aHoYGU"
   },
   "source": [
    "### 統整各個z值下的Test RMSE並討論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JypryTT8oaBl",
   "metadata": {
    "id": "JypryTT8oaBl"
   },
   "source": [
    "從以上結果可知在z=0.5時擁有最好的預測結果(最小的Test_RMSE)，而不是最大的z=1.0，然而我們還是可以從中看出這個模型L2與L1配飾程度相當，因為L2比例最好的時候佔了五成。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671bd54",
   "metadata": {
    "id": "b671bd54"
   },
   "source": [
    "### Q8 L2 + Customerized Loss (15%)\n",
    "考慮另一個比較特別的Loss Function\n",
    "\n",
    "$$\n",
    "qloss(\\mathbf{y}, \\hat{\\mathbf{y}}) = \\sum_{i=1}^n \\{ q (y_i - \\hat{y}_i)_+ + (1 - q) (\\hat{y}_i - y_i)_+ \\},\n",
    "$$\n",
    "其中q為參數且$0<=q<=1$，而$(y_i - \\hat{y}_i)_+$是取正值的意思。也就是說如果$(y_i - \\hat{y}_i) > 0$，則$(y_i - \\hat{y}_i)_+ = y_i - \\hat{y}_i$，否則$(y_i - \\hat{y}_i)_+ = 0$。\n",
    "\n",
    "令模型的Loss為$z \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 + (1 - z) \\sum_{i=1}^n \\{ 0.5 (y_i - \\hat{y}_i)_+ + 0.5 (\\hat{y}_i - y_i)_+ \\} $。請使用Q5中的MLP with Dropout模型(H = 90)，令z = 0。並以Adam訓練模型。畫出Training and Validation RMSE，並報告Test RMSE。注意這裡繪圖時應使用RMSE而不是這個特殊的Loss。\n",
    "\n",
    "另外，使用z = 0.1, 0.5, 0.9, 1.0訓練模型(不須提供訓練過程的Loss圖形)，統整各個z值下的Test RMSE並討論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d576f",
   "metadata": {
    "id": "6f3d576f"
   },
   "outputs": [],
   "source": [
    "class Q8MLP(Q7MLP):\n",
    "    def __init__(self,z,q):\n",
    "        super().__init__(z)\n",
    "        self.q = q\n",
    "        \n",
    "    def loss(self,y_pred,y_true):\n",
    "        relu = nn.ReLU()\n",
    "        qloss = (self.q*relu(y_pred-y_true)+(1-self.q)*relu(y_true-y_pred)).sum()\n",
    "        loss = self.z*(((y_pred-y_true)**2).sum())+(1-self.z)*qloss\n",
    "        return loss\n",
    "    \n",
    "    def plot(self,H,dropout=False):\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.plot(self.train_rmse_lst, label='subtrain')\n",
    "        plt.plot(self.valid_rmse_lst, label='valid')\n",
    "        plt.title(f' MLP with Four Hidden Layers and L2 + customerized Loss (H = {H}, z = {self.z}, dropout = {dropout})')\n",
    "        plt.xlabel('batchs (100 per)')\n",
    "        plt.ylabel('RMSE')\n",
    "        plt.legend(loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jmrV9huULWly",
   "metadata": {
    "id": "jmrV9huULWly"
   },
   "source": [
    "畫出Training and Validation RMSE，並報告Test RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a220daf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "a220daf5",
    "outputId": "748f9c07-4688-45c2-8c90-3ddeda4fb900"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Batch:100\n",
      "Training RMSE:10.926386833190918\n",
      "Validation RMSE:10.531264305114746\n",
      "=====================================\n",
      "Epoch:0 Batch:200\n",
      "Training RMSE:10.75102424621582\n",
      "Validation RMSE:10.034151077270508\n",
      "=====================================\n",
      "Epoch:0 Batch:300\n",
      "Training RMSE:10.492703437805176\n",
      "Validation RMSE:9.386927604675293\n",
      "=====================================\n",
      "Epoch:0 Batch:400\n",
      "Training RMSE:10.284648895263672\n",
      "Validation RMSE:9.282980918884277\n",
      "=====================================\n",
      "Epoch:1 Batch:500\n",
      "Training RMSE:10.16113567352295\n",
      "Validation RMSE:9.228525161743164\n",
      "=====================================\n",
      "Epoch:1 Batch:600\n",
      "Training RMSE:10.065363883972168\n",
      "Validation RMSE:9.133197784423828\n",
      "=====================================\n",
      "Epoch:1 Batch:700\n",
      "Training RMSE:9.996522903442383\n",
      "Validation RMSE:9.075819969177246\n",
      "=====================================\n",
      "Epoch:1 Batch:800\n",
      "Training RMSE:9.915518760681152\n",
      "Validation RMSE:9.060595512390137\n",
      "=====================================\n",
      "Epoch:2 Batch:900\n",
      "Training RMSE:9.863781929016113\n",
      "Validation RMSE:9.083978652954102\n",
      "=====================================\n",
      "Epoch:2 Batch:1000\n",
      "Training RMSE:9.820226669311523\n",
      "Validation RMSE:9.099539756774902\n",
      "=====================================\n",
      "Epoch:2 Batch:1100\n",
      "Training RMSE:9.776508331298828\n",
      "Validation RMSE:9.05945110321045\n",
      "=====================================\n",
      "Epoch:2 Batch:1200\n",
      "Training RMSE:9.748570442199707\n",
      "Validation RMSE:9.046650886535645\n",
      "=====================================\n",
      "Epoch:3 Batch:1300\n",
      "Training RMSE:9.722028732299805\n",
      "Validation RMSE:9.078142166137695\n",
      "=====================================\n",
      "Epoch:3 Batch:1400\n",
      "Training RMSE:9.70079517364502\n",
      "Validation RMSE:9.019786834716797\n",
      "=====================================\n",
      "Epoch:3 Batch:1500\n",
      "Training RMSE:9.677383422851562\n",
      "Validation RMSE:9.03069019317627\n",
      "=====================================\n",
      "Epoch:3 Batch:1600\n",
      "Training RMSE:9.65736198425293\n",
      "Validation RMSE:9.022371292114258\n",
      "=====================================\n",
      "Epoch:4 Batch:1700\n",
      "Training RMSE:9.637748718261719\n",
      "Validation RMSE:9.0247220993042\n",
      "=====================================\n",
      "Epoch:4 Batch:1800\n",
      "Training RMSE:9.620948791503906\n",
      "Validation RMSE:8.996472358703613\n",
      "=====================================\n",
      "Epoch:4 Batch:1900\n",
      "Training RMSE:9.60714054107666\n",
      "Validation RMSE:8.985280990600586\n",
      "=====================================\n",
      "Epoch:4 Batch:2000\n",
      "Training RMSE:9.59277057647705\n",
      "Validation RMSE:8.958212852478027\n",
      "=====================================\n",
      "Epoch:5 Batch:2100\n",
      "Training RMSE:9.580679893493652\n",
      "Validation RMSE:9.003180503845215\n",
      "=====================================\n",
      "Epoch:5 Batch:2200\n",
      "Training RMSE:9.569018363952637\n",
      "Validation RMSE:8.954142570495605\n",
      "=====================================\n",
      "Epoch:5 Batch:2300\n",
      "Training RMSE:9.556581497192383\n",
      "Validation RMSE:8.952140808105469\n",
      "=====================================\n",
      "Epoch:5 Batch:2400\n",
      "Training RMSE:9.54760456085205\n",
      "Validation RMSE:8.954716682434082\n",
      "=====================================\n",
      "Epoch:5 Batch:2500\n",
      "Training RMSE:9.538114547729492\n",
      "Validation RMSE:8.97603988647461\n",
      "=====================================\n",
      "Epoch:6 Batch:2600\n",
      "Training RMSE:9.529293060302734\n",
      "Validation RMSE:8.940903663635254\n",
      "=====================================\n",
      "Epoch:6 Batch:2700\n",
      "Training RMSE:9.52075481414795\n",
      "Validation RMSE:8.950692176818848\n",
      "=====================================\n",
      "Epoch:6 Batch:2800\n",
      "Training RMSE:9.513819694519043\n",
      "Validation RMSE:8.953154563903809\n",
      "=====================================\n",
      "Epoch:6 Batch:2900\n",
      "Training RMSE:9.50440788269043\n",
      "Validation RMSE:8.943597793579102\n",
      "=====================================\n",
      "Epoch:7 Batch:3000\n",
      "Training RMSE:9.497477531433105\n",
      "Validation RMSE:8.935580253601074\n",
      "=====================================\n",
      "Epoch:7 Batch:3100\n",
      "Training RMSE:9.488946914672852\n",
      "Validation RMSE:8.957916259765625\n",
      "=====================================\n",
      "Epoch:7 Batch:3200\n",
      "Training RMSE:9.482542991638184\n",
      "Validation RMSE:8.95175552368164\n",
      "=====================================\n",
      "Epoch:7 Batch:3300\n",
      "Training RMSE:9.476820945739746\n",
      "Validation RMSE:8.963581085205078\n",
      "=====================================\n",
      "Epoch:8 Batch:3400\n",
      "Training RMSE:9.471247673034668\n",
      "Validation RMSE:8.968523025512695\n",
      "=====================================\n",
      "Epoch:8 Batch:3500\n",
      "Training RMSE:9.463838577270508\n",
      "Validation RMSE:8.965078353881836\n",
      "=====================================\n",
      "Epoch:8 Batch:3600\n",
      "Training RMSE:9.458928108215332\n",
      "Validation RMSE:8.949837684631348\n",
      "=====================================\n",
      "Epoch:8 Batch:3700\n",
      "Training RMSE:9.454484939575195\n",
      "Validation RMSE:8.925609588623047\n",
      "=====================================\n",
      "Epoch:9 Batch:3800\n",
      "Training RMSE:9.451550483703613\n",
      "Validation RMSE:8.91616153717041\n",
      "=====================================\n",
      "Epoch:9 Batch:3900\n",
      "Training RMSE:9.445945739746094\n",
      "Validation RMSE:8.926837921142578\n",
      "=====================================\n",
      "Epoch:9 Batch:4000\n",
      "Training RMSE:9.441364288330078\n",
      "Validation RMSE:8.95773696899414\n",
      "=====================================\n",
      "Epoch:9 Batch:4100\n",
      "Training RMSE:9.437762260437012\n",
      "Validation RMSE:8.887683868408203\n",
      "=====================================\n",
      "Epoch:10 Batch:4200\n",
      "Training RMSE:9.433399200439453\n",
      "Validation RMSE:8.9332857131958\n",
      "=====================================\n",
      "Epoch:10 Batch:4300\n",
      "Training RMSE:9.429239273071289\n",
      "Validation RMSE:8.925355911254883\n",
      "=====================================\n",
      "Epoch:10 Batch:4400\n",
      "Training RMSE:9.426172256469727\n",
      "Validation RMSE:8.937382698059082\n",
      "=====================================\n",
      "Epoch:10 Batch:4500\n",
      "Training RMSE:9.421801567077637\n",
      "Validation RMSE:8.96412467956543\n",
      "=====================================\n",
      "Epoch:11 Batch:4600\n",
      "Training RMSE:9.417940139770508\n",
      "Validation RMSE:8.902637481689453\n",
      "=====================================\n",
      "Epoch:11 Batch:4700\n",
      "Training RMSE:9.413366317749023\n",
      "Validation RMSE:8.915251731872559\n",
      "=====================================\n",
      "Epoch:11 Batch:4800\n",
      "Training RMSE:9.408827781677246\n",
      "Validation RMSE:8.939489364624023\n",
      "=====================================\n",
      "Epoch:11 Batch:4900\n",
      "Training RMSE:9.406413078308105\n",
      "Validation RMSE:8.896449089050293\n",
      "=====================================\n",
      "Epoch:11 Batch:5000\n",
      "Training RMSE:9.403936386108398\n",
      "Validation RMSE:8.906608581542969\n",
      "=====================================\n",
      "Epoch:12 Batch:5100\n",
      "Training RMSE:9.400386810302734\n",
      "Validation RMSE:8.926435470581055\n",
      "=====================================\n",
      "Epoch:12 Batch:5200\n",
      "Training RMSE:9.398883819580078\n",
      "Validation RMSE:8.919670104980469\n",
      "=====================================\n",
      "Epoch:12 Batch:5300\n",
      "Training RMSE:9.394593238830566\n",
      "Validation RMSE:8.931890487670898\n",
      "=====================================\n",
      "Epoch:12 Batch:5400\n",
      "Training RMSE:9.392354965209961\n",
      "Validation RMSE:8.936334609985352\n",
      "=====================================\n",
      "Epoch:13 Batch:5500\n",
      "Training RMSE:9.388692855834961\n",
      "Validation RMSE:8.90617561340332\n",
      "=====================================\n",
      "Epoch:13 Batch:5600\n",
      "Training RMSE:9.38603401184082\n",
      "Validation RMSE:8.901252746582031\n",
      "=====================================\n",
      "Epoch:13 Batch:5700\n",
      "Training RMSE:9.384037971496582\n",
      "Validation RMSE:8.912657737731934\n",
      "=====================================\n",
      "Epoch:13 Batch:5800\n",
      "Training RMSE:9.381508827209473\n",
      "Validation RMSE:8.91329288482666\n",
      "=====================================\n",
      "Epoch:14 Batch:5900\n",
      "Training RMSE:9.378423690795898\n",
      "Validation RMSE:8.885331153869629\n",
      "=====================================\n",
      "Epoch:14 Batch:6000\n",
      "Training RMSE:9.377306938171387\n",
      "Validation RMSE:8.889175415039062\n",
      "=====================================\n",
      "Epoch:14 Batch:6100\n",
      "Training RMSE:9.373516082763672\n",
      "Validation RMSE:8.89164924621582\n",
      "=====================================\n",
      "Epoch:14 Batch:6200\n",
      "Training RMSE:9.371710777282715\n",
      "Validation RMSE:8.879081726074219\n",
      "=====================================\n",
      "Epoch:15 Batch:6300\n",
      "Training RMSE:9.369604110717773\n",
      "Validation RMSE:8.906914710998535\n",
      "=====================================\n",
      "Epoch:15 Batch:6400\n",
      "Training RMSE:9.368518829345703\n",
      "Validation RMSE:8.865074157714844\n",
      "=====================================\n",
      "Epoch:15 Batch:6500\n",
      "Training RMSE:9.365541458129883\n",
      "Validation RMSE:8.87671184539795\n",
      "=====================================\n",
      "Epoch:15 Batch:6600\n",
      "Training RMSE:9.363679885864258\n",
      "Validation RMSE:8.870548248291016\n",
      "=====================================\n",
      "Epoch:16 Batch:6700\n",
      "Training RMSE:9.361360549926758\n",
      "Validation RMSE:8.974384307861328\n",
      "=====================================\n",
      "Epoch:16 Batch:6800\n",
      "Training RMSE:9.359487533569336\n",
      "Validation RMSE:8.888301849365234\n",
      "=====================================\n",
      "Epoch:16 Batch:6900\n",
      "Training RMSE:9.357353210449219\n",
      "Validation RMSE:8.94090747833252\n",
      "=====================================\n",
      "Epoch:16 Batch:7000\n",
      "Training RMSE:9.355570793151855\n",
      "Validation RMSE:8.874007225036621\n",
      "=====================================\n",
      "Epoch:16 Batch:7100\n",
      "Training RMSE:9.35374927520752\n",
      "Validation RMSE:8.908191680908203\n",
      "=====================================\n",
      "Epoch:17 Batch:7200\n",
      "Training RMSE:9.35214900970459\n",
      "Validation RMSE:8.876585006713867\n",
      "=====================================\n",
      "Epoch:17 Batch:7300\n",
      "Training RMSE:9.350492477416992\n",
      "Validation RMSE:8.938750267028809\n",
      "=====================================\n",
      "Epoch:17 Batch:7400\n",
      "Training RMSE:9.348054885864258\n",
      "Validation RMSE:8.888063430786133\n",
      "=====================================\n",
      "Epoch:17 Batch:7500\n",
      "Training RMSE:9.346426963806152\n",
      "Validation RMSE:8.912435531616211\n",
      "=====================================\n",
      "Epoch:18 Batch:7600\n",
      "Training RMSE:9.345227241516113\n",
      "Validation RMSE:8.871546745300293\n",
      "=====================================\n",
      "Epoch:18 Batch:7700\n",
      "Training RMSE:9.34329605102539\n",
      "Validation RMSE:8.907608985900879\n",
      "=====================================\n",
      "Epoch:18 Batch:7800\n",
      "Training RMSE:9.341423034667969\n",
      "Validation RMSE:8.903560638427734\n",
      "=====================================\n",
      "Epoch:18 Batch:7900\n",
      "Training RMSE:9.34020709991455\n",
      "Validation RMSE:8.866523742675781\n",
      "=====================================\n",
      "Epoch:19 Batch:8000\n",
      "Training RMSE:9.337970733642578\n",
      "Validation RMSE:8.877617835998535\n",
      "=====================================\n",
      "Epoch:19 Batch:8100\n",
      "Training RMSE:9.336299896240234\n",
      "Validation RMSE:8.871175765991211\n",
      "=====================================\n",
      "Epoch:19 Batch:8200\n",
      "Training RMSE:9.33414077758789\n",
      "Validation RMSE:8.872475624084473\n",
      "=====================================\n",
      "Epoch:19 Batch:8300\n",
      "Training RMSE:9.33327579498291\n",
      "Validation RMSE:8.914106369018555\n",
      "=====================================\n",
      "Epoch:20 Batch:8400\n",
      "Training RMSE:9.331962585449219\n",
      "Validation RMSE:8.91291618347168\n",
      "=====================================\n",
      "Epoch:20 Batch:8500\n",
      "Training RMSE:9.330730438232422\n",
      "Validation RMSE:8.90287971496582\n",
      "=====================================\n",
      "Epoch:20 Batch:8600\n",
      "Training RMSE:9.329588890075684\n",
      "Validation RMSE:8.90318489074707\n",
      "=====================================\n",
      "Epoch:20 Batch:8700\n",
      "Training RMSE:9.327699661254883\n",
      "Validation RMSE:8.911865234375\n",
      "=====================================\n",
      "Epoch:21 Batch:8800\n",
      "Training RMSE:9.32688045501709\n",
      "Validation RMSE:8.919273376464844\n",
      "=====================================\n",
      "Epoch:21 Batch:8900\n",
      "Training RMSE:9.325772285461426\n",
      "Validation RMSE:8.883257865905762\n",
      "=====================================\n",
      "Epoch:21 Batch:9000\n",
      "Training RMSE:9.324151992797852\n",
      "Validation RMSE:8.892255783081055\n",
      "=====================================\n",
      "Epoch:21 Batch:9100\n",
      "Training RMSE:9.32332992553711\n",
      "Validation RMSE:8.871053695678711\n",
      "=====================================\n",
      "Epoch:22 Batch:9200\n",
      "Training RMSE:9.321649551391602\n",
      "Validation RMSE:8.905522346496582\n",
      "=====================================\n",
      "Epoch:22 Batch:9300\n",
      "Training RMSE:9.321009635925293\n",
      "Validation RMSE:8.849320411682129\n",
      "=====================================\n",
      "Epoch:22 Batch:9400\n",
      "Training RMSE:9.319499969482422\n",
      "Validation RMSE:8.878680229187012\n",
      "=====================================\n",
      "Epoch:22 Batch:9500\n",
      "Training RMSE:9.31771469116211\n",
      "Validation RMSE:8.878605842590332\n",
      "=====================================\n",
      "Epoch:22 Batch:9600\n",
      "Training RMSE:9.316595077514648\n",
      "Validation RMSE:8.902387619018555\n",
      "=====================================\n",
      "Epoch:23 Batch:9700\n",
      "Training RMSE:9.31538200378418\n",
      "Validation RMSE:8.93089485168457\n",
      "=====================================\n",
      "Epoch:23 Batch:9800\n",
      "Training RMSE:9.3145170211792\n",
      "Validation RMSE:8.888134002685547\n",
      "=====================================\n",
      "Epoch:23 Batch:9900\n",
      "Training RMSE:9.313620567321777\n",
      "Validation RMSE:8.847576141357422\n",
      "=====================================\n",
      "Epoch:23 Batch:10000\n",
      "Training RMSE:9.311959266662598\n",
      "Validation RMSE:8.891538619995117\n",
      "=====================================\n",
      "Epoch:24 Batch:10100\n",
      "Training RMSE:9.310463905334473\n",
      "Validation RMSE:8.90338134765625\n",
      "=====================================\n",
      "Epoch:24 Batch:10200\n",
      "Training RMSE:9.308893203735352\n",
      "Validation RMSE:8.894983291625977\n",
      "=====================================\n",
      "Epoch:24 Batch:10300\n",
      "Training RMSE:9.3079252243042\n",
      "Validation RMSE:8.826155662536621\n",
      "=====================================\n",
      "Epoch:24 Batch:10400\n",
      "Training RMSE:9.306934356689453\n",
      "Validation RMSE:8.930205345153809\n",
      "=====================================\n",
      "Epoch:25 Batch:10500\n",
      "Training RMSE:9.306236267089844\n",
      "Validation RMSE:8.878767013549805\n",
      "=====================================\n",
      "Epoch:25 Batch:10600\n",
      "Training RMSE:9.304893493652344\n",
      "Validation RMSE:8.885651588439941\n",
      "=====================================\n",
      "Epoch:25 Batch:10700\n",
      "Training RMSE:9.303205490112305\n",
      "Validation RMSE:8.896071434020996\n",
      "=====================================\n",
      "Epoch:25 Batch:10800\n",
      "Training RMSE:9.302445411682129\n",
      "Validation RMSE:8.880566596984863\n",
      "=====================================\n",
      "Epoch:26 Batch:10900\n",
      "Training RMSE:9.301518440246582\n",
      "Validation RMSE:8.896602630615234\n",
      "=====================================\n",
      "Epoch:26 Batch:11000\n",
      "Training RMSE:9.299912452697754\n",
      "Validation RMSE:8.860825538635254\n",
      "=====================================\n",
      "Epoch:26 Batch:11100\n",
      "Training RMSE:9.299178123474121\n",
      "Validation RMSE:8.899349212646484\n",
      "=====================================\n",
      "Epoch:26 Batch:11200\n",
      "Training RMSE:9.29786205291748\n",
      "Validation RMSE:8.918197631835938\n",
      "=====================================\n",
      "Epoch:27 Batch:11300\n",
      "Training RMSE:9.297477722167969\n",
      "Validation RMSE:8.860514640808105\n",
      "=====================================\n",
      "Epoch:27 Batch:11400\n",
      "Training RMSE:9.296187400817871\n",
      "Validation RMSE:8.90404987335205\n",
      "=====================================\n",
      "Epoch:27 Batch:11500\n",
      "Training RMSE:9.295369148254395\n",
      "Validation RMSE:8.871455192565918\n",
      "=====================================\n",
      "Epoch:27 Batch:11600\n",
      "Training RMSE:9.29440689086914\n",
      "Validation RMSE:8.88136100769043\n",
      "=====================================\n",
      "Epoch:27 Batch:11700\n",
      "Training RMSE:9.293617248535156\n",
      "Validation RMSE:8.901223182678223\n",
      "=====================================\n",
      "Epoch:28 Batch:11800\n",
      "Training RMSE:9.292980194091797\n",
      "Validation RMSE:8.902787208557129\n",
      "=====================================\n",
      "Epoch:28 Batch:11900\n",
      "Training RMSE:9.292214393615723\n",
      "Validation RMSE:8.88762378692627\n",
      "=====================================\n",
      "Epoch:28 Batch:12000\n",
      "Training RMSE:9.291085243225098\n",
      "Validation RMSE:8.86781120300293\n",
      "=====================================\n",
      "Epoch:28 Batch:12100\n",
      "Training RMSE:9.29037857055664\n",
      "Validation RMSE:8.878694534301758\n",
      "=====================================\n",
      "Epoch:29 Batch:12200\n",
      "Training RMSE:9.289201736450195\n",
      "Validation RMSE:8.89262580871582\n",
      "=====================================\n",
      "Epoch:29 Batch:12300\n",
      "Training RMSE:9.288105964660645\n",
      "Validation RMSE:8.851529121398926\n",
      "=====================================\n",
      "Epoch:29 Batch:12400\n",
      "Training RMSE:9.287485122680664\n",
      "Validation RMSE:8.887872695922852\n",
      "=====================================\n",
      "Epoch:29 Batch:12500\n",
      "Training RMSE:9.287162780761719\n",
      "Validation RMSE:8.869115829467773\n",
      "=====================================\n",
      "Epoch:30 Batch:12600\n",
      "Training RMSE:9.286738395690918\n",
      "Validation RMSE:8.897897720336914\n",
      "=====================================\n",
      "Epoch:30 Batch:12700\n",
      "Training RMSE:9.285337448120117\n",
      "Validation RMSE:8.902588844299316\n",
      "=====================================\n",
      "Epoch:30 Batch:12800\n",
      "Training RMSE:9.284611701965332\n",
      "Validation RMSE:8.886734962463379\n",
      "=====================================\n",
      "Epoch:30 Batch:12900\n",
      "Training RMSE:9.283869743347168\n",
      "Validation RMSE:8.88550090789795\n",
      "=====================================\n",
      "Epoch:31 Batch:13000\n",
      "Training RMSE:9.2828369140625\n",
      "Validation RMSE:8.933667182922363\n",
      "=====================================\n",
      "Epoch:31 Batch:13100\n",
      "Training RMSE:9.281781196594238\n",
      "Validation RMSE:8.875812530517578\n",
      "=====================================\n",
      "Epoch:31 Batch:13200\n",
      "Training RMSE:9.281004905700684\n",
      "Validation RMSE:8.885852813720703\n",
      "=====================================\n",
      "Epoch:31 Batch:13300\n",
      "Training RMSE:9.280851364135742\n",
      "Validation RMSE:8.853750228881836\n",
      "=====================================\n",
      "Epoch:32 Batch:13400\n",
      "Training RMSE:9.280139923095703\n",
      "Validation RMSE:8.85934829711914\n",
      "=====================================\n",
      "Epoch:32 Batch:13500\n",
      "Training RMSE:9.279376029968262\n",
      "Validation RMSE:8.857508659362793\n",
      "=====================================\n",
      "Epoch:32 Batch:13600\n",
      "Training RMSE:9.27869701385498\n",
      "Validation RMSE:8.873307228088379\n",
      "=====================================\n",
      "Epoch:32 Batch:13700\n",
      "Training RMSE:9.278162002563477\n",
      "Validation RMSE:8.83571720123291\n",
      "=====================================\n",
      "Epoch:33 Batch:13800\n",
      "Training RMSE:9.277420997619629\n",
      "Validation RMSE:8.844093322753906\n",
      "=====================================\n",
      "Epoch:33 Batch:13900\n",
      "Training RMSE:9.276522636413574\n",
      "Validation RMSE:8.868928909301758\n",
      "=====================================\n",
      "Epoch:33 Batch:14000\n",
      "Training RMSE:9.275508880615234\n",
      "Validation RMSE:8.887388229370117\n",
      "=====================================\n",
      "Epoch:33 Batch:14100\n",
      "Training RMSE:9.275327682495117\n",
      "Validation RMSE:8.89537239074707\n",
      "=====================================\n",
      "Epoch:33 Batch:14200\n",
      "Training RMSE:9.274755477905273\n",
      "Validation RMSE:8.887223243713379\n",
      "=====================================\n",
      "Epoch:34 Batch:14300\n",
      "Training RMSE:9.273859977722168\n",
      "Validation RMSE:8.905814170837402\n",
      "=====================================\n",
      "Epoch:34 Batch:14400\n",
      "Training RMSE:9.273472785949707\n",
      "Validation RMSE:8.822510719299316\n",
      "=====================================\n",
      "Epoch:34 Batch:14500\n",
      "Training RMSE:9.272712707519531\n",
      "Validation RMSE:8.865726470947266\n",
      "=====================================\n",
      "Epoch:34 Batch:14600\n",
      "Training RMSE:9.272175788879395\n",
      "Validation RMSE:8.890835762023926\n",
      "=====================================\n",
      "Epoch:35 Batch:14700\n",
      "Training RMSE:9.271270751953125\n",
      "Validation RMSE:8.848755836486816\n",
      "=====================================\n",
      "Epoch:35 Batch:14800\n",
      "Training RMSE:9.270310401916504\n",
      "Validation RMSE:8.848400115966797\n",
      "=====================================\n",
      "Epoch:35 Batch:14900\n",
      "Training RMSE:9.269761085510254\n",
      "Validation RMSE:8.842813491821289\n",
      "=====================================\n",
      "Epoch:35 Batch:15000\n",
      "Training RMSE:9.269209861755371\n",
      "Validation RMSE:8.894645690917969\n",
      "=====================================\n",
      "Epoch:36 Batch:15100\n",
      "Training RMSE:9.268547058105469\n",
      "Validation RMSE:8.851325035095215\n",
      "=====================================\n",
      "Epoch:36 Batch:15200\n",
      "Training RMSE:9.26767635345459\n",
      "Validation RMSE:8.834427833557129\n",
      "=====================================\n",
      "Epoch:36 Batch:15300\n",
      "Training RMSE:9.26692008972168\n",
      "Validation RMSE:8.875377655029297\n",
      "=====================================\n",
      "Epoch:36 Batch:15400\n",
      "Training RMSE:9.266616821289062\n",
      "Validation RMSE:8.854470252990723\n",
      "=====================================\n",
      "Epoch:37 Batch:15500\n",
      "Training RMSE:9.266095161437988\n",
      "Validation RMSE:8.854340553283691\n",
      "=====================================\n",
      "Epoch:37 Batch:15600\n",
      "Training RMSE:9.264914512634277\n",
      "Validation RMSE:8.870719909667969\n",
      "=====================================\n",
      "Epoch:37 Batch:15700\n",
      "Training RMSE:9.264552116394043\n",
      "Validation RMSE:8.88331413269043\n",
      "=====================================\n",
      "Epoch:37 Batch:15800\n",
      "Training RMSE:9.26421070098877\n",
      "Validation RMSE:8.873900413513184\n",
      "=====================================\n",
      "Epoch:38 Batch:15900\n",
      "Training RMSE:9.263588905334473\n",
      "Validation RMSE:8.874672889709473\n",
      "=====================================\n",
      "Epoch:38 Batch:16000\n",
      "Training RMSE:9.263373374938965\n",
      "Validation RMSE:8.871969223022461\n",
      "=====================================\n",
      "Epoch:38 Batch:16100\n",
      "Training RMSE:9.262906074523926\n",
      "Validation RMSE:8.831435203552246\n",
      "=====================================\n",
      "Epoch:38 Batch:16200\n",
      "Training RMSE:9.26218318939209\n",
      "Validation RMSE:8.865785598754883\n",
      "=====================================\n",
      "Epoch:38 Batch:16300\n",
      "Training RMSE:9.26142692565918\n",
      "Validation RMSE:8.89875602722168\n",
      "=====================================\n",
      "Epoch:39 Batch:16400\n",
      "Training RMSE:9.260793685913086\n",
      "Validation RMSE:8.87136173248291\n",
      "=====================================\n",
      "Epoch:39 Batch:16500\n",
      "Training RMSE:9.26028823852539\n",
      "Validation RMSE:8.874255180358887\n",
      "=====================================\n",
      "Epoch:39 Batch:16600\n",
      "Training RMSE:9.259499549865723\n",
      "Validation RMSE:8.856407165527344\n",
      "=====================================\n",
      "Epoch:39 Batch:16700\n",
      "Training RMSE:9.259261131286621\n",
      "Validation RMSE:8.869497299194336\n",
      "=====================================\n",
      "Epoch:40 Batch:16800\n",
      "Training RMSE:9.258288383483887\n",
      "Validation RMSE:8.866874694824219\n",
      "=====================================\n",
      "Epoch:40 Batch:16900\n",
      "Training RMSE:9.258183479309082\n",
      "Validation RMSE:8.888126373291016\n",
      "=====================================\n",
      "Epoch:40 Batch:17000\n",
      "Training RMSE:9.257877349853516\n",
      "Validation RMSE:8.912276268005371\n",
      "=====================================\n",
      "Epoch:40 Batch:17100\n",
      "Training RMSE:9.257214546203613\n",
      "Validation RMSE:8.87692642211914\n",
      "=====================================\n",
      "Epoch:41 Batch:17200\n",
      "Training RMSE:9.25693130493164\n",
      "Validation RMSE:8.848957061767578\n",
      "=====================================\n",
      "Epoch:41 Batch:17300\n",
      "Training RMSE:9.256902694702148\n",
      "Validation RMSE:8.856680870056152\n",
      "=====================================\n",
      "Epoch:41 Batch:17400\n",
      "Training RMSE:9.25606918334961\n",
      "Validation RMSE:8.851370811462402\n",
      "=====================================\n",
      "Epoch:41 Batch:17500\n",
      "Training RMSE:9.255438804626465\n",
      "Validation RMSE:8.896206855773926\n",
      "=====================================\n",
      "Epoch:42 Batch:17600\n",
      "Training RMSE:9.25483226776123\n",
      "Validation RMSE:8.891530990600586\n",
      "=====================================\n",
      "Epoch:42 Batch:17700\n",
      "Training RMSE:9.254209518432617\n",
      "Validation RMSE:8.889566421508789\n",
      "=====================================\n",
      "Epoch:42 Batch:17800\n",
      "Training RMSE:9.253703117370605\n",
      "Validation RMSE:8.8776273727417\n",
      "=====================================\n",
      "Epoch:42 Batch:17900\n",
      "Training RMSE:9.253195762634277\n",
      "Validation RMSE:8.846643447875977\n",
      "=====================================\n",
      "Epoch:43 Batch:18000\n",
      "Training RMSE:9.252886772155762\n",
      "Validation RMSE:8.843754768371582\n",
      "=====================================\n",
      "Epoch:43 Batch:18100\n",
      "Training RMSE:9.25260066986084\n",
      "Validation RMSE:8.893590927124023\n",
      "=====================================\n",
      "Epoch:43 Batch:18200\n",
      "Training RMSE:9.252201080322266\n",
      "Validation RMSE:8.838189125061035\n",
      "=====================================\n",
      "Epoch:43 Batch:18300\n",
      "Training RMSE:9.25177001953125\n",
      "Validation RMSE:8.869983673095703\n",
      "=====================================\n",
      "Epoch:44 Batch:18400\n",
      "Training RMSE:9.25088882446289\n",
      "Validation RMSE:8.871471405029297\n",
      "=====================================\n",
      "Epoch:44 Batch:18500\n",
      "Training RMSE:9.250173568725586\n",
      "Validation RMSE:8.908113479614258\n",
      "=====================================\n",
      "Epoch:44 Batch:18600\n",
      "Training RMSE:9.249799728393555\n",
      "Validation RMSE:8.865178108215332\n",
      "=====================================\n",
      "Epoch:44 Batch:18700\n",
      "Training RMSE:9.249397277832031\n",
      "Validation RMSE:8.862947463989258\n",
      "=====================================\n",
      "Epoch:44 Batch:18800\n",
      "Training RMSE:9.2491455078125\n",
      "Validation RMSE:8.918228149414062\n",
      "=====================================\n",
      "Epoch:45 Batch:18900\n",
      "Training RMSE:9.248635292053223\n",
      "Validation RMSE:8.878074645996094\n",
      "=====================================\n",
      "Epoch:45 Batch:19000\n",
      "Training RMSE:9.24797534942627\n",
      "Validation RMSE:8.88288688659668\n",
      "=====================================\n",
      "Epoch:45 Batch:19100\n",
      "Training RMSE:9.247579574584961\n",
      "Validation RMSE:8.85202693939209\n",
      "=====================================\n",
      "Epoch:45 Batch:19200\n",
      "Training RMSE:9.247289657592773\n",
      "Validation RMSE:8.834885597229004\n",
      "=====================================\n",
      "Epoch:46 Batch:19300\n",
      "Training RMSE:9.247078895568848\n",
      "Validation RMSE:8.835585594177246\n",
      "=====================================\n",
      "Epoch:46 Batch:19400\n",
      "Training RMSE:9.246675491333008\n",
      "Validation RMSE:8.847267150878906\n",
      "=====================================\n",
      "Epoch:46 Batch:19500\n",
      "Training RMSE:9.246201515197754\n",
      "Validation RMSE:8.87913703918457\n",
      "=====================================\n",
      "============Early Stop==================\n",
      "best step:14400 best loss:8.822510719299316\n",
      "H = 90 z =0 Test_RMSE = 8.991192817687988\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "lr = 0.001\n",
    "input_shape = subtrainset.Xnp.shape[1]\n",
    "output_shape = 1\n",
    "H = 90\n",
    "z=0\n",
    "q=0.5\n",
    "\n",
    "Q8_model = Q8MLP(z,q)\n",
    "model = Q8_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "model = Q8_model.train(device,optimizer,verbose=True)\n",
    "Q8test_RMSE = Q8_model.test(device,testloader)\n",
    "print(f'H = {H} z ={z} Test_RMSE = {Q8test_RMSE}')\n",
    "Q8_model.plot(H,dropout=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Gmdjmc1OLgC6",
   "metadata": {
    "id": "Gmdjmc1OLgC6"
   },
   "source": [
    "使用z = 0.1, 0.5, 0.9, 1.0訓練模型(不須提供訓練過程的Loss圖形)，統整各個z值下的Test RMSE並討論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fPl-h4RFLcJQ",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "fPl-h4RFLcJQ",
    "outputId": "5f245a37-fc2e-4e7b-e3b2-b482cdd3b19a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q8MLP(H=90 z =0.0) is training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============Early Stop==================\n",
      "best step:9800 best loss:8.875958442687988\n",
      "H = 90 z =0.0 Test_RMSE = 8.97962760925293\n",
      "Q8MLP(H=90 z =0.1) is training\n",
      "============Early Stop==================\n",
      "best step:17000 best loss:8.56253433227539\n",
      "H = 90 z =0.1 Test_RMSE = 8.795825958251953\n",
      "Q8MLP(H=90 z =0.5) is training\n",
      "============Early Stop==================\n",
      "best step:17700 best loss:8.576507568359375\n",
      "H = 90 z =0.5 Test_RMSE = 8.787635803222656\n",
      "Q8MLP(H=90 z =0.9) is training\n",
      "============Early Stop==================\n",
      "best step:17800 best loss:8.569211959838867\n",
      "H = 90 z =0.9 Test_RMSE = 8.78547477722168\n",
      "Q8MLP(H=90 z =1.0) is training\n",
      "============Early Stop==================\n",
      "best step:19400 best loss:8.563044548034668\n",
      "H = 90 z =1.0 Test_RMSE = 8.780756950378418\n"
     ]
    }
   ],
   "source": [
    "z_list = [0.0, 0.1, 0.5, 0.9, 1.0]\n",
    "q = 0.5\n",
    "\n",
    "for z in z_list:\n",
    "    Q8_model = Q8MLP(z,q)\n",
    "    model = Q8_model.net(input_shape,output_shape,H,device,dropout=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    print(f'Q8MLP(H={H} z ={z}) is training')\n",
    "    model = Q8_model.train(device,optimizer,verbose=False)\n",
    "    test_RMSE = Q8_model.test(device,testloader)\n",
    "    print(f'H = {H} z ={z} Test_RMSE = {test_RMSE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rMHn_AFqLOKO",
   "metadata": {
    "id": "rMHn_AFqLOKO"
   },
   "source": [
    "### 統整各個z值下的Test RMSE並討論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Fftx0rZRLWn4",
   "metadata": {
    "id": "Fftx0rZRLWn4"
   },
   "source": [
    "，從以上結果我們可以看出隨著z變大，Test_RMSE呈現下降趨勢，z 的值越大，代表 L2 Loss 的佔比越大，因此我們可以推估 L2 Loss 較適合這個情境，故在z=1.0時有較好的表現。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "r10741071_簡睿閎hw4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "740329b21a1252e1c253d583311b6d3f7744168cabc7e2151c4ba7a007513518"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
